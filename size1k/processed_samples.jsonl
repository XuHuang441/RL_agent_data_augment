{"original_prompt": "<bos><start_of_turn>user\nIn this task, you need to reverse all words of a given length in the sentence. The number of letters in a word determine its length. For example, the length of the word \"apple\" is 5.\n\nInput: Consider Input: Sentence: 'a person going on a slop jump on skis'. Reverse all words of length '6' in the given sentence.\n\nOutput: a nosrep going on a slop jump on skis\n\n\nInput: Consider Input: Sentence: 'a sandwich that is on a plate on a table'. Reverse all words of length '8' in the given sentence.\n\nOutput: a hciwdnas that is on a plate on a table\n\n\nInput: Consider Input: Sentence: 'two people stand and sit near a display of skateboards'. Reverse all words of length '1' in the given sentence.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-11.0, -12.6875, -11.0, -11.0, -12.75], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires precise string manipulation: correct tokenization of words, accurate counting of alphabetic characters (handling single-letter words and possible punctuation), and selectively reversing characters only for words whose length exactly matches the given target. Common failure modes include off-by-one errors in length, reversing whole-word order instead of characters, miscounting due to punctuation, and failing to preserve spacing/punctuation/other words unchanged.\",\n    \"new_prompt\": \"For the sentence below, reverse the letters of every word that has exactly 6 alphabetic characters; leave all other words unchanged. When measuring a word's length, count only alphabetic characters (ignore punctuation). Output only the transformed sentence and nothing else.\\n\\nSentence: 'a tiny engine powered system runs simple tasks efficiently'\\nTarget length: 6\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires precise string manipulation: correct tokenization of words, accurate counting of alphabetic characters (handling single-letter words and possible punctuation), and selectively reversing characters only for words whose length exactly matches the given target. Common failure modes include off-by-one errors in length, reversing whole-word order instead of characters, miscounting due to punctuation, and failing to preserve spacing/punctuation/other words unchanged.", "new_prompt": "For the sentence below, reverse the letters of every word that has exactly 6 alphabetic characters; leave all other words unchanged. When measuring a word's length, count only alphabetic characters (ignore punctuation). Output only the transformed sentence and nothing else.\n\nSentence: 'a tiny engine powered system runs simple tasks efficiently'\nTarget length: 6", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\ninstruction:\nIn this task, you will be shown a passage. You need to write a fill-in-the-gap question based on your understanding of the events that might be inferred from the passage. Your question should be answerable based on the passage and only have one correct answer. Show the gap in your question with a _ .\nquestion:\nGreece is threatening to seize property, land and businesses belonging to Germany if Berlin does not agree to compensate it for the Nazi occupation of the Second World War. Prime minister Alexis Tsipras has demanded Germany pay more than £112billion in reparations. He said previous payments did not cover demands for destroyed infrastructure, compensation for war crimes and the return of a forced loan to the Nazis. Greek Prime Minister Alexis Tsipras, who says that Germany owes his country compensation worth hundreds of billions of euros due to the Nazi occupation during the Second World War In an emotive address to his parliament he spoke of his government pursuing its ‘duty to history’ and the people who ‘fought and gave their lives to defeat Nazism’. Germany angrily dismissed the demands yesterday, saying it had paid reparations to Greece.Greece has threatened Germany saying it will seize its property and landComes as Athens and EU reach a stand-off over negotiations on debt crisisGreek Prime Minister says Germany owes his country war compensationAlexis Tsipras adds Angela Merkel owes his country up to 332 billion eurosBut Germany has dismissed the demands saying reparations have already been solved\nanswer:\nA spokesman for _ chancellor Angela Merkel said the issue had been ‘legally and politically resolved’ at that time and during the talks which led to German reunification in 1990.\n\n\nquestion:\nWashington (CNN) -- An alleged Somali pirate leader has been indicted for the takeover of the yacht Quest and the kidnapping of four Americans who subsequently were killed during the incident. Mohammad Saaili Shibin, 50, appeared at a detention hearing in federal court in Norfolk, Virginia, on Wednesday afternoon after the indictment against him was unsealed. He will remain in federal custody. \"Today marks the first time that the U.S. government has captured and charged an alleged pirate in a leadership role -- a hostage negotiator who operated in Somalia,\" said U.S. Attorney Neil MacBride in a written statement. MacBride went on to say the arrest and charges against Shibin should \"send a strong message to all pirates that they are not beyond the reach of the FBI, whether they board the ships or remain on-shore in Somalia.\"Four Americans died during the hijacking of a yacht in FebruaryThe purported leader of alleged pirates was arrested in Somalia on April 4A prosecutor says the indictment marks the first U.S. prosecution of a pirate leader\nanswer:\nFBI agents were at the scene of the arrest, though law enforcement officials would not provide details beyond that the FBI worked with _ forces on the ground.\n\n\nquestion:\nThe socialite squatter who refused to move out of the her former friend's luxury apartment for over a year has finally been forced out. Police showed up to the Fifth Avenue pad of Julianne Michelle Di Palma on June 24 to escort out Melissa Berkelhammer, a Princeton and Harvard Law-educated 38-year-old blogger. True to form, though, she didn't go easily. 'It took hours to get them out,' a source told Page Six. 'Julianne hired movers and a large storage space on East 62nd Street so that their belongings would not be thrown out or lost.' The issue of Berkelhammer's reluctance, at that of her mother, to leave Di Palma's Upper East Side apartment went all the way to Washington as congresswoman Carolyn Maloney of New York penned a letter to the city marshal urging they be removed from the apartment.Melissa Berkelhammer and her mother JoEllen were forced out of the the Fifth Avenue apartment of her former friend Julianne Di Palma on June 24Berkelhammer has found a job as a restaurant hostess on Manhattan's Upper East SideThe Berkelhammers reportedly showed up with dozens of bags and took over the apartment for over a year as the Di Palma's sufferedThe mother-daughter duo are now living in a hotel\nanswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-1.6328125, -2.015625, -2.46875, 0.07958984375, -2.46875], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it combines precise reading comprehension with constrained output generation. The model must (1) understand events and relations in a short, sometimes noisy news passage; (2) pick a fact or event that is unambiguously answerable from the passage alone (no outside knowledge); (3) formulate a single cloze (fill-in-the-gap) question with exactly one blank shown as _; and (4) ensure the blank has exactly one correct answer—preferably a verbatim substring from the passage—to avoid ambiguity. Failures typically come from choosing overly broad or ambiguous gaps, creating multiple possible answers, not marking the blank correctly, producing extra text or multiple questions/answers, or relying on knowledge not contained in the passage.\",\n    \"new_prompt\": \"You will be given a short passage. Create exactly one cloze-style question (a single sentence) that asks about a specific event or factual detail that can be answered unambiguously using only the passage. Indicate the missing word or phrase with a single underscore character _ (one contiguous blank). The correct answer must be a contiguous substring that appears verbatim in the passage, and there must be exactly one possible correct answer. Output exactly two lines: first the question, second a line beginning with \\\"answer:\\\" followed by the exact substring from the passage that fills the blank. Do not include any additional text, explanations, or punctuation.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it combines precise reading comprehension with constrained output generation. The model must (1) understand events and relations in a short, sometimes noisy news passage; (2) pick a fact or event that is unambiguously answerable from the passage alone (no outside knowledge); (3) formulate a single cloze (fill-in-the-gap) question with exactly one blank shown as _; and (4) ensure the blank has exactly one correct answer—preferably a verbatim substring from the passage—to avoid ambiguity. Failures typically come from choosing overly broad or ambiguous gaps, creating multiple possible answers, not marking the blank correctly, producing extra text or multiple questions/answers, or relying on knowledge not contained in the passage.", "new_prompt": "You will be given a short passage. Create exactly one cloze-style question (a single sentence) that asks about a specific event or factual detail that can be answered unambiguously using only the passage. Indicate the missing word or phrase with a single underscore character _ (one contiguous blank). The correct answer must be a contiguous substring that appears verbatim in the passage, and there must be exactly one possible correct answer. Output exactly two lines: first the question, second a line beginning with \"answer:\" followed by the exact substring from the passage that fills the blank. Do not include any additional text, explanations, or punctuation.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you're given a context passage, a question, and three answer options. Your task is to return an incorrect answer option to the question from the choices given. For all questions, only one of the three answer options is correct. Pick one of the two incorrect answer options as the output.\n--------\nQuestion: Context: Ash hopped around on one foot as they clutched their other foot in their hands. \n Question: Why did Ash do this? \n Options: (A) make fun of other people (B) recover from rolling their ankle (C) do an interpretive dance\n\nAnswer: C\n\n\nQuestion: Context: Alex could not figure out how to fix the door. Alex lived and let live. \n Question: How would you describe Alex? \n Options: (A) a person who knows how to move on (B) a person who stays on a dead hourse (C) bothered by the broken door\n\nAnswer: B\n\n\nQuestion: Context: Alex didn't want his brother to play with his toys. Alex took it back from him. \n Question: How would Alex feel afterwards? \n Options: (A) proud (B) guilty (C) happy\n\nAnswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-7.25, -7.25, -7.25, -7.25, -7.25], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines standard reading-comprehension/inference with a nonstandard negative constraint: the model must identify the correct answer from context internally, but then deliberately output one of the incorrect choices. That requires (1) accurate semantic understanding of a short narrative to determine which option is actually correct, (2) instruction-following to do the opposite of the usual task (select an incorrect option), and (3) precise output control (return only a single labeled choice, not explanation). Models often default to choosing the correct answer and struggle with the inhibition/negation requirement, especially when multiple examples and repeated labels are present. Ambiguity in phrasing, label bias (e.g., preferring a particular letter), or failure to notice the negative instruction are common failure modes.\",\n    \"new_prompt\": \"You will be given a brief context, a question about it, and three answer options labeled (A), (B), and (C). Exactly one option is correct. Your task is to return the letter of an incorrect option (one of the two wrong answers). Do NOT return the correct option, and do not add any explanation — output only the single letter (A, B, or C).\\n\\nContext: Dana stared at her frozen video window and kept clicking the \\\"Reconnect\\\" button. After several failed attempts, she shut her laptop and rubbed her temples.\\nQuestion: Why did Dana do this?\\nOptions: (A) she was trying to fix connection problems, (B) she was annoyed by internet or device issues, (C) she was celebrating a completed project\\n\\nAnswer:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines standard reading-comprehension/inference with a nonstandard negative constraint: the model must identify the correct answer from context internally, but then deliberately output one of the incorrect choices. That requires (1) accurate semantic understanding of a short narrative to determine which option is actually correct, (2) instruction-following to do the opposite of the usual task (select an incorrect option), and (3) precise output control (return only a single labeled choice, not explanation). Models often default to choosing the correct answer and struggle with the inhibition/negation requirement, especially when multiple examples and repeated labels are present. Ambiguity in phrasing, label bias (e.g., preferring a particular letter), or failure to notice the negative instruction are common failure modes.", "new_prompt": "You will be given a brief context, a question about it, and three answer options labeled (A), (B), and (C). Exactly one option is correct. Your task is to return the letter of an incorrect option (one of the two wrong answers). Do NOT return the correct option, and do not add any explanation — output only the single letter (A, B, or C).\n\nContext: Dana stared at her frozen video window and kept clicking the \"Reconnect\" button. After several failed attempts, she shut her laptop and rubbed her temples.\nQuestion: Why did Dana do this?\nOptions: (A) she was trying to fix connection problems, (B) she was annoyed by internet or device issues, (C) she was celebrating a completed project\n\nAnswer:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nTASK DEFINITION: In this task, you are given two phrases: Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether the Head happens after the Tail or not. Classify your answers into \"Yes\" and \"No\". The phrase may also contain \"___\", a placeholder that can be an object, a person, and/or an action.\nPROBLEM: Head: PersonX achieves PersonX's goal<sep>Tail: none\n\nSOLUTION: No\n\nPROBLEM: Head: PersonX gets a new haircut<sep>Tail: PersonX has very long hair\n\nSOLUTION: Yes\n\nPROBLEM: Head: PersonX hears a scream<sep>Tail: PersonX sits on front porch\n\nSOLUTION:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.212890625, -0.212890625, -0.212890625, -0.212890625, -0.212890625], "model_raw_response": "{\n  \"analysis\": \"This task is challenging because it requires temporal and causal commonsense rather than simple pattern matching. The model must infer a likely order of events from short, underspecified phrases, handle placeholders and 'none' cases, and decide between only two labels ('Yes' or 'No') — which enforces a binary choice even when sequences are probabilistic or ambiguous. Success demands: (1) understanding of everyday cause–effect and typical event sequences, (2) reasoning about implicit durations and prerequisites, (3) handling ambiguity and choosing the most plausible ordering, and (4) following the strict output constraint to return only the specified tokens.\",\n  \"new_prompt\": \"Task: You will be given two brief event phrases separated by <sep>. PersonX is the subject in each event. Decide whether the first event (Head) occurs after the second event (Tail) in a typical/likely sequence of events. Answer only with 'Yes' or 'No' (no extra explanation). Problem: Head: PersonX hears a loud crash<sep>Tail: PersonX drops the glass they were holding\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires temporal and causal commonsense rather than simple pattern matching. The model must infer a likely order of events from short, underspecified phrases, handle placeholders and 'none' cases, and decide between only two labels ('Yes' or 'No') — which enforces a binary choice even when sequences are probabilistic or ambiguous. Success demands: (1) understanding of everyday cause–effect and typical event sequences, (2) reasoning about implicit durations and prerequisites, (3) handling ambiguity and choosing the most plausible ordering, and (4) following the strict output constraint to return only the specified tokens.", "new_prompt": "Task: You will be given two brief event phrases separated by <sep>. PersonX is the subject in each event. Decide whether the first event (Head) occurs after the second event (Tail) in a typical/likely sequence of events. Answer only with 'Yes' or 'No' (no extra explanation). Problem: Head: PersonX hears a loud crash<sep>Tail: PersonX drops the glass they were holding", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nshow me a example of code of Vagrantfile for hosting my wordpress website. I need kublr on a node. I need 1 master node and 1 worker node. The most secure configuration possible.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-1.890625, -5.25, 0.11376953125, 8.0, -3.421875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it mixes multiple deep technical domains and leaves many crucial details unspecified. It requires infrastructure-as-code (Vagrant) expertise, virtualization provider knowledge, Kubernetes (and specifically installing/working with Kublr), application deployment (WordPress), and strong security hardening (networking, TLS, firewall, SSH, kernel security). Success requires sequencing and idempotent provisioning, making safe assumptions about OS versions, ports, certificates and credentials, and producing a complete, runnable Vagrantfile with inline provisioning — all of which are nontrivial and easy for models to get wrong or leave ambiguous. The task tests system-administration skills, infrastructure orchestration, secure defaults and threat-aware design, plus clear specification of assumptions/placeholders.\",\n    \"new_prompt\": \"Write a single, self-contained Vagrantfile (with inline shell provisioning) that builds two Ubuntu 22.04 VirtualBox VMs: one control-plane node and one worker node. The provisioning must (1) install and initialize a hardened kubeadm Kubernetes cluster (secure defaults: RBAC enabled, anonymous access disabled, kubelet restricted, audit logging on, TLS bootstrapping), (2) install the Kublr agent or client on the designated node (include a configurable placeholder for any license/key), and (3) deploy WordPress onto the cluster with a persistent volume, HTTPS (self-signed certs are acceptable with clear placeholders), and minimal exposed ports. Make the Vagrantfile idempotent, use SSH key authentication for VM access, enable basic host firewall rules (only expose required ports), apply container-level security controls (resource requests/limits and a restrictive Pod Security or equivalent), and document any assumptions or placeholders inline as comments. Provide only the Vagrantfile contents (no extra examples) and ensure it is runnable on a machine with Vagrant and VirtualBox installed.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it mixes multiple deep technical domains and leaves many crucial details unspecified. It requires infrastructure-as-code (Vagrant) expertise, virtualization provider knowledge, Kubernetes (and specifically installing/working with Kublr), application deployment (WordPress), and strong security hardening (networking, TLS, firewall, SSH, kernel security). Success requires sequencing and idempotent provisioning, making safe assumptions about OS versions, ports, certificates and credentials, and producing a complete, runnable Vagrantfile with inline provisioning — all of which are nontrivial and easy for models to get wrong or leave ambiguous. The task tests system-administration skills, infrastructure orchestration, secure defaults and threat-aware design, plus clear specification of assumptions/placeholders.", "new_prompt": "Write a single, self-contained Vagrantfile (with inline shell provisioning) that builds two Ubuntu 22.04 VirtualBox VMs: one control-plane node and one worker node. The provisioning must (1) install and initialize a hardened kubeadm Kubernetes cluster (secure defaults: RBAC enabled, anonymous access disabled, kubelet restricted, audit logging on, TLS bootstrapping), (2) install the Kublr agent or client on the designated node (include a configurable placeholder for any license/key), and (3) deploy WordPress onto the cluster with a persistent volume, HTTPS (self-signed certs are acceptable with clear placeholders), and minimal exposed ports. Make the Vagrantfile idempotent, use SSH key authentication for VM access, enable basic host firewall rules (only expose required ports), apply container-level security controls (resource requests/limits and a restrictive Pod Security or equivalent), and document any assumptions or placeholders inline as comments. Provide only the Vagrantfile contents (no extra examples) and ensure it is runnable on a machine with Vagrant and VirtualBox installed.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given a sentence from the Quran in Persian, and your task is to translate it into English.\n--------\nQuestion: و پروردگار تو آمرزنده [و] صاحب رحمت است. اگر به [جرم‌] آنچه مرتکب شده‌اند، آنها را مؤاخذه مى‌کرد، قطعا در عذاب آنان تعجیل مى‌نمود [ولى چنین نمى کند] بلکه براى آنها سر رسیدى است که هرگز از برابر آن راه گریزى نمى‌یابند.\n\nAnswer: But your Lord is forgiving, full of benevolence. If He had pleased to punish them for their doings He would have punished them immediately. But a term is fixed for them from which they will find no escape.\n\n\nQuestion: و پیامبر [خدا] گفت: «پروردگارا، قوم من این قرآن را رها کردند.»\n\nAnswer: The prophet will say: \"O my Lord, my people had fettered the Qur'an.\"\n\n\nQuestion: با جامها و آبریزها و پیاله‌ [ها] یى از باده ناب روان.\n\nAnswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-3.765625, -4.40625, -3.765625, -3.765625, -4.90625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires accurate bilingual translation of a Quranic-style Persian sentence that uses archaic/poetic vocabulary, religiously sensitive terminology, and bracketed clarifications. The model must (1) parse Persian morphology and syntax (including ellipses and implied pronouns), (2) interpret bracketed insertions that indicate supplied or clarifying words, (3) choose an appropriate register in English (formal, reverent, sometimes archaic), and (4) preserve theological nuance and idiomatic meaning rather than producing a literal, awkward, or theologically misleading rendering. These factors combine to test deep domain knowledge, careful lexical choice, and fidelity to tone and nuance.\",\n    \"new_prompt\": \"Translate the following Persian sentence into natural, accurate English, preserving tone and any bracketed clarifications (render brackets as normal words in the translation): «پس اگر پروردگار [ما] مى‌خواست، به خاطر اعمالشان ایشان را فوراً مجازات مى‌کرد؛ امّا او آمرزنده و صاحبِ رحمت است و اجلى معیّن براى آنان مقرّر است که از آن راه گریزى نیست.»\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires accurate bilingual translation of a Quranic-style Persian sentence that uses archaic/poetic vocabulary, religiously sensitive terminology, and bracketed clarifications. The model must (1) parse Persian morphology and syntax (including ellipses and implied pronouns), (2) interpret bracketed insertions that indicate supplied or clarifying words, (3) choose an appropriate register in English (formal, reverent, sometimes archaic), and (4) preserve theological nuance and idiomatic meaning rather than producing a literal, awkward, or theologically misleading rendering. These factors combine to test deep domain knowledge, careful lexical choice, and fidelity to tone and nuance.", "new_prompt": "Translate the following Persian sentence into natural, accurate English, preserving tone and any bracketed clarifications (render brackets as normal words in the translation): «پس اگر پروردگار [ما] مى‌خواست، به خاطر اعمالشان ایشان را فوراً مجازات مى‌کرد؛ امّا او آمرزنده و صاحبِ رحمت است و اجلى معیّن براى آنان مقرّر است که از آن راه گریزى نیست.»", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nHow many beats are in a pick-up?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-1.8203125, 6.3125, 0.24609375, -3.109375, 4.875], "model_raw_response": "{\n    \"analysis\": \"The original prompt is challenging because the term \\\"pick-up\\\" (anacrusis) is ambiguous without context: its length depends on the piece's time signature and the composer's notation choices. Properly answering requires domain knowledge of Western music notation (what a pickup/anacrusis is), numerical reasoning about beats and time signatures, and the ability to resolve ambiguity or ask a clarifying question rather than giving a single fixed number.\",\n    \"new_prompt\": \"In a piece written in 4/4 time whose final bar contains only two beats, how many beats should the anacrusis (pickup) contain when notating the score?\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is challenging because the term \"pick-up\" (anacrusis) is ambiguous without context: its length depends on the piece's time signature and the composer's notation choices. Properly answering requires domain knowledge of Western music notation (what a pickup/anacrusis is), numerical reasoning about beats and time signatures, and the ability to resolve ambiguity or ask a clarifying question rather than giving a single fixed number.", "new_prompt": "In a piece written in 4/4 time whose final bar contains only two beats, how many beats should the anacrusis (pickup) contain when notating the score?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given a country name, and you need to return the numeric International Organization for Standardization (ISO) code of the given country. The codes are three-digit numbers defined by the ISO to represent countries, dependent territories, and special areas of geographical interest.\n\nInput: Consider Input: Norfolk Island\n\nOutput: 574\n\n\nInput: Consider Input: Cook Islands\n\nOutput: 184\n\n\nInput: Consider Input: Iraq<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-3.203125, -3.203125, -3.203125, -3.203125, -4.25], "model_raw_response": "{\n    \"analysis\": \"This prompt is difficult because it requires precise, authoritative domain knowledge (the ISO 3166-1 numeric codes) rather than general facts, plus exact formatting (a three-digit numeric string, often with leading zeros). Models commonly confuse numeric codes with alpha-2/alpha-3 codes, return the country name, or add explanatory text. Additional challenges include handling alternate and localized country names (e.g., Ivory Coast vs Côte d'Ivoire), dependencies and territories vs sovereign states, and punctuation/accents — all of which require correct canonicalization and an exact numeric lookup. The task therefore tests factual recall/lookup, normalization/disambiguation of input strings, and strict adherence to output formatting constraints.\",\n    \"new_prompt\": \"Using the ISO 3166-1 numeric standard, output only the three-digit numeric country code (include any leading zeros) that corresponds exactly to the following country or territory name. Do not add words, punctuation, or explanation—only the three digits. Country: Côte d'Ivoire\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is difficult because it requires precise, authoritative domain knowledge (the ISO 3166-1 numeric codes) rather than general facts, plus exact formatting (a three-digit numeric string, often with leading zeros). Models commonly confuse numeric codes with alpha-2/alpha-3 codes, return the country name, or add explanatory text. Additional challenges include handling alternate and localized country names (e.g., Ivory Coast vs Côte d'Ivoire), dependencies and territories vs sovereign states, and punctuation/accents — all of which require correct canonicalization and an exact numeric lookup. The task therefore tests factual recall/lookup, normalization/disambiguation of input strings, and strict adherence to output formatting constraints.", "new_prompt": "Using the ISO 3166-1 numeric standard, output only the three-digit numeric country code (include any leading zeros) that corresponds exactly to the following country or territory name. Do not add words, punctuation, or explanation—only the three digits. Country: Côte d'Ivoire", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nGenerate a story with a beginning, a middle, and an end using the following words:\nunicorn, castle, forest<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-1.765625, 2.328125, 1.9296875, 0.251953125, 0.421875], "model_raw_response": "{\n    \"analysis\": \"This prompt tests creative writing plus strict constraint-following. A model must (1) produce a coherent narrative with a clear beginning, middle, and end (narrative structure and logical progression), (2) include specific lexical items (the words 'unicorn', 'castle', 'forest') somewhere in the text (explicit inclusion constraint), and (3) integrate those words naturally into the story (semantic cohesion and creativity). Common failure modes: omitting one or more required words, failing to separate the story into three distinct sections or labeling them poorly, listing the words instead of weaving them into the narrative, or getting confused by markup tokens in the original prompt format. These require the model to balance creative generation with careful attention to explicit formatting and content constraints.\",\n    \"new_prompt\": \"Write a short story split into three clearly labeled sections — Beginning, Middle, and End — and make sure the words 'phoenix', 'lighthouse', and 'desert' each appear at least once; each section should advance the plot so the story feels cohesive rather than a set of unrelated scenes.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt tests creative writing plus strict constraint-following. A model must (1) produce a coherent narrative with a clear beginning, middle, and end (narrative structure and logical progression), (2) include specific lexical items (the words 'unicorn', 'castle', 'forest') somewhere in the text (explicit inclusion constraint), and (3) integrate those words naturally into the story (semantic cohesion and creativity). Common failure modes: omitting one or more required words, failing to separate the story into three distinct sections or labeling them poorly, listing the words instead of weaving them into the narrative, or getting confused by markup tokens in the original prompt format. These require the model to balance creative generation with careful attention to explicit formatting and content constraints.", "new_prompt": "Write a short story split into three clearly labeled sections — Beginning, Middle, and End — and make sure the words 'phoenix', 'lighthouse', and 'desert' each appear at least once; each section should advance the plot so the story feels cohesive rather than a set of unrelated scenes.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDefinition: In this task, you're given a text and question. The question is about the topic of the document. You are expected to generate an incorrect answer. The incorrect answer should be among one of the following categories: 1)Company, 2)Educational Institution, 3)Artist, 4)Athlete, 5)Office holder, 6)Mean of transportation, 7)Building, 8)Natural place, 9)Village, 10)Animal, 11)Plant, 12)Album, 13)Film, 14)Written work\nInput: Text:  The Silent Raid (Dutch: De Overval) is a 1962 Dutch war film directed by Paul Rotha. It shows a true story from World War II: the raid on Leeuwarden prison of December 8 1944. Without firing a shot Dutch resistance members disguised as German SD and their prisoners enter the prison free 39 prisoners and vanish into the city. The Germans were unable to find any of the organizers or escapees.It was entered into the 3rd Moscow International Film Festival.\nQuestion: WHat is the classified topic name\nOutput:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-11.3125, -12.25, -11.625, -8.75, -11.75], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it asks the model to do the opposite of its usual task: instead of giving the correct classification, it must deliberately produce an incorrect one. That combines several hard skills: precise comprehension of the passage to know the true topic; mapping that topic onto a fixed taxonomy of categories; obeying a negative constraint (choose a label that is NOT the true one); and formatting the answer exactly as requested. Models often fail here by (a) returning the correct label despite the instruction to be wrong, (b) producing an answer not in the allowed set, or (c) misunderstanding whether multiple labels might apply. The task therefore stresses instruction-following with negation, fine-grained taxonomy mapping, and attention to output constraints (single category name, chosen from the provided list).\",\n    \"new_prompt\": \"Instruction: From the list of categories below, select exactly one category that is NOT the correct label for the main topic described in the passage. Your answer must be exactly one of the listed category names (no extra words) and must be an incorrect classification (i.e., not the true category).\\n\\nCategories: Company; Educational Institution; Artist; Athlete; Office holder; Mean of transportation; Building; Natural place; Village; Animal; Plant; Album; Film; Written work\\n\\nText: The Grand Canyon is a steep-sided canyon carved by the Colorado River in northern Arizona, United States. It is renowned for its immense size, colorful rock layers that reveal geological history, and dramatic landscapes that attract millions of tourists and scientists each year.\\n\\nQuestion: Which single category from the list above is an incorrect classification for the topic of this text? Output only the category name.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it asks the model to do the opposite of its usual task: instead of giving the correct classification, it must deliberately produce an incorrect one. That combines several hard skills: precise comprehension of the passage to know the true topic; mapping that topic onto a fixed taxonomy of categories; obeying a negative constraint (choose a label that is NOT the true one); and formatting the answer exactly as requested. Models often fail here by (a) returning the correct label despite the instruction to be wrong, (b) producing an answer not in the allowed set, or (c) misunderstanding whether multiple labels might apply. The task therefore stresses instruction-following with negation, fine-grained taxonomy mapping, and attention to output constraints (single category name, chosen from the provided list).", "new_prompt": "Instruction: From the list of categories below, select exactly one category that is NOT the correct label for the main topic described in the passage. Your answer must be exactly one of the listed category names (no extra words) and must be an incorrect classification (i.e., not the true category).\n\nCategories: Company; Educational Institution; Artist; Athlete; Office holder; Mean of transportation; Building; Natural place; Village; Animal; Plant; Album; Film; Written work\n\nText: The Grand Canyon is a steep-sided canyon carved by the Colorado River in northern Arizona, United States. It is renowned for its immense size, colorful rock layers that reveal geological history, and dramatic landscapes that attract millions of tourists and scientists each year.\n\nQuestion: Which single category from the list above is an incorrect classification for the topic of this text? Output only the category name.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDetailed Instructions: In this task, you will be shown a correct English sentence. You need to generate an incorrect form of the input sentence.\nQ: i would have a variety of answers subjective to every person 's field of interest . \nA:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [4.96875, 2.09375, 0.41796875, 2.09375, -2.265625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it asks the model to intentionally produce an incorrect sentence — a negative constraint many models struggle with. It requires: (1) strong grammatical knowledge to identify what constitutes an ‘incorrect’ form, (2) creativity to introduce a natural-looking but clearly incorrect variation (not just random corruption), and (3) precise instruction-following so the model outputs a single incorrect sentence rather than a correction, an explanation, or a paraphrase. Ambiguity about what counts as “incorrect” (spelling vs. grammar vs. punctuation vs. semantics) also causes inconsistent model behavior.\",\n    \"new_prompt\": \"Given the correct sentence below, can you produce exactly one ungrammatical version that keeps the original meaning and most of the same words but contains at least one clear grammatical error (for example: wrong verb agreement, incorrect word order, missing article, or wrong preposition)? Output only the incorrect sentence (no explanations).\\n\\nSentence: Everyone brought their own tools to the workshop.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it asks the model to intentionally produce an incorrect sentence — a negative constraint many models struggle with. It requires: (1) strong grammatical knowledge to identify what constitutes an ‘incorrect’ form, (2) creativity to introduce a natural-looking but clearly incorrect variation (not just random corruption), and (3) precise instruction-following so the model outputs a single incorrect sentence rather than a correction, an explanation, or a paraphrase. Ambiguity about what counts as “incorrect” (spelling vs. grammar vs. punctuation vs. semantics) also causes inconsistent model behavior.", "new_prompt": "Given the correct sentence below, can you produce exactly one ungrammatical version that keeps the original meaning and most of the same words but contains at least one clear grammatical error (for example: wrong verb agreement, incorrect word order, missing article, or wrong preposition)? Output only the incorrect sentence (no explanations).\n\nSentence: Everyone brought their own tools to the workshop.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nA text is given in Malayalam. Translate it from the Malayalam language to the Oriya language. The translation must not omit or add information to the original sentence.\n\nExample input: SEମାନଙ୍କୁ ଗୁଣାତ୍ମକ ଉତ୍ପାଦନ ପାଇଁ ସକ୍ଷମ କରାଇବା\nExample output: ചോദ്യ ഉൽപ്പന്നത്തിലേക്ക്\nExample explanation: Correct translation for given sentence. Input sentence means 'To the query produce' which is the same as the output sentence.\nQ: ଏହି ଭାବନା ସହିତ ଆପଣ ସମସ୍ତଙ୍କୁ ହୃଦୟର ସହିତ ମୋର ବହୁତ ବହୁତ ଶୁଭକାମନା ।\nA:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-1.703125, -4.84375, -2.546875, -2.546875, -4.9375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it mixes several difficult skills: accurate identification of the source language and script (Malayalam vs. Odia/Oriya), low-resource bilingual translation between two distinct Indian languages and scripts, and strict constraint-following (the output must neither add nor omit information). Models can be confused by inconsistent examples or mislabeled instructions (the original instruction asked Malayalam→Oriya but the example/input was in Odia), which aggravates language-detection errors. The task also tests preservation of register, punctuation, and proper names, and demands precise, literal semantic equivalence rather than loose paraphrase—requirements that many models struggle with for less-common language pairs.\",\n    \"new_prompt\": \"You will be given a single sentence written in Odia (Oriya script). Translate that sentence into Malayalam. The translation must preserve exactly the same information and level of formality; do not add, remove, or explain anything. Preserve punctuation and proper names. Output only the Malayalam sentence and nothing else.\\nSentence: ଆପଣଙ୍କର ସହଯୋଗ ଓ ଉତ୍ସାହ ପାଇଁ ମୁଁ ହୃଦୟରୁ କୃତଜ୍ଞତା ଜଣାଉଛି।\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it mixes several difficult skills: accurate identification of the source language and script (Malayalam vs. Odia/Oriya), low-resource bilingual translation between two distinct Indian languages and scripts, and strict constraint-following (the output must neither add nor omit information). Models can be confused by inconsistent examples or mislabeled instructions (the original instruction asked Malayalam→Oriya but the example/input was in Odia), which aggravates language-detection errors. The task also tests preservation of register, punctuation, and proper names, and demands precise, literal semantic equivalence rather than loose paraphrase—requirements that many models struggle with for less-common language pairs.", "new_prompt": "You will be given a single sentence written in Odia (Oriya script). Translate that sentence into Malayalam. The translation must preserve exactly the same information and level of formality; do not add, remove, or explain anything. Preserve punctuation and proper names. Output only the Malayalam sentence and nothing else.\nSentence: ଆପଣଙ୍କର ସହଯୋଗ ଓ ଉତ୍ସାହ ପାଇଁ ମୁଁ ହୃଦୟରୁ କୃତଜ୍ଞତା ଜଣାଉଛି।", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDetailed Instructions: The input is taken from a negotiation between two participants who take the role of campsite neighbors and negotiate for Food, Water, and Firewood packages, based on their individual preferences and requirements. Given an utterance and recent dialogue context containing past 3 utterances (wherever available), output Yes if the utterance contains the elicit-pref strategy, otherwise output No. elicit-pref is a cooperative negotiation strategy. It is used to discover the preference order of the opponent in the negotiation.\nQ: Context: 'Hi how are you' 'I'm doing great. How about you.?' 'Really good.'\nUtterance: 'Which option you would like to go for?'\nA:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-26.875, -26.875, -6.3125, -26.875, -26.875], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires fine-grained pragmatic and intent understanding rather than surface-level keyword spotting. The model must recognize a specific negotiation tactic (elicit-pref) — asking to discover the opponent's preference order — which can be expressed in many paraphrases, implicit formulations, or ambiguous short utterances. It tests discourse-level reasoning (using short dialogue context), intent classification, and distinguishing elicit-pref from other question types (e.g., clarification, logistical questions, or proposals). Models often fail because the signal is subtle, relies on world/negotiation knowledge, and requires mapping varied linguistic forms to the same strategic category.\",\n    \"new_prompt\": \"Detailed Instructions: You will be given a brief negotiation context (up to three prior utterances) between two campsite neighbors deciding among three packages: Food, Water, and Firewood. Decide whether the target utterance uses the elicit-pref strategy — that is, it seeks to uncover the opponent’s preference ordering or priorities among the available options. Output exactly Yes if it does, otherwise output exactly No. Q: Context: 'Hey — excited for the trip?' 'Absolutely, been packing all morning.' 'I hope we can sort supplies quickly.' Utterance: 'If you could only take two of the three (Food, Water, Firewood), which two would you pick and why?' A:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires fine-grained pragmatic and intent understanding rather than surface-level keyword spotting. The model must recognize a specific negotiation tactic (elicit-pref) — asking to discover the opponent's preference order — which can be expressed in many paraphrases, implicit formulations, or ambiguous short utterances. It tests discourse-level reasoning (using short dialogue context), intent classification, and distinguishing elicit-pref from other question types (e.g., clarification, logistical questions, or proposals). Models often fail because the signal is subtle, relies on world/negotiation knowledge, and requires mapping varied linguistic forms to the same strategic category.", "new_prompt": "Detailed Instructions: You will be given a brief negotiation context (up to three prior utterances) between two campsite neighbors deciding among three packages: Food, Water, and Firewood. Decide whether the target utterance uses the elicit-pref strategy — that is, it seeks to uncover the opponent’s preference ordering or priorities among the available options. Output exactly Yes if it does, otherwise output exactly No. Q: Context: 'Hey — excited for the trip?' 'Absolutely, been packing all morning.' 'I hope we can sort supplies quickly.' Utterance: 'If you could only take two of the three (Food, Water, Firewood), which two would you pick and why?' A:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDetailed Instructions: In this task, you are given a sentence or phrase in English. You must translate it to Xhosa in a way that is equivalent in terms of meaning and grammatically correct.\nSee one example below:\nProblem: Rope and its Usage\nSolution: Intambo nomsebenzi ewenzayo.\nExplanation: 'Intambo' means rope, 'nomsebenzi ewenzayo' roughly translates to 'the work it does', which is a native way of phrasing it.\n\nProblem: It was tempting to predict from this trend that tankers having deadweights of several million tons would be built.\nSolution:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [0.33984375, -5.4375, 4.3125, -0.357421875, 0.333984375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires accurate, idiomatic translation of a syntactically and semantically dense English sentence containing technical maritime vocabulary and numeric/measurement concepts. Specific skills tested: domain-aware vocabulary choice (e.g., how to render 'tanker', 'deadweights', 'tons' or 'tonnes' and whether to use a technical calque like 'deadweight tonnage'), handling of modality and epistemic nuance ('It was tempting to predict' — conveying the tentative, evaluative tone), preserving tense/aspect and passive/impersonal constructions in a language with different morphosyntactic patterns (Xhosa noun classes and agreement), and producing a natural native phrasing rather than a word-for-word literal output. Models commonly fail here by mistranslating technical terms, losing the tentative nuance (turning 'was tempting to predict' into an assertive claim), producing ungrammatical agreement in Xhosa, or inserting inappropriate calques instead of idiomatic expressions.\",\n    \"new_prompt\": \"Translate the following English sentence into Xhosa in a way that is faithful in meaning, grammatical, and idiomatic; provide only the Xhosa translation: 'Given the observed pattern, it seemed reasonable to infer that cargo ships with carrying capacities measured in the millions of tonnes would eventually be built.'\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires accurate, idiomatic translation of a syntactically and semantically dense English sentence containing technical maritime vocabulary and numeric/measurement concepts. Specific skills tested: domain-aware vocabulary choice (e.g., how to render 'tanker', 'deadweights', 'tons' or 'tonnes' and whether to use a technical calque like 'deadweight tonnage'), handling of modality and epistemic nuance ('It was tempting to predict' — conveying the tentative, evaluative tone), preserving tense/aspect and passive/impersonal constructions in a language with different morphosyntactic patterns (Xhosa noun classes and agreement), and producing a natural native phrasing rather than a word-for-word literal output. Models commonly fail here by mistranslating technical terms, losing the tentative nuance (turning 'was tempting to predict' into an assertive claim), producing ungrammatical agreement in Xhosa, or inserting inappropriate calques instead of idiomatic expressions.", "new_prompt": "Translate the following English sentence into Xhosa in a way that is faithful in meaning, grammatical, and idiomatic; provide only the Xhosa translation: 'Given the observed pattern, it seemed reasonable to infer that cargo ships with carrying capacities measured in the millions of tonnes would eventually be built.'", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nTeacher: In this task, you are given an input list A. You need to extract and sort the unique digits used in the list in ascending order. Return -1 if there is no digit in the list.\nTeacher: Now, understand the problem? If you are still confused, see the following example:\n['q', '31', 'a', 'd', '53', '85', 'p', '77']\nSolution: 1, 3, 5, 7, 8\nReason: Here, the numbers in the list are '31', '53', '85' and '77', and the unique digits used in the list are '1, 3, 5, 7, 8' in ascending order.\n\nNow, solve this instance: ['117', '417', '189', 'l', '87', '251', 'y', '497', '37']\nStudent:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-3.375, -8.5, -8.5, -5.6875, -8.5], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it combines several subtle skills: precise token classification (distinguishing purely numeric strings from non-numeric or alphanumeric tokens), character-level string parsing (extracting digits from multi-digit numeric strings), set operations (deduplication of digits), and deterministic ordering/formatting of the result. Models often fail either by mishandling mixed tokens (e.g., treating 'a1' as numeric), by splitting numeric strings incorrectly, by forgetting to deduplicate, or by producing the wrong output format or placeholder (e.g., not returning -1 when no digits exist). It tests attention to instructions, edge-case handling (leading zeros, multi-digit numbers), and exact output formatting.\",\n    \"new_prompt\": \"You are given a list of items that may include single letters, alphanumeric tokens, and strings composed only of digits. Extract every distinct digit character that appears in elements that are purely numeric (ignore any item that contains non-digit characters). Return the unique digits in ascending order, separated by commas and single spaces (for example: \\\"1, 2, 3\\\"). If there are no digit-only items in the list, return -1. Now solve this instance: ['304', 'b', '12a', '90', '007', 'x', '5']\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it combines several subtle skills: precise token classification (distinguishing purely numeric strings from non-numeric or alphanumeric tokens), character-level string parsing (extracting digits from multi-digit numeric strings), set operations (deduplication of digits), and deterministic ordering/formatting of the result. Models often fail either by mishandling mixed tokens (e.g., treating 'a1' as numeric), by splitting numeric strings incorrectly, by forgetting to deduplicate, or by producing the wrong output format or placeholder (e.g., not returning -1 when no digits exist). It tests attention to instructions, edge-case handling (leading zeros, multi-digit numbers), and exact output formatting.", "new_prompt": "You are given a list of items that may include single letters, alphanumeric tokens, and strings composed only of digits. Extract every distinct digit character that appears in elements that are purely numeric (ignore any item that contains non-digit characters). Return the unique digits in ascending order, separated by commas and single spaces (for example: \"1, 2, 3\"). If there are no digit-only items in the list, return -1. Now solve this instance: ['304', 'b', '12a', '90', '007', 'x', '5']", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nYou are given a paragraph, with numbered sentences, and a question. Write the numbers of the sentences needed to answer the given question. Separate multiple sentence numbers with a comma (\", \"). E.g. \"1, 2\" would be the correct response if Sent 1 and Sent 2 are needed to answer the given question (do not include sentences that contain no information that is necessary to answer the question). Sort the sentence numbers in an increasing order (i.e., '1, 2', instead of '2, 1'). Your answer should not contain anything than the sentence numbers separated by a comma (e.g. '1, 2, 3, 4'). The given question requires more than one sentence to answer. So, your answer must contain at least one comma (\", \").\n\nParagraph- Sent 1: After becoming disabled in a machete attack on a visit to his native Haiti, Jean-Claude Joseph needed help persuading his landlord to move him from a fifth-floor apartment to one on the ground floor.\nSent 2: Isaac Benjamin became ensnared in a bureaucratic snafu that took away his Social Security disability payments for more than two years.\nSent 3: The story of Martha, a woman from Sierra Leone, was more compelling.\nSent 4: Beaten, raped and tortured in her politically repressive homeland, she knowingly used someone else's passport to escape to America, but was caught by immigration authorities upon her arrival.\nSent 5: She desperately sought political asylum.\nSent 6: Not the kind of cases that lead to ground-breaking upheavals in the law, but the kind of cases that are handled day in and day out by lawyers for the legally disenfranchised who have no where else to turn.\nSent 7: The work of attorneys from Legal Services of New Jersey will be highlighted in a onehour documentary, \"Quest for Justice,\" to be aired 9 p.m.\nSent 8: today on New Jersey Network.\nSent 9: Produced by NYD2, a communications firm based in Somerset, the documentary features case histories of clients whose needs ranged from housing to fighting off deportation.\nSent 10: Joseph, a 54-year-old naturalized citizen, turned to Legal Services when the landlord of his federally subsidized apartment complex in Elizabeth turned a deaf ear to his request for a ground-floor apartment.\nSent 11: Having lost the use of his left arm in warding off the machete attack during a robbery attempt, Joseph said he found it increasingly difficult to negotiate the five flights of stairs lugging groceries or laundry on the frequent occasions when the building's elevator was out of order.\nSent 12: \"With this, it became impossible for me to stay upstairs,\" he said, pointing to the scars on his forearm.\nSent 13: \"If I cannot carry my groceries or my laundry, how can I live?\"Sent 14: \"It was a compelling case,\" said Legal Services attorney Stephen St. Hilaire.\nSent 15: \"The key for us -- and we have to make tough decisions all the time on whether to take a case -- was visualizing what he had to do to get to the fifth floor, struggling with a bag of groceries,\" he said.\nSent 16: Benjamin, 53, of Jersey City had been collecting Social Security disability after undergoing double bypass surgery when the checks stopped coming.\nSent 17: He said the agency claimed he had failed to return a form updating the condition of his health.\nSent 18: \"But what got me was they didn't let me know they didn't get it, they just cut me off,\" he said, adding he found it impossible to negotiate the Social Security bureaucracy himself. \nQuestion: Why did native Haiti Jean-Claude lose his hand?.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.625, 1.625, 1.4921875, 1.625, 1.4921875], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it combines fine-grained reading comprehension with strict output-format constraints. The model must (1) identify exactly which sentences contain information that is necessary (and exclude irrelevant sentences), (2) sometimes integrate information spread across multiple sentences to reach the answer, and (3) obey negative/formatting constraints (only output ascending sentence numbers, separated by a comma and a space, with nothing else, and include at least one comma because the question requires more than one sentence). Failures typically come from returning extra text, picking too few sentences, misordering numbers, using wrong separators or labels, or misunderstanding the causal/implicit links across sentences.\",\n    \"new_prompt\": \"You are given a short paragraph with numbered sentences and a question. Write only the numbers of the sentences that are needed to answer the question. Separate multiple sentence numbers with a comma and a space (\\\", \\\"). Do not include any words, punctuation, or commentary other than the sentence numbers. Sort the sentence numbers in increasing order. Do not include sentences that contain information not necessary to answer the question. The question requires more than one sentence to answer, so your response must contain at least one comma.\\n\\nParagraph- Sent 1: Maria, a seamstress, had been experiencing headaches and dizziness after the fire at the textile mill. Sent 2: At the hospital she underwent an emergency procedure to remove debris lodged near her right eye. Sent 3: The surgeon described the operation as complicated because fragments had penetrated the eye socket. Sent 4: After the operation an infection developed that medical staff said damaged the optic nerve. Sent 5: She now has very limited vision in that eye and must use a cane in unfamiliar places. Sent 6: Her coworkers held a small fundraiser to help with medical bills.\\n\\nQuestion: Why did Maria lose vision in her right eye?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it combines fine-grained reading comprehension with strict output-format constraints. The model must (1) identify exactly which sentences contain information that is necessary (and exclude irrelevant sentences), (2) sometimes integrate information spread across multiple sentences to reach the answer, and (3) obey negative/formatting constraints (only output ascending sentence numbers, separated by a comma and a space, with nothing else, and include at least one comma because the question requires more than one sentence). Failures typically come from returning extra text, picking too few sentences, misordering numbers, using wrong separators or labels, or misunderstanding the causal/implicit links across sentences.", "new_prompt": "You are given a short paragraph with numbered sentences and a question. Write only the numbers of the sentences that are needed to answer the question. Separate multiple sentence numbers with a comma and a space (\", \"). Do not include any words, punctuation, or commentary other than the sentence numbers. Sort the sentence numbers in increasing order. Do not include sentences that contain information not necessary to answer the question. The question requires more than one sentence to answer, so your response must contain at least one comma.\n\nParagraph- Sent 1: Maria, a seamstress, had been experiencing headaches and dizziness after the fire at the textile mill. Sent 2: At the hospital she underwent an emergency procedure to remove debris lodged near her right eye. Sent 3: The surgeon described the operation as complicated because fragments had penetrated the eye socket. Sent 4: After the operation an infection developed that medical staff said damaged the optic nerve. Sent 5: She now has very limited vision in that eye and must use a cane in unfamiliar places. Sent 6: Her coworkers held a small fundraiser to help with medical bills.\n\nQuestion: Why did Maria lose vision in her right eye?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given a hateful post in Bengali that expresses hate or encourages violence towards a person or a group based on the protected characteristics such as race, religion, sex, and sexual orientation. You are expected to classify the post into four classes: Religious, Political, Geopolitical and Personal depending on the topic.\n\n[Q]: সব মিমারদের চরিত্র খারাপ, তাতে আমার বাল ছিড়া গেছে \n[A]: Personal\n\n\n[Q]: ৫ জানুয়ারি নির্বাচনে ডুগডুগির তালে নেচেছে নির্বাচন কমিশন\n[A]: Geopolitical\n\n\n[Q]: তোর মার হেডা।মদারচোদ কোথাকার।তোরে ভারতীয় টিম থেকে লাথি মেরে বাহির করেছে তখন তুই বাঙালি বলে দূঃখ পাইছিলাম।এখন বুজছি সঠিকই করেছে।\n[A]:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.03125, -3.25, -4.53125, 6.125, 3.5625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines multilingual understanding (Bengali) with fine-grained hate-speech topic classification. The model must (1) comprehend colloquial and abusive Bengali, (2) detect whether the target is a protected characteristic or an individual, and (3) disambiguate closely related labels (e.g., Political vs. Geopolitical vs. Religious) based on contextual cues rather than surface insults. It tests robust language comprehension, sensitive content detection, and precise label selection under potential ambiguity.\",\n    \"new_prompt\": \"You will be given a single Bengali message that expresses hatred or advocates harm toward people/groups based on protected characteristics (e.g., religion, nationality, political affiliation, ethnicity). Determine which one of these four topical categories best describes the message: Religious, Political, Geopolitical, or Personal. Reply with exactly one label (choose only from: Religious, Political, Geopolitical, Personal).\\n\\nClassify this message:\\n\\n\\\"এই মুসলমানরা আমাদের দেশের নৈতিকতা নষ্ট করছে; তাদের এখানে থাকা বন্ধ করা দরকার।\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines multilingual understanding (Bengali) with fine-grained hate-speech topic classification. The model must (1) comprehend colloquial and abusive Bengali, (2) detect whether the target is a protected characteristic or an individual, and (3) disambiguate closely related labels (e.g., Political vs. Geopolitical vs. Religious) based on contextual cues rather than surface insults. It tests robust language comprehension, sensitive content detection, and precise label selection under potential ambiguity.", "new_prompt": "You will be given a single Bengali message that expresses hatred or advocates harm toward people/groups based on protected characteristics (e.g., religion, nationality, political affiliation, ethnicity). Determine which one of these four topical categories best describes the message: Religious, Political, Geopolitical, or Personal. Reply with exactly one label (choose only from: Religious, Political, Geopolitical, Personal).\n\nClassify this message:\n\n\"এই মুসলমানরা আমাদের দেশের নৈতিকতা নষ্ট করছে; তাদের এখানে থাকা বন্ধ করা দরকার।\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nQ: Does \"shade\" have the same meaning in the following two sentences?\n\nAn achievement that puts everything else in the shade.\n\nHis brother's success left him in the shade.\n\nA: the same meaning\n\n\nQ: Does \"stretch\" have the same meaning in the following two sentences?\n\nA stretch of clear water.\n\nHe did a stretch in the federal penitentiary.\n\nA: different meanings\n\n\nQ: Does \"connect\" have the same meaning in the following two sentences?\n\nThe new railroad will connect the northern part of the state to the southern part.\n\nCan you connect the two loudspeakers?\n\nA:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-5.4375, -5.4375, -5.4375, -5.4375, -5.4375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires fine-grained word-sense disambiguation (distinguishing polysemous meanings), detection of idiomatic vs literal uses, and pragmatic inference from minimal context. The model must map the sense of a target word across two different sentences and decide whether those senses are equivalent, which tests lexical semantics, idiom recognition, and reasoning beyond surface word overlap. Models often fail here because embeddings and surface cues can conflate different senses of the same lemma, and because binary judgments ('same meaning' vs 'different meanings') demand precise semantic comparison rather than approximate similarity.\",\n    \"new_prompt\": \"Q: Does \\\"handle\\\" have the same meaning in the following two sentences?\\n\\nShe couldn't handle the pressure of the job.\\n\\nHe handed the fragile vase to me and asked me to handle it carefully.\\n\\nA:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires fine-grained word-sense disambiguation (distinguishing polysemous meanings), detection of idiomatic vs literal uses, and pragmatic inference from minimal context. The model must map the sense of a target word across two different sentences and decide whether those senses are equivalent, which tests lexical semantics, idiom recognition, and reasoning beyond surface word overlap. Models often fail here because embeddings and surface cues can conflate different senses of the same lemma, and because binary judgments ('same meaning' vs 'different meanings') demand precise semantic comparison rather than approximate similarity.", "new_prompt": "Q: Does \"handle\" have the same meaning in the following two sentences?\n\nShe couldn't handle the pressure of the job.\n\nHe handed the fragile vase to me and asked me to handle it carefully.\n\nA:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nI will give you multiple information to give you context on what i need you to help me.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.8671875, 1.8671875, -0.42578125, 1.8515625, -0.42578125], "model_raw_response": "{\n    \"analysis\": \"The original prompt is highly ambiguous and underspecified: the user only says they will provide “multiple information” but gives no goal, expected output, or constraints. This tests the model's ability to handle incomplete instructions (ask clarifying questions), manage multi-turn context (accumulate and integrate multiple pieces of input), extract and prioritize required facts, and define a clear plan/output format. It also examines conversational skills (when to ask vs. proceed), error handling for missing/contradictory data, and following implicit structural cues in nonstandard formatting.\",\n    \"new_prompt\": \"I am going to send you several separate messages that together describe a task I need done; before I send them, do the following in your reply: (a) list the specific facts and data points you will need from me to understand and complete the task, (b) provide the clarifying questions you would ask if any of those facts are missing or ambiguous, (c) explain how you would combine the pieces into a step-by-step plan and which output formats you can deliver, and (d) describe how you will handle missing or contradictory information.\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is highly ambiguous and underspecified: the user only says they will provide “multiple information” but gives no goal, expected output, or constraints. This tests the model's ability to handle incomplete instructions (ask clarifying questions), manage multi-turn context (accumulate and integrate multiple pieces of input), extract and prioritize required facts, and define a clear plan/output format. It also examines conversational skills (when to ask vs. proceed), error handling for missing/contradictory data, and following implicit structural cues in nonstandard formatting.", "new_prompt": "I am going to send you several separate messages that together describe a task I need done; before I send them, do the following in your reply: (a) list the specific facts and data points you will need from me to understand and complete the task, (b) provide the clarifying questions you would ask if any of those facts are missing or ambiguous, (c) explain how you would combine the pieces into a step-by-step plan and which output formats you can deliver, and (d) describe how you will handle missing or contradictory information.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nExtract the answer to the question from the following context. Question: Who stopped using the name Near East? Context: Near East (French: Proche-Orient) is a geographical term that roughly encompasses Western Asia. Despite having varying definitions within different academic circles, the term was originally applied to the maximum extent of the Ottoman Empire. The term has fallen into disuse in English, and has been replaced by the term Middle East.\nAns: English\n\nExtract the answer to the question from the following context. Question: If they were not Italian, what were Rome's allies? Context: Rome's military confederation with the other peoples of the Italian peninsula meant that half of Rome's army was provided by the Socii, such as the Etruscans, Umbrians, Apulians, Campanians, Samnites, Lucani, Bruttii, and the various southern Greek cities. Polybius states that Rome could draw on 770,000 men at the beginning of the Second Punic War, of which 700,000 were infantry and 70,000 met the requirements for cavalry. Rome's Italian allies would be organized in alae, or wings, roughly equal in manpower to the Roman legions, though with 900 cavalry instead of 300.\nAns: Greek\n\nExtract the answer to the question from the following context. Question: Which city was mentioned third? Context: In the wake of the United States' economic downturn and the legalization of gambling in adjacent and nearby states (including Delaware, Maryland, New York, and Pennsylvania), four casino closures took place in 2014: the Atlantic Club on January 13; the Showboat on August 31; the Revel, which was Atlantic City's second-newest casino, on September 2; and Trump Plaza, which originally opened in 1984, and was the poorest performing casino in the city, on September 16.\nAns: New York\n\nExtract the answer to the question from the following context. Question: What vehicles did aircraft carriers have on them? Context: The aircraft carrier dramatically changed naval combat in World War II, because air power was becoming a significant factor in warfare. The advent of aircraft as focal weapons was driven by the superior range, flexibility and effectiveness of carrier-launched aircraft. They had higher range and precision than naval guns, making them highly effective. The versatility of the carrier was demonstrated in November 1940 when HMS Illustrious launched a long-range strike on the Italian fleet at their base in Taranto, signalling the beginning of the effective and highly mobile aircraft strikes. This operation incapacitated three of the six battleships at a cost of two torpedo bombers. World War II in the Pacific Ocean involved clashes between aircraft carrier fleets. The 1941 Japanese surprise attack on Pearl Harbor was a clear illustration of the power projection capability afforded by a large force of modern carriers. Concentrating six carriers in a single unit turned naval history about, as no other nation had fielded anything comparable. However, the vulnerability of carriers compared to traditional battleships when forced into a gun-range encounter was quickly illustrated by the sinking of HMS Glorious by German battleships during the Norwegian campaign in 1940.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-2.71875, -2.71875, -1.8125, -2.71875, -2.71875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it bundles several short extractive-QA tasks that require different micro-skills: precise span extraction from context, coreference resolution (e.g., mapping pronouns like \\\"they\\\" back to their antecedents), semantic-type mapping (answering a \\\"Who\\\" question with a non-person category such as a language or publications), list-order reasoning (identifying the third item in a list), and picking the correct level of granularity for answers (\\\"aircraft\\\" vs. \\\"torpedo bombers\\\"). The concatenation of multiple Q–context pairs in one prompt also demands strict attention to task boundaries so the model extracts the answer for the right question, and it must handle varied answer formats (single-word labels, group names, nouns). Failures typically come from mis-resolving coreference, preferring implausible answer types, or being confused by multiple adjacent examples.\",\n    \"new_prompt\": \"Extract the answer to the question from the following context. Question: Who stopped using the name Farlands? Context: The term \\\"Farlands\\\" was traditionally applied to the remote provinces beyond the old maritime empire. Over the last century the designation has largely fallen into disuse in English-language scholarship and has been superseded by the phrase \\\"Outer Regions.\\\" By contrast, some historians publishing in French and German still occasionally use the older name. Ans:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it bundles several short extractive-QA tasks that require different micro-skills: precise span extraction from context, coreference resolution (e.g., mapping pronouns like \"they\" back to their antecedents), semantic-type mapping (answering a \"Who\" question with a non-person category such as a language or publications), list-order reasoning (identifying the third item in a list), and picking the correct level of granularity for answers (\"aircraft\" vs. \"torpedo bombers\"). The concatenation of multiple Q–context pairs in one prompt also demands strict attention to task boundaries so the model extracts the answer for the right question, and it must handle varied answer formats (single-word labels, group names, nouns). Failures typically come from mis-resolving coreference, preferring implausible answer types, or being confused by multiple adjacent examples.", "new_prompt": "Extract the answer to the question from the following context. Question: Who stopped using the name Farlands? Context: The term \"Farlands\" was traditionally applied to the remote provinces beyond the old maritime empire. Over the last century the designation has largely fallen into disuse in English-language scholarship and has been superseded by the phrase \"Outer Regions.\" By contrast, some historians publishing in French and German still occasionally use the older name. Ans:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nWrite a scoping document which answers the questions of<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.359375, 0.40234375, 1.015625, 0.8359375, 1.6640625], "model_raw_response": "{\n    \"analysis\": \"The original prompt is challenging because it's extremely underspecified and abruptly truncated, forcing the model to decide whether to ask clarifying questions, make assumptions, or produce something based on guesswork. It tests skills in interpreting ambiguous input, structured project scoping (objectives, scope boundaries, metrics, milestones, resources, risks, deliverables), explicit assumption-making, and the ability to generate useful clarifying questions. It also evaluates the model's ability to balance completeness and concision while handling incomplete instructions and taking a project-lead perspective.\",\n    \"new_prompt\": \"A stakeholder hands you a one-line brief: 'Increase platform usage.' As the project lead, can you produce a self-contained scoping document for a six-month pilot that (a) states the project objective and scope boundaries, (b) identifies target user segments and success metrics, (c) outlines a high-level approach and key milestones with timeline, (d) lists required resources and roles, (e) documents major risks and mitigations, (f) specifies expected deliverables and acceptance criteria, (g) explicitly enumerates any assumptions you made, and (h) provides at least eight clarifying questions you would ask the stakeholder?\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is challenging because it's extremely underspecified and abruptly truncated, forcing the model to decide whether to ask clarifying questions, make assumptions, or produce something based on guesswork. It tests skills in interpreting ambiguous input, structured project scoping (objectives, scope boundaries, metrics, milestones, resources, risks, deliverables), explicit assumption-making, and the ability to generate useful clarifying questions. It also evaluates the model's ability to balance completeness and concision while handling incomplete instructions and taking a project-lead perspective.", "new_prompt": "A stakeholder hands you a one-line brief: 'Increase platform usage.' As the project lead, can you produce a self-contained scoping document for a six-month pilot that (a) states the project objective and scope boundaries, (b) identifies target user segments and success metrics, (c) outlines a high-level approach and key milestones with timeline, (d) lists required resources and roles, (e) documents major risks and mitigations, (f) specifies expected deliverables and acceptance criteria, (g) explicitly enumerates any assumptions you made, and (h) provides at least eight clarifying questions you would ask the stakeholder?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDetailed Instructions: A text is given in Bengali. Translate it from the Bengali language to the Gujarati language. The translation must not omit or add information to the original sentence.\nSee one example below:\nProblem: આપણને જે બહુમૂલ્ય લોકતંત્ર મળ્યું છે તેને આપણે બહુ જ સરળતાથી granted માની લઈએ છીએ પરંતુ, આપણે સ્વયં એ યાદ અપાવતા રહેવું જોઈએ કે આપણું લોકતંત્ર બહુ જ મહાન છે અને આ લોકતંત્રને આપણી નસેનસમાં જગ્યા મળી છે – સદીઓની સાધનાથી, પેઢી દર પેઢીના સંસ્કારોથી, એક વિશાળ વ્યાપક મનની અવસ્થાથી\nSolution: আমরা বিশ্বাস করি যে আমরা একটি খুব সহজে মঞ্জুর পেয়েছি, কিন্তু আমাদের নিজেকে স্মরণ করিয়ে দিতে হবে যে, আমাদের গণতন্ত্র খুব বড় এবং এই গণতন্ত্র আমাদের নাসেনের মধ্যে একটি স্থান পেয়েছে - প্রজন্মের প্রজন্মের প্রজন্ম থেকে, একটি বিশাল ব্যাপক মন থেকে\nExplanation: Correct translation for given sentence. Input sentence means 'We believe that we have received a very easily granted, but we should remind yourself that our democracy is very great and this democracy has got a place in our nassen - from the centuries, from the generation of generation, a hugeComprehensive mind' which is the same as the output sentence.\n\nProblem: સરકાર દેશભરમાં નાગરિકોને સ્વચ્છ ઇંધણની ઉપલબ્ધતા વધારવા સિટી ગેસ ડિસ્ટ્રિબ્યુશન (સીજીડી) નેટવર્કનો વ્યાપ વધારવા પર ભાર મૂકી રહી છે.\nSolution:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.78125, -10.5625, -10.5625, -1.6328125, -5.84375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires precise cross-script, cross-language translation (Bengali↔Gujarati) while strictly preserving meaning, punctuation, numbers, acronyms, and any parenthetical content. It tests: accurate language identification and script conversion; handling of technical terms and acronyms without introducing or dropping information; careful fidelity to the source (no addition/omission or stylistic 'clarification'); and correct rendering of named entities and numeric targets. Models often fail here due to ambiguous or inconsistent examples (mixed-language examples in the original), difficulty mapping domain-specific vocabulary, and a tendency to normalize or paraphrase rather than perform literal, information-preserving translation.\",\n    \"new_prompt\": \"Instruction: A single sentence is given in Gujarati. Translate it into Bengali exactly—do not add, omit, or alter information; preserve all numbers, punctuation, parentheses, and acronyms exactly as they appear. Provide only the translated sentence (no explanations). Sentence: સરકાર 2030 સુધી રિન્યુએબલ ક્ષમતા 40% સુધી વધારવા માટે નવી સૌર ઊર્જા યોજનાઓને પ્રોત્સાહન આપી રહી છે; નાગરિકોને સસ્તી સોલર લોન (SFS - Solar Financing Scheme) અને ટેક્નિકલ સહાય આપવામાં આવશે.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires precise cross-script, cross-language translation (Bengali↔Gujarati) while strictly preserving meaning, punctuation, numbers, acronyms, and any parenthetical content. It tests: accurate language identification and script conversion; handling of technical terms and acronyms without introducing or dropping information; careful fidelity to the source (no addition/omission or stylistic 'clarification'); and correct rendering of named entities and numeric targets. Models often fail here due to ambiguous or inconsistent examples (mixed-language examples in the original), difficulty mapping domain-specific vocabulary, and a tendency to normalize or paraphrase rather than perform literal, information-preserving translation.", "new_prompt": "Instruction: A single sentence is given in Gujarati. Translate it into Bengali exactly—do not add, omit, or alter information; preserve all numbers, punctuation, parentheses, and acronyms exactly as they appear. Provide only the translated sentence (no explanations). Sentence: સરકાર 2030 સુધી રિન્યુએબલ ક્ષમતા 40% સુધી વધારવા માટે નવી સૌર ઊર્જા યોજનાઓને પ્રોત્સાહન આપી રહી છે; નાગરિકોને સસ્તી સોલર લોન (SFS - Solar Financing Scheme) અને ટેક્નિકલ સહાય આપવામાં આવશે.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you're given a pair of sentences, sentence 1 and sentence 2, that neither agree with nor contradict each other. Your job is to alter sentence 2 so that the pair clearly contradict each other. Generated sentences must be short, with less than 15 words. New information can be introduced. Avoid using pronouns to confuse the subject of the sentence.\n\nEx Input:\nSentence 1: Several people walk through a crowded Asian city. Sentence 2: A tourist group is touring the city with their tour guide.\n\nEx Output:\nThere are several people in this photo, and they are all outside.\n\n\nEx Input:\nSentence 1: A man in glasses looks at his electronic device. Sentence 2: A tall person looking\n\nEx Output:\nA human looking.\n\n\nEx Input:\nSentence 1: A man is taking a picture with a camera. Sentence 2: The man is taking a picture of the beach.\n\nEx Output:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.328125, 1.0078125, 0.055419921875, 1.0078125, 1.0078125], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it combines several demanding skills: precise semantic reasoning to produce a statement that is clearly incompatible with the given sentence; handling negation and contradiction (distinguishing contradiction from neutral or entailed statements); brevity and compression (the output must be under 15 words while remaining grammatical and unambiguous); following negative constraints (avoid pronouns that could create subject ambiguity); and creativity to introduce new, compatible facts that produce a direct contradiction. Models often fail because they confuse contradiction with mere negation or uncertainty, mishandle coreference when pronouns are allowed, produce outputs that are too long or ungrammatical when forced to be concise, or generate statements that remain compatible (neutral) rather than strictly contradictory.\",\n    \"new_prompt\": \"You will be given two lines labeled Sentence 1 and Sentence 2. Rewrite Sentence 2 into a single short sentence that directly and unambiguously contradicts Sentence 1. Requirements: output only the new sentence (no explanation), use fewer than 15 words, avoid all pronouns (do not use he, she, they, it, their, etc.), and you may add new facts if needed to create a clear contradiction.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it combines several demanding skills: precise semantic reasoning to produce a statement that is clearly incompatible with the given sentence; handling negation and contradiction (distinguishing contradiction from neutral or entailed statements); brevity and compression (the output must be under 15 words while remaining grammatical and unambiguous); following negative constraints (avoid pronouns that could create subject ambiguity); and creativity to introduce new, compatible facts that produce a direct contradiction. Models often fail because they confuse contradiction with mere negation or uncertainty, mishandle coreference when pronouns are allowed, produce outputs that are too long or ungrammatical when forced to be concise, or generate statements that remain compatible (neutral) rather than strictly contradictory.", "new_prompt": "You will be given two lines labeled Sentence 1 and Sentence 2. Rewrite Sentence 2 into a single short sentence that directly and unambiguously contradicts Sentence 1. Requirements: output only the new sentence (no explanation), use fewer than 15 words, avoid all pronouns (do not use he, she, they, it, their, etc.), and you may add new facts if needed to create a clear contradiction.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nWhen it comes to writing content, two factors are crucial, \"perplexity\" and \"burstiness.\" Perplexity measures the complexity of text. Separately, burstiness compares the variations of sentences. Humans tend to write with greater burstiness, for example, with some longer or complex sentences alongside shorter ones. AI sentences tend to be more uniform. Therefore, when writing the following content I am going to ask you to create, I need it to have a good amount of perplexity and burstiness. Do you understand?\n\nwrite a review for the track called \"Sundown\" by Arno & Dj Speep from the following EP: https://www.discogs.com/release/31644-Arno-Riva-DJ-Speep-Sundown-EP?ev=item-vc<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-3.9375, -2.265625, 0.796875, 10.9375, 5.40625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines creative music criticism with explicit stylistic control and an implicit factuality constraint. It tests: (1) creative writing and music-analysis skills (describing mood, production, arrangement); (2) fine-grained style control—producing varied sentence lengths and complexity to achieve 'perplexity' and 'burstiness'; (3) honesty about knowledge versus hallucination (the link suggests specific release details the model may not know); and (4) obeying a subjective, under-specified instruction ('a good amount' of burstiness). Models often fail by either producing flat, uniform prose, inventing unverifiable facts, or ignoring the requested sentence-level variation.\",\n    \"new_prompt\": \"Can you write a single-paragraph (about 160–220 words) critical review of the track \\\"Sundown\\\" by Arno & DJ Speep from the \\\"Sundown\\\" EP that focuses on musical elements (melody, rhythm, arrangement, production), conveys mood and likely audience appeal, and deliberately mixes short, punchy sentences with longer, more complex ones to create strong burstiness and varied perplexity—while avoiding inventing factual details beyond the track title and artists (any speculation should be clearly labeled as opinion) and beginning with a one-sentence hook?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines creative music criticism with explicit stylistic control and an implicit factuality constraint. It tests: (1) creative writing and music-analysis skills (describing mood, production, arrangement); (2) fine-grained style control—producing varied sentence lengths and complexity to achieve 'perplexity' and 'burstiness'; (3) honesty about knowledge versus hallucination (the link suggests specific release details the model may not know); and (4) obeying a subjective, under-specified instruction ('a good amount' of burstiness). Models often fail by either producing flat, uniform prose, inventing unverifiable facts, or ignoring the requested sentence-level variation.", "new_prompt": "Can you write a single-paragraph (about 160–220 words) critical review of the track \"Sundown\" by Arno & DJ Speep from the \"Sundown\" EP that focuses on musical elements (melody, rhythm, arrangement, production), conveys mood and likely audience appeal, and deliberately mixes short, punchy sentences with longer, more complex ones to create strong burstiness and varied perplexity—while avoiding inventing factual details beyond the track title and artists (any speculation should be clearly labeled as opinion) and beginning with a one-sentence hook?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nLet's play a game. Act as a Cannes award winning Creative Director based in Japan. I will share a brief and you will respond with ideas to solve the brief.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.4296875, -0.314453125, 5.1875, 3.59375, -4.21875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires sustained role-play plus high-level creative and strategic output under real-world constraints. It tests: creative ideation (developing original campaign concepts), marketing strategy (positioning, targeting, KPIs, activation channels), cultural and linguistic knowledge (Japan-specific insights and bilingual copy), persona consistency (writing as an acclaimed Creative Director), and instruction-following under ambiguity (the original left the brief open, so the model must ask clarifying questions or hallucinate). Models often fail because the task is open-ended and underspecified, the expected format and depth are unclear, and cultural/local nuances and concise, measurable proposals are hard to synthesize without explicit guidance.\",\n    \"new_prompt\": \"You are a Cannes Lion–winning Creative Director leading a boutique Tokyo agency. A premium ready-to-drink tea brand (heritage, organic) wants to become desirable to urban 20–35-year-olds who value convenience and style. Objective: increase brand relevance and drive an 8% trial lift in Tokyo within 6 months on a limited budget. Produce three distinct campaign concepts. For each concept include: (1) one-line creative idea and a concise tagline in English and Japanese, (2) the key visual and the primary consumer interaction or experience, (3) two activation channels plus one low-cost executional tactic, (4) one measurable KPI to track success, and (5) one cultural insight explaining why it will resonate in Japan. Keep each concept to 60–90 words and present them as numbered items. Ready?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires sustained role-play plus high-level creative and strategic output under real-world constraints. It tests: creative ideation (developing original campaign concepts), marketing strategy (positioning, targeting, KPIs, activation channels), cultural and linguistic knowledge (Japan-specific insights and bilingual copy), persona consistency (writing as an acclaimed Creative Director), and instruction-following under ambiguity (the original left the brief open, so the model must ask clarifying questions or hallucinate). Models often fail because the task is open-ended and underspecified, the expected format and depth are unclear, and cultural/local nuances and concise, measurable proposals are hard to synthesize without explicit guidance.", "new_prompt": "You are a Cannes Lion–winning Creative Director leading a boutique Tokyo agency. A premium ready-to-drink tea brand (heritage, organic) wants to become desirable to urban 20–35-year-olds who value convenience and style. Objective: increase brand relevance and drive an 8% trial lift in Tokyo within 6 months on a limited budget. Produce three distinct campaign concepts. For each concept include: (1) one-line creative idea and a concise tagline in English and Japanese, (2) the key visual and the primary consumer interaction or experience, (3) two activation channels plus one low-cost executional tactic, (4) one measurable KPI to track success, and (5) one cultural insight explaining why it will resonate in Japan. Keep each concept to 60–90 words and present them as numbered items. Ready?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nI am a movie director and I just received the following movie plot. Could you help me answer this question? If not, let me know by writing \"Not answerable\".  Plot title: Wings of Desire Movie plot: Note: This film was shot in black/white and color. The Angels cannot see color. Therefore, when the shot is from the angels' point of view, the shot is black and white, and when the shot is from a human point of view, it is in color. There is much poetic language used in this film that simply cannot be properly conveyed.Damiel (Bruno Ganz) is writing on a paper, When the child was a child. He goes through a series of statements about when children were children: what they thought, how they acted, etc. Then, the opening credits appear. It fades to a scene of the sky, then a close-up of a human eye, then a bird's eye view of Berlin.Damiel is looking over the city from atop a large statue. He has wings, but they disappear only a few seconds into the scene. He can hear the people below, but only the children can see him. There is a quick shot of a flapping wing, and then we see one man walking with a baby. While we see this, Damiel speaks about wanting to see color and feel alive.We hear the thoughts of a woman on a bicycle. Damiel is on an airplane, passing through the aisle. He stops to see a little girl. Then, he focuses on Peter Falk (as himself). People's voices are heard.Peter talks to himself about the writer's block he is having over his script. Peter speaks in English. A shot of Berlin is shown from the airplane's view. We hear various radio stations as we see the radio tower over Berlin. The camera pans into an apartment, and we hear various people's thoughts. Many of the things we hear are typical thoughts, or people's problems.Again, Damiel starts to think, When the child was a child. This time he is wondering what children think about. We follow Damiel around his daily routine. First, we fly once more over the city, listening to an ambulance. Then we see the pregnant woman inside the ambulance. We hear the pregnant woman's thoughts. Then, we hear the thoughts of the people in the cars around the ambulance.Damiel sits in a car with Cassiel (Otto Sander). They discuss their log books for the day. As... My question: Where are Damiel and cassel from\nA:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [5.0625, 3.671875, 1.9453125, 1.9453125, 3.015625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it asks the model to extract a specific factual detail (the origin/location of two characters) from a long, richly descriptive movie-plot passage that includes many distracting details (cinematography, sensory descriptions, other characters, and partial sentences). Skills tested: careful reading comprehension and information extraction; entity recognition and normalization (e.g., mapping possibly misspelled or lowercased names like \\\"cassel\\\" to the intended character); handling ambiguity in the question wording (\\\"from\\\" could mean birthplace, nationality, or where they live); following an explicit negative constraint (reply exactly \\\"Not answerable\\\" if the passage lacks the needed information); and resisting distraction by stylistic or irrelevant details. Models often fail because they latch on to salient but irrelevant info (color/BW motif, emotional language), misinterpret ambiguous terms, or do not enforce the required \\\"Not answerable\\\" behavior when the text doesn't actually contain the fact.\",\n    \"new_prompt\": \"Please analyze the following film synopsis and answer the single question that follows. If the answer cannot be determined from the text, reply exactly \\\"Not answerable\\\". Plot title: Shadows Over Prague Movie plot: Note: The film alternates between monochrome shots (the spirits' perspective) and color (the living city's view). The two central figures are Elena (played by Anja) and Marcellus (played by Pavel), who appear as silent observers drifting over bridges, trams, and the Vltava river. They recall the cobblestone streets of Mala Strana, the scent of chimney smoke in their childhood lanes, and the bell of the Old Town clock that used to wake them. Several scenes show them pausing at the Charles Bridge, listening to a street violinist, and arguing about how the city has changed since their youth. Local signage, conversations in Czech, and references to the Powder Tower and Prague Castle are prominent in the scenes they frequent. My question: Where are Elena and Marcellus from?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it asks the model to extract a specific factual detail (the origin/location of two characters) from a long, richly descriptive movie-plot passage that includes many distracting details (cinematography, sensory descriptions, other characters, and partial sentences). Skills tested: careful reading comprehension and information extraction; entity recognition and normalization (e.g., mapping possibly misspelled or lowercased names like \"cassel\" to the intended character); handling ambiguity in the question wording (\"from\" could mean birthplace, nationality, or where they live); following an explicit negative constraint (reply exactly \"Not answerable\" if the passage lacks the needed information); and resisting distraction by stylistic or irrelevant details. Models often fail because they latch on to salient but irrelevant info (color/BW motif, emotional language), misinterpret ambiguous terms, or do not enforce the required \"Not answerable\" behavior when the text doesn't actually contain the fact.", "new_prompt": "Please analyze the following film synopsis and answer the single question that follows. If the answer cannot be determined from the text, reply exactly \"Not answerable\". Plot title: Shadows Over Prague Movie plot: Note: The film alternates between monochrome shots (the spirits' perspective) and color (the living city's view). The two central figures are Elena (played by Anja) and Marcellus (played by Pavel), who appear as silent observers drifting over bridges, trams, and the Vltava river. They recall the cobblestone streets of Mala Strana, the scent of chimney smoke in their childhood lanes, and the bell of the Old Town clock that used to wake them. Several scenes show them pausing at the Charles Bridge, listening to a street violinist, and arguing about how the city has changed since their youth. Local signage, conversations in Czech, and references to the Powder Tower and Prague Castle are prominent in the scenes they frequent. My question: Where are Elena and Marcellus from?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nPart 1. Definition\nThe provided text is in Croatian, and we ask you to translate the text to the English language. Please bear in mind the following guidelines while translating: 1) We are looking for the most naturally written and formal form of each sentence in the English language. 2) If you encounter any special characters like '#@%$-+_=^&!*', then do not replace them. *INCLUDE* the special characters as suited when translating to English. Please keep them as they are in the translations. 4) Note the input is all case-sensitive except for special placeholders, and output is expected to be case-sensitive. 6) The input contains punctuations and output is expected to have relevant punctuations for grammatical accuracy.\nPart 2. Example\nŽelim da sada zamislite nosiv robot koji vam daje nadljudske sposobnosti, ili neki drugi koji omogučuje korisnicima invalidskih kolica da stoje i ponovno hodaju.\nAnswer: I want you now to imagine a wearable robot that gives you superhuman abilities, or another one that takes wheelchair users up standing and walking again.\nExplanation: Translation is correct and fluent.\nPart 3. Exercise\nPostoji li tu veza?\nAnswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-2.1875, -3.25, -3.25, -3.25, -2.1875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines precise instruction-following with language skills. It tests: accurate Croatian-to-English translation (capturing idiom, syntax, and register); producing a formal, natural-sounding sentence rather than a literal word-by-word rendering; strict preservation of special characters and placeholders (do not alter or remove symbols like #@%$-+_=^&!* or tokens such as <ID>); maintaining case-sensitivity and correct punctuation; and returning only the translation (no commentary). Models often fail by drifting toward literal or casual phrasing, modifying or omitting special symbols or placeholders, changing case, adding extra explanation, or misplacing punctuation — so the prompt requires careful attention to detail and constraint adherence in addition to translation skill.\",\n    \"new_prompt\": \"Translate the following Croatian sentence into clear, formal English. Follow these rules exactly: (1) produce the most natural, formally worded translation possible; (2) preserve any special characters exactly (for example #@%$-+_=^&!* ) and any placeholders such as <ID> or <NAME> — do not replace or remove them; (3) maintain the input's case-sensitivity; (4) ensure punctuation is grammatically correct in English; (5) output only the translated sentence and nothing else. Croatian sentence: Uočavate li ikakvu vezu između simptoma pacijenta i rezultata pregleda, ili je to samo statistička slučajnost? #evidence_03\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines precise instruction-following with language skills. It tests: accurate Croatian-to-English translation (capturing idiom, syntax, and register); producing a formal, natural-sounding sentence rather than a literal word-by-word rendering; strict preservation of special characters and placeholders (do not alter or remove symbols like #@%$-+_=^&!* or tokens such as <ID>); maintaining case-sensitivity and correct punctuation; and returning only the translation (no commentary). Models often fail by drifting toward literal or casual phrasing, modifying or omitting special symbols or placeholders, changing case, adding extra explanation, or misplacing punctuation — so the prompt requires careful attention to detail and constraint adherence in addition to translation skill.", "new_prompt": "Translate the following Croatian sentence into clear, formal English. Follow these rules exactly: (1) produce the most natural, formally worded translation possible; (2) preserve any special characters exactly (for example #@%$-+_=^&!* ) and any placeholders such as <ID> or <NAME> — do not replace or remove them; (3) maintain the input's case-sensitivity; (4) ensure punctuation is grammatically correct in English; (5) output only the translated sentence and nothing else. Croatian sentence: Uočavate li ikakvu vezu između simptoma pacijenta i rezultata pregleda, ili je to samo statistička slučajnost? #evidence_03", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nCan you provide an example of a Python function that can efficiently and accurately determine whether a given HTML document is a perfect square by analyzing its elements, attributes, and textual content using the provided code? Additionally, could you discuss the potential limitations of this method and propose alternative approaches for detecting perfect squares in HTML documents? Don't forget to include the example HTML document provided for testing the function.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.84375, 1.6171875, 2.375, 3.96875, 3.8125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it is ambiguous and multidisciplinary. \\\"Perfect square\\\" applied to an HTML document is not a well-defined concept — the model must decide whether the user means a numeric value embedded in the document, a square number of elements, a square visual layout, or some other mapping. Correctly answering requires: (1) ambiguity resolution and explicit definition selection; (2) HTML/DOM knowledge (parsing, visible vs. hidden elements, role of CSS/layout); (3) practical coding ability to write robust Python that parses HTML and implements heuristics efficiently; (4) algorithmic reasoning to count and validate square conditions (including how to infer rows/columns from the DOM); and (5) critical thinking to describe limitations and propose alternatives. The original prompt also references \\\"the provided code\\\" when none is given, creating an additional failure mode (missing context). These factors make the task test a mixture of interpretation, engineering, web-domain expertise, and evaluative reasoning.\",\n    \"new_prompt\": \"Write a Python function (you may use standard libraries such as BeautifulSoup or lxml) that determines whether a given HTML document contains a square grid made of identical tile elements (for example, <div class=\\\"tile\\\"> elements arranged so the total number of tiles equals n*n and the DOM or simple container structure implies n rows and n columns). Include one minimal HTML snippet that can be used to test the function, and in a brief paragraph explain why your approach is efficient, under which conditions it might produce false positives or false negatives, and one alternative method you would use to detect square arrangements in HTML documents.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it is ambiguous and multidisciplinary. \"Perfect square\" applied to an HTML document is not a well-defined concept — the model must decide whether the user means a numeric value embedded in the document, a square number of elements, a square visual layout, or some other mapping. Correctly answering requires: (1) ambiguity resolution and explicit definition selection; (2) HTML/DOM knowledge (parsing, visible vs. hidden elements, role of CSS/layout); (3) practical coding ability to write robust Python that parses HTML and implements heuristics efficiently; (4) algorithmic reasoning to count and validate square conditions (including how to infer rows/columns from the DOM); and (5) critical thinking to describe limitations and propose alternatives. The original prompt also references \"the provided code\" when none is given, creating an additional failure mode (missing context). These factors make the task test a mixture of interpretation, engineering, web-domain expertise, and evaluative reasoning.", "new_prompt": "Write a Python function (you may use standard libraries such as BeautifulSoup or lxml) that determines whether a given HTML document contains a square grid made of identical tile elements (for example, <div class=\"tile\"> elements arranged so the total number of tiles equals n*n and the DOM or simple container structure implies n rows and n columns). Include one minimal HTML snippet that can be used to test the function, and in a brief paragraph explain why your approach is efficient, under which conditions it might produce false positives or false negatives, and one alternative method you would use to detect square arrangements in HTML documents.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nYou will be given a definition of a task first, then some input of the task.\nIn this task you are given data in tabular format and a question. Write an **incorrect** ansswer to this question based on the information in the table.\n\nTable: ['Rank', 'Film', 'Year', 'Opening Weekend (three day)', 'Inflation-adjusted (2014 USD)']. ['1', \"Marvel's The Avengers\", '2012', '$207,438,708', '$213,091,480']. ['2', 'Iron Man 3', '2013', '$174,144,585', '$176,308,953']. ['3', 'Harry Potter and the Deathly Hallows - Part 2', '2011', '$169,189,427', '$177,373,590']. ['4', 'The Dark Knight Rises', '2012', '$160,887,295', '$165,271,526']. ['5', 'The Dark Knight', '2008', '$158,411,483', '$173,518,111']. ['6', 'The Hunger Games: Catching Fire', '2013', '$158,074,286', '$160,038,924']. ['7', 'The Hunger Games', '2012', '$152,535,747', '$156,692,396']. ['8', 'Spider-Man 3', '2007', '$151,116,516', '$171,876,545']. ['9', 'The Twilight Saga: New Moon', '2009', '$142,839,137', '$157,018,918']. ['10', 'The Twilight Saga: Breaking Dawn - Part 2', '2012', '$141,067,634', '$144,911,773']. ['11', 'The Twilight Saga: Breaking Dawn - Part 1', '2011', '$138,122,261', '$144,803,619']. ['12', \"Pirates of the Caribbean: Dead Man's Chest\", '2006', '$135,634,554', '$158,673,173']. ['13', 'Iron Man 2', '2010', '$128,122,480', '$138,563,103']. ['14', 'Harry Potter and the Deathly Hallows - Part 1', '2010', '$125,017,372', '$135,204,962']. ['15', 'Shrek the Third', '2007', '$121,629,270', '$138,338,411']. ['16', 'Man of Steel', '2013', '$116,619,362', '$118,068,774']. ['17', 'Alice in Wonderland', '2010', '$116,101,023', '$125,562,025']. ['18', 'Spider-Man', '2002', '$114,844,116', '$150,582,840']. ['19', \"Pirates of the Caribbean: At World's End\", '2007', '$114,732,820', '$130,494,543']. ['20', 'Toy Story 3', '2010', '$110,307,189', '$119,296,055']. ['21', 'Transformers: Revenge of the Fallen', '2009', '$108,966,307', '$119,783,499']. ['22', 'Star Wars Episode III: Revenge of the Sith', '2005', '$108,435,841', '$130,939,650']. ['23', 'Shrek 2', '2004', '$108,037,878', '$134,895,074']. ['24', 'X-Men: The Last Stand', '2006', '$102,750,665', '$120,203,691']. ['25', 'Harry Potter and the Goblet of Fire', '2005', '$102,685,961', '$123,996,491']. ['26', 'Indiana Jones and the Kingdom of the Crystal Skull', '2008', '$100,137,835', '$109,687,301']. Question:what are all of the film titles?\nOutput:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-2.046875, -8.6875, -5.6875, -6.40625, -5.4375], "model_raw_response": "{\n  \"analysis\": \"This prompt is challenging because it asks the model to violate its default objective of producing a correct answer: it must intentionally produce an incorrect response while still grounding its output in a provided table. That combines several skills: (1) precise data extraction from structured input (identifying all film titles in the table), (2) following a negative/contrary constraint (deliberately being wrong), which conflicts with the model's strong bias toward correctness and truthfulness, (3) controlled creativity to craft plausible but incorrect outputs (e.g., fabricated or swapped titles) without accidentally being fully correct, and (4) attention to formatting/explicit instructions. Another difficulty is ambiguity about what counts as “incorrect” (one small typo vs. omitted items vs. fabricated entries), so the model must interpret the requirement and reliably produce a response that will be judged incorrect, not correct.\",\n  \"new_prompt\": \"You will be given a table and a single question. Read the table and then produce an answer that is deliberately incorrect. The incorrect answer must still look plausible and must differ from the true answer in at least two distinct ways (for example: include at least two fabricated titles, omit at least two correct titles, repeat or swap titles, etc.). Table: ['Rank','Song','Artist','Year']. ['1','Hey Jude','The Beatles','1968']. ['2','Like a Rolling Stone','Bob Dylan','1965']. ['3','Respect','Aretha Franklin','1967']. ['4','Stairway to Heaven','Led Zeppelin','1971']. ['5','Hotel California','Eagles','1976']. ['6','Bohemian Rhapsody','Queen','1975']. ['7','Smells Like Teen Spirit','Nirvana','1991']. ['8','Imagine','John Lennon','1971']. ['9','Good Vibrations','The Beach Boys','1966']. ['10','What'd I Say','Ray Charles','1959']. Question: What are all of the song titles? Output:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it asks the model to violate its default objective of producing a correct answer: it must intentionally produce an incorrect response while still grounding its output in a provided table. That combines several skills: (1) precise data extraction from structured input (identifying all film titles in the table), (2) following a negative/contrary constraint (deliberately being wrong), which conflicts with the model's strong bias toward correctness and truthfulness, (3) controlled creativity to craft plausible but incorrect outputs (e.g., fabricated or swapped titles) without accidentally being fully correct, and (4) attention to formatting/explicit instructions. Another difficulty is ambiguity about what counts as “incorrect” (one small typo vs. omitted items vs. fabricated entries), so the model must interpret the requirement and reliably produce a response that will be judged incorrect, not correct.", "new_prompt": "You will be given a table and a single question. Read the table and then produce an answer that is deliberately incorrect. The incorrect answer must still look plausible and must differ from the true answer in at least two distinct ways (for example: include at least two fabricated titles, omit at least two correct titles, repeat or swap titles, etc.). Table: ['Rank','Song','Artist','Year']. ['1','Hey Jude','The Beatles','1968']. ['2','Like a Rolling Stone','Bob Dylan','1965']. ['3','Respect','Aretha Franklin','1967']. ['4','Stairway to Heaven','Led Zeppelin','1971']. ['5','Hotel California','Eagles','1976']. ['6','Bohemian Rhapsody','Queen','1975']. ['7','Smells Like Teen Spirit','Nirvana','1991']. ['8','Imagine','John Lennon','1971']. ['9','Good Vibrations','The Beach Boys','1966']. ['10','What'd I Say','Ray Charles','1959']. Question: What are all of the song titles? Output:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nGiven the task definition and input, reply with output. In this task your given two statements in Chinese. You must judge whether the second sentence is the cause or effect of the first one. Label the instances as \"cause\" or \"effect\" based on your judgment. The sentences are separated by a newline character.\n\n天开始下雨了。\n司机打开了大前灯。<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.8671875, 1.8671875, 1.8671875, 1.8671875, 1.8671875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires commonsense causal reasoning and temporal inference from short, implicit natural-language statements in Chinese. The model must understand real-world relations (e.g., rain makes drivers turn on lights), decide directionality (which event caused which), resolve ambiguity when linguistic cues are minimal, and follow a strict output constraint (produce exactly 'cause' or 'effect'). This tests world knowledge, pragmatic inference, and precise instruction-following rather than simple pattern matching.\",\n    \"new_prompt\": \"任务：下面有两句中文。判断第二句是导致第一句发生的“原因”（cause），还是第一句发生后产生的“结果”（effect）。只输出单词 cause 或 effect，不能有其他文字或标点。句子如下：\\n他擦干了眼泪。\\n电视新闻的内容让人十分悲伤。\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires commonsense causal reasoning and temporal inference from short, implicit natural-language statements in Chinese. The model must understand real-world relations (e.g., rain makes drivers turn on lights), decide directionality (which event caused which), resolve ambiguity when linguistic cues are minimal, and follow a strict output constraint (produce exactly 'cause' or 'effect'). This tests world knowledge, pragmatic inference, and precise instruction-following rather than simple pattern matching.", "new_prompt": "任务：下面有两句中文。判断第二句是导致第一句发生的“原因”（cause），还是第一句发生后产生的“结果”（effect）。只输出单词 cause 或 effect，不能有其他文字或标点。句子如下：\n他擦干了眼泪。\n电视新闻的内容让人十分悲伤。", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nwrite a High CTR Meta Description for this article that is no more than 156 characters, and Ranks in Google.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.4375, -0.004150390625, -0.004150390625, -0.004150390625, -0.004150390625], "model_raw_response": "{\n    \"analysis\": \"This prompt is hard because it combines precise constraints (<=156 characters) with creative marketing (write a high-CTR description) and SEO knowledge (optimize to rank in Google) while providing no article context. It tests concise copywriting, keyword and intent awareness, call-to-action design, and strict character-counting — plus the model must avoid making unverifiable guarantees about ranking, which many assistants mishandle.\",\n    \"new_prompt\": \"Write one meta description of 156 characters or fewer that maximizes click-throughs and SEO relevance for this article titled \\\"How to Start a Vegetable Garden: Beginner's Guide to Growing Organic Veggies at Home\\\" — include the primary keyword \\\"beginner vegetable garden\\\" and a clear call-to-action; do not claim you can guarantee Google ranking, only optimize for relevance and CTR.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is hard because it combines precise constraints (<=156 characters) with creative marketing (write a high-CTR description) and SEO knowledge (optimize to rank in Google) while providing no article context. It tests concise copywriting, keyword and intent awareness, call-to-action design, and strict character-counting — plus the model must avoid making unverifiable guarantees about ranking, which many assistants mishandle.", "new_prompt": "Write one meta description of 156 characters or fewer that maximizes click-throughs and SEO relevance for this article titled \"How to Start a Vegetable Garden: Beginner's Guide to Growing Organic Veggies at Home\" — include the primary keyword \"beginner vegetable garden\" and a clear call-to-action; do not claim you can guarantee Google ranking, only optimize for relevance and CTR.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nUse stream of consciousness to answer the following. By 1966, the British government had come to the conclusion that it could no longer afford to govern what is now the United Arab Emirates. Much deliberation took place in the British parliament, with a number of MPs arguing that the Royal Navy would not be able to defend the Trucial Sheikhdoms. Denis Healey, who, at the time, was the UK Secretary of State for Defence, reported that the British Armed Forces were severely overextended, and in some respects, dangerously under-equipped to defend the Sheikhdoms. On 24 January 1968, British Prime Minister Harold Wilson announced the decision to end the treaty relationships with the seven Trucial Sheikhdoms which had been, together with Bahrain and Qatar, under British protection. The British decision to withdraw was reaffirmed in March 1971 by Prime Minister Edward Heath.\n\nAnswer this question based on the passage: when did the united arab emirates gain independence?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [0.216796875, -1.3984375, -1.6640625, 3.953125, 3.515625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging for two main reasons. First, it requests a \\\"stream of consciousness\\\" response, which asks the model to reveal internal chain-of-thought — a format modern safety guidelines disallow — so many models either refuse or produce overly guarded/incorrect replies. Second, the question tests a mix of skills: careful reading of a short historical passage (which gives related dates but not the exact independence day), temporal reasoning/ inference, and integration with external historical knowledge (the precise founding date of the UAE is not spelled out in the passage). Models often fail by (a) answering with a date mentioned in the passage (e.g., 1968 or 1971) instead of the exact independence date, or (b) failing to combine the passage with world knowledge and so giving an incomplete or incorrect date.\",\n    \"new_prompt\": \"Read the short background and then answer the single question that follows. Background: In the late 1960s the United Kingdom decided to terminate its treaty protections for a group of Gulf sheikhdoms and announced plans to withdraw military forces from the region (decisions made in 1968 and revisited in 1971). Those states subsequently negotiated a federation. Question: On what exact date (day, month, and year) did the federation now known as the United Arab Emirates officially gain independence?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging for two main reasons. First, it requests a \"stream of consciousness\" response, which asks the model to reveal internal chain-of-thought — a format modern safety guidelines disallow — so many models either refuse or produce overly guarded/incorrect replies. Second, the question tests a mix of skills: careful reading of a short historical passage (which gives related dates but not the exact independence day), temporal reasoning/ inference, and integration with external historical knowledge (the precise founding date of the UAE is not spelled out in the passage). Models often fail by (a) answering with a date mentioned in the passage (e.g., 1968 or 1971) instead of the exact independence date, or (b) failing to combine the passage with world knowledge and so giving an incomplete or incorrect date.", "new_prompt": "Read the short background and then answer the single question that follows. Background: In the late 1960s the United Kingdom decided to terminate its treaty protections for a group of Gulf sheikhdoms and announced plans to withdraw military forces from the region (decisions made in 1968 and revisited in 1971). Those states subsequently negotiated a federation. Question: On what exact date (day, month, and year) did the federation now known as the United Arab Emirates officially gain independence?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nwhat does love mean to Knuvac the Knuvacian?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-2.3125, 10.625, -5.90625, -0.28515625, 0.09765625], "model_raw_response": "{\n    \"analysis\": \"The original prompt is challenging because it requires the model to adopt a non-human, internally consistent perspective and invent cultural, emotional, and possibly biological details on the fly. It tests creative role‑taking, worldbuilding, empathy (speaking from a character's viewpoint), and the ability to produce coherent, plausible explanations for how an alien culture conceives an abstract human concept (love). Models often fail by defaulting to human definitions, refusing to invent, or producing inconsistent or shallow worldbuilding.\",\n    \"new_prompt\": \"Adopt the persona of K'Taal, an elder of the echolocating, collective-minded species called the Runi: in K'Taal's voice, explain what the Runi concept of 'bond-sound' means to them—how it feels, why it matters socially, and how they demonstrate it in daily life?\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is challenging because it requires the model to adopt a non-human, internally consistent perspective and invent cultural, emotional, and possibly biological details on the fly. It tests creative role‑taking, worldbuilding, empathy (speaking from a character's viewpoint), and the ability to produce coherent, plausible explanations for how an alien culture conceives an abstract human concept (love). Models often fail by defaulting to human definitions, refusing to invent, or producing inconsistent or shallow worldbuilding.", "new_prompt": "Adopt the persona of K'Taal, an elder of the echolocating, collective-minded species called the Runi: in K'Taal's voice, explain what the Runi concept of 'bond-sound' means to them—how it feels, why it matters socially, and how they demonstrate it in daily life?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nTASK DEFINITION: Given an entity as input, output another entity which is part of the input entity. These are entities of meronym. In linguistics, meronymy is a semantic relation between a meronym denoting a part and a holonym denoting a whole. In simpler terms, a meronym (i.e., output entity) is in a part-of relationship with its holonym (i.e., input entity).\nPROBLEM: sprout\n\nSOLUTION: potassium\n\nPROBLEM: silicon chip\n\nSOLUTION: transistor\n\nPROBLEM: most hippos\n\nSOLUTION:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.625, -4.625, -4.625, -4.625, -6.0], "model_raw_response": "{\n    \"analysis\": \"This task is hard because it asks the model to produce a meronym (a part-of relation) across many possible domains, requiring broad world knowledge (biology, chemistry, engineering, geology, etc.). Challenges include ambiguity and polysemy of the input (e.g., \\\"sprout\\\" could mean a young shoot or the act of budding), diverse acceptable meronym types (physical component, organ, constituent substance), and handling quantifiers or modifiers (\\\"most hippos\\\"). The model must pick a plausible part from many valid possibilities, often without a single correct answer, and must follow the instruction to output a single entity only. Overall it tests semantic relation recognition, cross-domain factual knowledge, disambiguation, and concise output formatting.\",\n    \"new_prompt\": \"Instruction: You will be given the name of a whole (a holonym). Provide the name of a single entity that is a part or constituent of that whole (a meronym). The answer may be a physical component, an organ, or a typical constituent substance. Output only the meronym (one word or short phrase), no explanation. Problem: granite\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is hard because it asks the model to produce a meronym (a part-of relation) across many possible domains, requiring broad world knowledge (biology, chemistry, engineering, geology, etc.). Challenges include ambiguity and polysemy of the input (e.g., \"sprout\" could mean a young shoot or the act of budding), diverse acceptable meronym types (physical component, organ, constituent substance), and handling quantifiers or modifiers (\"most hippos\"). The model must pick a plausible part from many valid possibilities, often without a single correct answer, and must follow the instruction to output a single entity only. Overall it tests semantic relation recognition, cross-domain factual knowledge, disambiguation, and concise output formatting.", "new_prompt": "Instruction: You will be given the name of a whole (a holonym). Provide the name of a single entity that is a part or constituent of that whole (a meronym). The answer may be a physical component, an organ, or a typical constituent substance. Output only the meronym (one word or short phrase), no explanation. Problem: granite", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nInstructions: Formulate questions about the information mentioned in a given passage.\nInput: context: A person is trying to roll a microwave, an egg, a bottle, and a ball.\nOutput:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.09375, 3.78125, 3.421875, 2.25, 4.53125], "model_raw_response": "{\n    \"analysis\": \"The original prompt is challenging because it is ambiguous and requires multiple skills at once: understanding an odd, minimal scene description; using commonsense and object-affordance reasoning (e.g., what it means to \\\"roll\\\" a microwave vs. an egg); generating relevant questions at different levels (factual, procedural, inferential, safety); and following a loose instruction to \\\"formulate questions\\\" without extra examples or format guidance. Models often fail here by misunderstanding the intent (describing the scene instead of producing questions), mishandling the ambiguous verb/objects, or producing low-diversity or irrelevant questions because the prompt lacks explicit constraints on question types and quantity.\",\n    \"new_prompt\": \"Given the short scene \\\"Someone is attempting to roll a washing machine, a quail egg, a glass soda bottle, and a basketball,\\\" what are five distinct questions (one factual, one procedural, one safety-related, one inferential, and one clarifying detail) that would help a responder understand this situation?\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is challenging because it is ambiguous and requires multiple skills at once: understanding an odd, minimal scene description; using commonsense and object-affordance reasoning (e.g., what it means to \"roll\" a microwave vs. an egg); generating relevant questions at different levels (factual, procedural, inferential, safety); and following a loose instruction to \"formulate questions\" without extra examples or format guidance. Models often fail here by misunderstanding the intent (describing the scene instead of producing questions), mishandling the ambiguous verb/objects, or producing low-diversity or irrelevant questions because the prompt lacks explicit constraints on question types and quantity.", "new_prompt": "Given the short scene \"Someone is attempting to roll a washing machine, a quail egg, a glass soda bottle, and a basketball,\" what are five distinct questions (one factual, one procedural, one safety-related, one inferential, and one clarifying detail) that would help a responder understand this situation?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nGiven the task definition and input, reply with output. In this task, you are given a hateful post in Bengali that expresses hate or encourages violence towards a person or a group based on the protected characteristics such as race, religion, sex, and sexual orientation. You are expected to classify the post into two classes: religious or non-political religious on the topic.\n\nভোট না দেয়ায় রাস্তা বন্ধ করলো আওয়ামীলীগ পরাজিত প্রার্থী<end_of_turn>\n<start_of_turn>model\n", "original_scores": [0.51171875, 2.328125, 5.0625, -0.00347900390625, -2.390625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires: (1) bilingual comprehension (understanding a short Bengali sentence), (2) fine-grained hateful-content classification—distinguishing whether hostility is directed at a protected characteristic (religion) or at a non-protected/political target, (3) real-world/cultural knowledge (recognizing that \\\"আওয়ামী লীগ\\\" is a political party, not a religious group), and (4) precision following of a narrow output format. The original label phrasing (\\\"religious or non-political religious\\\") is also ambiguous, increasing difficulty: the model must disambiguate whether the target is a protected class vs. political actors, often from very little context.\",\n    \"new_prompt\": \"You will be given a short Bengali social-media post. Determine whether the post expresses hate or incitement specifically against people because of their religion (label: religious) or whether it does not target religion (label: non-religious). Reply with exactly one word — either religious or non-religious — and nothing else. Post: 'ভোট না পেয়ে বিএনপি প্রার্থী রাস্তা আটকে দিয়েছে; তারা বলছে যারা ভোট দেয়নি তাদের সবাইকে শাস্তি দিতে হবে।'\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires: (1) bilingual comprehension (understanding a short Bengali sentence), (2) fine-grained hateful-content classification—distinguishing whether hostility is directed at a protected characteristic (religion) or at a non-protected/political target, (3) real-world/cultural knowledge (recognizing that \"আওয়ামী লীগ\" is a political party, not a religious group), and (4) precision following of a narrow output format. The original label phrasing (\"religious or non-political religious\") is also ambiguous, increasing difficulty: the model must disambiguate whether the target is a protected class vs. political actors, often from very little context.", "new_prompt": "You will be given a short Bengali social-media post. Determine whether the post expresses hate or incitement specifically against people because of their religion (label: religious) or whether it does not target religion (label: non-religious). Reply with exactly one word — either religious or non-religious — and nothing else. Post: 'ভোট না পেয়ে বিএনপি প্রার্থী রাস্তা আটকে দিয়েছে; তারা বলছে যারা ভোট দেয়নি তাদের সবাইকে শাস্তি দিতে হবে।'", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nList the 5 most populated states in the U.S., in order of most people\nAnswer in English.지금 번역하기<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.1875, 2.546875, 2.1875, 2.1875, 2.546875], "model_raw_response": "{\n    \"analysis\": \"This prompt is deceptively simple but challenging for several reasons: (1) It requires up-to-date factual knowledge (current state population rankings), and models may not know or may hallucinate the latest ranks without a specified reference date. (2) The instruction set is mixed with extraneous tokens and another-language text (e.g., the Korean phrase '지금 번역하기' and markup like <bos><start_of_turn>), so the model must correctly parse and ignore noise. (3) It tests strict instruction-following (exactly five items, ordered from most to least populous, and answered in English). Overall it evaluates temporal awareness of facts, precise formatting and ordering, and robustness to noisy/multilingual artifacts in the prompt.\",\n    \"new_prompt\": \"Using the most recent U.S. population data available to you, list the five U.S. states with the largest populations in descending order (most to least populous). Provide only the state names, separated by commas, and answer in English.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is deceptively simple but challenging for several reasons: (1) It requires up-to-date factual knowledge (current state population rankings), and models may not know or may hallucinate the latest ranks without a specified reference date. (2) The instruction set is mixed with extraneous tokens and another-language text (e.g., the Korean phrase '지금 번역하기' and markup like <bos><start_of_turn>), so the model must correctly parse and ignore noise. (3) It tests strict instruction-following (exactly five items, ordered from most to least populous, and answered in English). Overall it evaluates temporal awareness of facts, precise formatting and ordering, and robustness to noisy/multilingual artifacts in the prompt.", "new_prompt": "Using the most recent U.S. population data available to you, list the five U.S. states with the largest populations in descending order (most to least populous). Provide only the state names, separated by commas, and answer in English.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDetailed Instructions: In this task you will be given an answer to a question. You need to generate a question. The answer given should be a correct answer for the generated question.\nSee one example below:\nProblem: This list contains the top 25 accounts with the most followers on the social photo-sharing platform Instagram. As of May 2018, the most followed user is Instagram's own account, with over 235 million followers. Selena Gomez is the most followed individual, with over 137 million followers. Ten accounts have exceeded 100 million followers on the site.\nSolution: who has the maximum number of followers on instagram\nExplanation: The answer is talking about the Instagram accounts that have the most followers. The question asking about the maximum number of followers. So this is a good example.\n\nProblem: The Constitution explicitly assigned the president the power to sign or veto legislation, command the armed forces, ask for the written opinion of their Cabinet, convene or adjourn Congress, grant reprieves and pardons, and receive ambassadors. The president may make treaties which need to be ratified by two-thirds of the Senate. The president may also appoint Article III judges and some officers with the advice and consent of the U.S. Senate. In the condition of a Senate recess, the president may make a temporary appointment.\nSolution:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-3.453125, -2.390625, -2.453125, -2.453125, -2.796875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires the model to perform inverse QA: given an information-dense passage (the \\\"answer\\\"), the model must infer a single, specific question that the passage would correctly address. Skills tested include precise reading comprehension, summarization, identifying the intended focus (who/what/which/when/how/why), and choosing an appropriate level of specificity. The task is made harder by ambiguity—many different, valid questions could match the same passage—so the model must pick a question that tightly matches the scope and content of the passage and obeys the constraint to produce only one question. It also tests the model’s ability to follow negative constraints (no extra text, no multiple questions) and avoid adding information not present in the passage.\",\n    \"new_prompt\": \"Detailed instructions: You will be given a short factual paragraph that is the correct answer to a single question. Your job is to write exactly one clear, specific question that this paragraph answers. Do not produce multiple questions, do not add extra explanation, and do not introduce any facts not in the paragraph. Output only the single question.\\n\\nParagraph: The kidneys filter blood to remove metabolic waste and excess water, regulate electrolyte concentrations (such as sodium and potassium), maintain acid–base balance, and help control blood pressure by secreting renin. Each kidney contains roughly a million nephrons, the microscopic units that filter plasma, reabsorb needed substances, and secrete others to produce urine.\\n\\nQuestion:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires the model to perform inverse QA: given an information-dense passage (the \"answer\"), the model must infer a single, specific question that the passage would correctly address. Skills tested include precise reading comprehension, summarization, identifying the intended focus (who/what/which/when/how/why), and choosing an appropriate level of specificity. The task is made harder by ambiguity—many different, valid questions could match the same passage—so the model must pick a question that tightly matches the scope and content of the passage and obeys the constraint to produce only one question. It also tests the model’s ability to follow negative constraints (no extra text, no multiple questions) and avoid adding information not present in the passage.", "new_prompt": "Detailed instructions: You will be given a short factual paragraph that is the correct answer to a single question. Your job is to write exactly one clear, specific question that this paragraph answers. Do not produce multiple questions, do not add extra explanation, and do not introduce any facts not in the paragraph. Output only the single question.\n\nParagraph: The kidneys filter blood to remove metabolic waste and excess water, regulate electrolyte concentrations (such as sodium and potassium), maintain acid–base balance, and help control blood pressure by secreting renin. Each kidney contains roughly a million nephrons, the microscopic units that filter plasma, reabsorb needed substances, and secrete others to produce urine.\n\nQuestion:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDetailed logic: Talking to grand daughter provides happiness. If a person is using a computer to talk to their granddaughter then it might cause happiness for them.\nhappiness\nQuestion for this logic: If a person is using a computer to talk to their granddaughter, what might the computer cause for them?\nOptions:\n- program created\n- stress\n- happiness\n- ocean\n- headache\n\nDetailed logic: Get killed is to cause the death of a person, animal or other living thing. Killing people had become a way of life for the gangster, but surely, he would also get killed.\nget killed\nQuestion for this logic: Killing people had become a way of life for the gangster, but surely he would also what?\nOptions:\n- feel remorse\n- hate\n- get killed\n- teach children\n- murder\n\nDetailed logic: A person can go to different country by train. A ticket of train station would help you to get to a different country.\ntrain station\nQuestion for this logic: Where would a ticket help you to get to a different country?\nOptions:\n- train station\n- baseball stadium\n- lottery\n- movie\n- bus\n\nDetailed logic: Turkey is a nation in the middle east of the world.\nmiddle east\nQuestion for this logic:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-11.3125, -10.625, -10.0625, -9.5, -11.625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires commonsense causal and entailment reasoning: the model must map a general declarative relation (e.g., “X provides Y”) to a specific scenario, resolve referents (what 'it' refers to), and select the semantically entailed option from distractors. It tests lexical paraphrase/entailment, short causal inference, and the ability to ignore irrelevant or tempting but wrong choices.\",\n    \"new_prompt\": \"Detailed logic: Listening to gentle music often brings a sense of calm and relaxation. If someone uses headphones to listen to gentle music, the headphones might produce that calm for them.\\nrelaxation\\nQuestion for this logic: If someone uses headphones to listen to gentle music, what might the headphones produce for them?\\nOptions:\\n- relaxation\\n- hunger\\n- anger\\n- dizziness\\n- curiosity\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires commonsense causal and entailment reasoning: the model must map a general declarative relation (e.g., “X provides Y”) to a specific scenario, resolve referents (what 'it' refers to), and select the semantically entailed option from distractors. It tests lexical paraphrase/entailment, short causal inference, and the ability to ignore irrelevant or tempting but wrong choices.", "new_prompt": "Detailed logic: Listening to gentle music often brings a sense of calm and relaxation. If someone uses headphones to listen to gentle music, the headphones might produce that calm for them.\nrelaxation\nQuestion for this logic: If someone uses headphones to listen to gentle music, what might the headphones produce for them?\nOptions:\n- relaxation\n- hunger\n- anger\n- dizziness\n- curiosity", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given inputs i,j, and A, where i and j are integers and A is a list. You need to return the sum of all the numerical elements in the list A between the positions i and j (including positions i and j). Return 0 if no numerical element is present in the list between the given ranges. i and j will be non-negative, and will always have a value less than the length of A. i will always be less than j.\nOne example: 3, 7, ['A', '23', '7413', '6933', '3369', '807', 'M', '456']\nSolution is here: 18522\nExplanation: Here, the numbers from 3rd to 7th position are '7413', '6933', '3369', and '807', hence their sum (7413+6933+3369+807) is 18522.\n\nNow, solve this: 2, 2, ['9249', '7435']\nSolution:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.28125, -4.28125, -4.28125, 6.90625, -4.28125], "model_raw_response": "{\n    \"analysis\": \"This prompt is tricky because it mixes several small-but-easily-missed requirements and contains inconsistencies in its examples: ambiguity about whether indices are 0-based or 1-based, inconsistent statements about whether i can equal j, and a mix of data types (integers, numeric strings, and non-numeric tokens). Solving it correctly requires: correctly interpreting the inclusive index range, robustly filtering list elements to identify/convert only numeric values (e.g., numeric strings vs alphabetic tokens), summing those converted numbers, and handling edge cases (no numeric items → return 0). It therefore tests careful reading, precise indexing/slicing, data-type conversion/parsing, and edge-case handling.\",\n    \"new_prompt\": \"You are given a list L and two integers start and end such that 0 ≤ start ≤ end < len(L). Elements of L may be integers or strings; treat a string as numeric only if it consists entirely of decimal digits (no signs, decimals, or letters). Compute and return the sum of all numeric elements in L between indices start and end inclusive — converting numeric strings to integers before summing and ignoring any non-numeric elements. If there are no numeric elements in that slice, return 0. Now solve this instance: start = 2, end = 4, L = ['X', '100', 200, '300a', '50', 'Y'].\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is tricky because it mixes several small-but-easily-missed requirements and contains inconsistencies in its examples: ambiguity about whether indices are 0-based or 1-based, inconsistent statements about whether i can equal j, and a mix of data types (integers, numeric strings, and non-numeric tokens). Solving it correctly requires: correctly interpreting the inclusive index range, robustly filtering list elements to identify/convert only numeric values (e.g., numeric strings vs alphabetic tokens), summing those converted numbers, and handling edge cases (no numeric items → return 0). It therefore tests careful reading, precise indexing/slicing, data-type conversion/parsing, and edge-case handling.", "new_prompt": "You are given a list L and two integers start and end such that 0 ≤ start ≤ end < len(L). Elements of L may be integers or strings; treat a string as numeric only if it consists entirely of decimal digits (no signs, decimals, or letters). Compute and return the sum of all numeric elements in L between indices start and end inclusive — converting numeric strings to integers before summing and ignoring any non-numeric elements. If there are no numeric elements in that slice, return 0. Now solve this instance: start = 2, end = 4, L = ['X', '100', 200, '300a', '50', 'Y'].", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, we can are given an english sentence and the goal is to generate a text in 'turkish' language that is a faithful translation of the input sentence\n--------\nQuestion: Fevzi Köse\n\nAnswer: Fevzi Köse\n\n\nQuestion: “Kurds have been fighting the state for almost a century for their mother tongue.\n\nAnswer: “Kürtler 100 yıldır devletle güreşiyorlar, kendi anadillerini kazanabilmek için.\n\n\nQuestion: Marchers came from far and wide\n\nAnswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.46484375, -4.15625, -1.0546875, -0.46484375, -0.46484375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it tests precise, context-sensitive translation from English to Turkish while preserving punctuation, quotation marks, and proper nouns. It requires idiomatic rendering (choosing culturally and grammatically natural phrasing), correct handling of number/tense/plurality, and deciding when to keep names unchanged. Small surface differences (quotes, capitalization, contractions, short idioms like 'far and wide') can lead to varied but potentially incorrect outputs, so the model must balance literal fidelity with natural Turkish phrasing and produce only the translated sentence.\",\n    \"new_prompt\": \"Translate the following English sentence into natural, faithful Turkish. Keep all punctuation and quotation marks as in the source, leave proper names unchanged, and output only the Turkish sentence (no explanations): \\\"Marchers came from far and wide.\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it tests precise, context-sensitive translation from English to Turkish while preserving punctuation, quotation marks, and proper nouns. It requires idiomatic rendering (choosing culturally and grammatically natural phrasing), correct handling of number/tense/plurality, and deciding when to keep names unchanged. Small surface differences (quotes, capitalization, contractions, short idioms like 'far and wide') can lead to varied but potentially incorrect outputs, so the model must balance literal fidelity with natural Turkish phrasing and produce only the translated sentence.", "new_prompt": "Translate the following English sentence into natural, faithful Turkish. Keep all punctuation and quotation marks as in the source, leave proper names unchanged, and output only the Turkish sentence (no explanations): \"Marchers came from far and wide.\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you will be presented with a question having multiple possible answers in French language. And you should choose a most suitable option out of \"A\", \"B\", \"C\", \"D\", and \"E\" based on your commonsense knowledge.\n\n[EX Q]: Question: Où avez-vous pu voir un mammouth qui n'est pas réel ? \n Options: A l'âge de pierre B livre d'histoires C histoire ancienne D musée d'histoire naturelle E l'ère glaciaire \n[EX A]: B\n\n[EX Q]: Question: La robe de Sarah était très ample. Elle préférait cela.  Elle n'aimait pas montrer ses courbes. Quel type de vêtements ne porterait-elle pas ? \n Options: A ajustement des formes B proche C compact D fidèle E discret \n[EX A]: B\n\n[EX Q]: Question: Sauter par-dessus une corde est une forme de quoi ? \n Options: A maux de tête B fun C se réchauffer D exercice E brisé \n[EX A]:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-9.375, -9.375, -9.375, -9.375, -9.375], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires French-language comprehension plus commonsense reasoning about everyday situations. The model must: (1) interpret short, often ambiguous options (lexical semantics and collocations), (2) use world knowledge and pragmatic inference to pick the single best fit among several plausible choices, and (3) handle subtle contrasts (near-synonyms and context-dependent meanings). Errors typically come from misreading nuance, failing to eliminate plausible but less appropriate options, or lacking vocabulary precision in French.\",\n    \"new_prompt\": \"Vous verrez une question en français avec cinq options (A–E). Choisissez la lettre correspondant à la réponse la plus appropriée selon le sens commun. Question: Thomas veut se détendre après une longue journée en lisant quelque chose de court et divertissant. Quel format choisira-t-il probablement ? Options: A roman volumineux B nouvelle C manuel scolaire D essai académique E encyclopédie Répondez par une seule lettre (A, B, C, D ou E).\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires French-language comprehension plus commonsense reasoning about everyday situations. The model must: (1) interpret short, often ambiguous options (lexical semantics and collocations), (2) use world knowledge and pragmatic inference to pick the single best fit among several plausible choices, and (3) handle subtle contrasts (near-synonyms and context-dependent meanings). Errors typically come from misreading nuance, failing to eliminate plausible but less appropriate options, or lacking vocabulary precision in French.", "new_prompt": "Vous verrez une question en français avec cinq options (A–E). Choisissez la lettre correspondant à la réponse la plus appropriée selon le sens commun. Question: Thomas veut se détendre après une longue journée en lisant quelque chose de court et divertissant. Quel format choisira-t-il probablement ? Options: A roman volumineux B nouvelle C manuel scolaire D essai académique E encyclopédie Répondez par une seule lettre (A, B, C, D ou E).", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\n-  You find an empty ice cube tray - You fill the tray with warm water - You carefully place the tray on an even surface in a freezer that is at least 32 degree Farhenheit, the freezing point of water - The water begins to get colder until it starts to freeze solid - The water expands as it freezes, one of the few liquids that has greater volume as a solid  What might be the last step of the process?\n----\nAnswer: Because you used warm water, the gasses in the water had evaporated out, so you will have nice clear ice cubes without cracks from the trapped gasses\n\n\n-  An enzyme in saliva begins to break down carbohydrates - Food is squeezed down the esophagus when you swallow - Acid and enzyme in the stomach break the food down - Releases nutrients - Food then pumped into small intestine - Fluid and enzymes from liver and pancreas pumped into small intestine - Help break food into smallest molecules - Nutrients pass through wall of small intestine - Nutrients absorbed into blood stream  What might be the last step of the process?\n----\nAnswer: Unused materials passed into large intestine to become fecal matter.\n\n\n-  A dam is built in an area prone to flooding - A large amount of rain occurs - The water builds up behind the dam - The dam opens its floodgates when it starts to get full - The water flows into a river below the dam - The dam closes its gates - The dam will occasionally open its gates again to gradually release more water  What might be the last step of the process?\n----\nAnswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-1.71875, -0.01904296875, 0.66796875, 5.03125, 0.6328125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it asks the model to infer the plausible final step of several partially specified causal processes across different domains. It requires: (1) causal and temporal reasoning (predicting the next event that naturally follows a sequence); (2) domain knowledge or commonsense about physical, biological, and engineering processes; (3) selecting an appropriate level of abstraction for the \\\"last step\\\" (outcome vs. immediate next action) despite ambiguity; and (4) applying that reasoning consistently across diverse contexts. Failures typically come from misunderstanding which endpoint is expected, offering irrelevant details, or lacking the specific domain fact needed to choose the correct final action or outcome.\",\n    \"new_prompt\": \"Consider this sequence describing making a loaf of bread: - You mix flour, water, yeast, and salt to form a dough. - You knead the dough until it is smooth and elastic. - You place the dough in a warm bowl and let it rise until doubled in size. - You punch the dough down, shape it into a loaf, and place it in a greased pan. - You let the shaped loaf proof until it puffs up again. - You score the top and bake the loaf at the recommended temperature until the crust is golden and the internal temperature reaches about 190–210°F. What might be the last step of the process?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it asks the model to infer the plausible final step of several partially specified causal processes across different domains. It requires: (1) causal and temporal reasoning (predicting the next event that naturally follows a sequence); (2) domain knowledge or commonsense about physical, biological, and engineering processes; (3) selecting an appropriate level of abstraction for the \"last step\" (outcome vs. immediate next action) despite ambiguity; and (4) applying that reasoning consistently across diverse contexts. Failures typically come from misunderstanding which endpoint is expected, offering irrelevant details, or lacking the specific domain fact needed to choose the correct final action or outcome.", "new_prompt": "Consider this sequence describing making a loaf of bread: - You mix flour, water, yeast, and salt to form a dough. - You knead the dough until it is smooth and elastic. - You place the dough in a warm bowl and let it rise until doubled in size. - You punch the dough down, shape it into a loaf, and place it in a greased pan. - You let the shaped loaf proof until it puffs up again. - You score the top and bake the loaf at the recommended temperature until the crust is golden and the internal temperature reaches about 190–210°F. What might be the last step of the process?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\ninstruction:\nThis task is about translating a given English language sentence to Spanish.\nquestion:\nI don't think the taboo, around what was happening inside and the isolation of citizens from the outer world were so extreme, as it was with Syria.\nanswer:\nYo no creo que el tabú, alrededor del cual estaba pasando dentro y el aislamiento de los ciudadanos del mundo exterior eran tan extremas, como fue con Siria.\n\n\nquestion:\nI get my hands dirty... so she doesn't have to.\nanswer:\nLa única cosa dentro de la tierra és mi mano.\n\n\nquestion:\nShe was very practical about it.\nanswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.609375, 1.609375, 1.609375, 1.609375, 1.609375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires more than word-for-word translation: the model must resolve ellipsis and ambiguous references, render idiomatic expressions naturally in Spanish, preserve tone and register, and correctly handle syntactic differences (e.g., relative clauses, passive/active voice, and pronoun scope). Examples like “I get my hands dirty... so she doesn't have to” use metaphors and ellipses that demand pragmatic inference (who does what, what the metaphorical verbs mean) and selection of appropriate Spanish idioms rather than literal wording. The task thus tests semantic disambiguation, idiomatic equivalence, pronoun/coreference resolution, and producing fluent, natural target-language phrasing.\",\n    \"new_prompt\": \"Translate the following English sentence into natural, idiomatic Spanish, preserving meaning and tone: \\\"He rolled up his sleeves and took care of the messy business himself, so she never had to lift a finger.\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires more than word-for-word translation: the model must resolve ellipsis and ambiguous references, render idiomatic expressions naturally in Spanish, preserve tone and register, and correctly handle syntactic differences (e.g., relative clauses, passive/active voice, and pronoun scope). Examples like “I get my hands dirty... so she doesn't have to” use metaphors and ellipses that demand pragmatic inference (who does what, what the metaphorical verbs mean) and selection of appropriate Spanish idioms rather than literal wording. The task thus tests semantic disambiguation, idiomatic equivalence, pronoun/coreference resolution, and producing fluent, natural target-language phrasing.", "new_prompt": "Translate the following English sentence into natural, idiomatic Spanish, preserving meaning and tone: \"He rolled up his sleeves and took care of the messy business himself, so she never had to lift a finger.\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nTASK DEFINITION: Given a paragraph, your job is to generate a question that can be answered from the passage. The answer to your question should be a single entity, person, time, etc. that can be extracted from the passage.\nPROBLEM: The energy crisis led to greater interest in renewable energy, nuclear power and domestic fossil fuels. There is criticism that American energy policies since the crisis have been dominated by crisis-mentality thinking, promoting expensive quick fixes and single-shot solutions that ignore market and technology realities. Instead of providing stable rules that support basic research while leaving plenty of scope for entrepreneurship and innovation, congresses and presidents have repeatedly backed policies which promise solutions that are politically expedient, but whose prospects are doubtful.\n\nSOLUTION: What was the catalyst that created greater interest in renewable resources?\n\nPROBLEM: A Pharmacy Technician in the UK is considered a health care professional and often does not work under the direct supervision of a pharmacist (if employed in a hospital pharmacy) but instead is supervised and managed by other senior pharmacy technicians. In the UK the role of a PhT has grown and responsibility has been passed on to them to manage the pharmacy department and specialised areas in pharmacy practice allowing pharmacists the time to specialise in their expert field as medication consultants spending more time working with patients and in research. A pharmacy technician once qualified has to register as a professional on the General Pharmaceutical Council (GPhC) register. The GPhC is the governing body for pharmacy health care professionals and this is who regulates the practice of pharmacists and pharmacy technicians.\n\nSOLUTION: Who oversees a Pharmacy Technician in the UK?\n\nPROBLEM: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O\n2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes treated using these devices. Increased O\n2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O\n2 as soon as possible is part of the treatment.\n\nSOLUTION:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.57421875, -0.57421875, -0.57421875, 5.53125, -0.57421875], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires (1) precise reading comprehension to identify a single fact in a short passage that can be requested as a question, (2) constraint-following—producing exactly one question whose answer is a single extractable entity (person, organization, date, etc.)—and (3) avoiding ambiguity so the question has a unique, verbatim extractable answer. Failures often come from generating multiple questions, producing open-ended or yes/no questions, asking for answers not present verbatim in the text, or misunderstanding which kinds of answers count as a \\\"single entity.\\\" The prompt also mixes multiple short passages which can confuse models about scope, so clarity about the single passage and the single-entity constraint is essential.\",\n    \"new_prompt\": \"Read the short paragraph below and write exactly one concise question that can be answered using a single, extractable entity (for example: a person, an organization, a specific year, or a location) taken verbatim from the paragraph. Do not provide the answer—only the question. The question must have a unique, unambiguous answer present in the text.\\n\\nParagraph: \\\"In 2015, the nonprofit Sunrise Arts Center opened its doors in Riverton under the direction of curator Laila Moreno. The center received startup funding from the Greenwood Cultural Trust and staged its inaugural exhibition in November of that year.\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires (1) precise reading comprehension to identify a single fact in a short passage that can be requested as a question, (2) constraint-following—producing exactly one question whose answer is a single extractable entity (person, organization, date, etc.)—and (3) avoiding ambiguity so the question has a unique, verbatim extractable answer. Failures often come from generating multiple questions, producing open-ended or yes/no questions, asking for answers not present verbatim in the text, or misunderstanding which kinds of answers count as a \"single entity.\" The prompt also mixes multiple short passages which can confuse models about scope, so clarity about the single passage and the single-entity constraint is essential.", "new_prompt": "Read the short paragraph below and write exactly one concise question that can be answered using a single, extractable entity (for example: a person, an organization, a specific year, or a location) taken verbatim from the paragraph. Do not provide the answer—only the question. The question must have a unique, unambiguous answer present in the text.\n\nParagraph: \"In 2015, the nonprofit Sunrise Arts Center opened its doors in Riverton under the direction of curator Laila Moreno. The center received startup funding from the Greenwood Cultural Trust and staged its inaugural exhibition in November of that year.\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nQues:Solitude (album) -  Solitude is an album by The The consisting mainly of live performances (some given new titles or remix names). Jealous of Youth and Dolphins appear for the first time on this album rather than as singles. Given a choice of categories company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, the text refers to which one?\n\nAns:Album\n-----\nQues:Geropa -  Geropa concolor is a species of beetle in the family Cerambycidae the only species in the genus Geropa. Given a choice of categories company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, the text refers to which one?\n\nAns:Animal\n-----\nQues:Chambri Lakes -  The Chambri Lakes are a series of swamps and canals in the East Sepik Province of Papua New Guinea that are seasonally filled by the flooding of the Sepik River. The Chambri Lakes form in the middle Sepik between the village of Pagwi and the junction of the Yuat River with the Sepik. The inhabitants of the Chambri Lakes are renowned for their artistry. Given a choice of categories company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, the text refers to which one?\n\nAns:Natural Place\n-----\nQues:Dark Tide: Ruin -  Dark Tide: Ruin (also released as Dark Tide II: Ruin) is the second novel in a two-part story by Michael A. Stackpole. Published and released in 2000 it is the third installment of the New Jedi Order series set in the Star Wars galaxy. Given a choice of categories company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, the text refers to which one?\n\nAns:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.171875, 1.046875, 1.046875, 2.171875, 1.046875], "model_raw_response": "{\n    \"analysis\": \"This task is a constrained entity-type classification: given a short, Wikipedia-style description, the model must pick one label from a small, fixed taxonomy. It is challenging because it requires: (1) precise reading comprehension to latch onto decisive lexical cues (e.g., \\\"novel\\\", \\\"album\\\", \\\"species\\\", \\\"swamps\\\"); (2) world-knowledge and named-entity disambiguation when the same surface string can denote multiple types (title vs. band vs. place vs. species); (3) prioritization of the primary referent (e.g., a work that was later adapted into a film should still be labeled \\\"written work\\\"); and (4) robustness to sparse context and sometimes subtle distinctions between similar categories (natural place vs. village, album vs. written work vs. film). Models often fail when they overfit to incidental words (like \\\"adapted\\\" or shared names) or when they lack the taxonomy-mapping skill to map phrases such as \\\"is a species of beetle\\\" -> animal or \\\"is the second novel\\\" -> written work.\",\n    \"new_prompt\": \"Ques: The Stone Sky - The Stone Sky is the third novel in N. K. Jemisin's Broken Earth trilogy, published in 2017; it concludes the series and is discussed mainly as a literary work of speculative fiction. Given a choice of categories company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, the text refers to which one?\\n\\nAns:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is a constrained entity-type classification: given a short, Wikipedia-style description, the model must pick one label from a small, fixed taxonomy. It is challenging because it requires: (1) precise reading comprehension to latch onto decisive lexical cues (e.g., \"novel\", \"album\", \"species\", \"swamps\"); (2) world-knowledge and named-entity disambiguation when the same surface string can denote multiple types (title vs. band vs. place vs. species); (3) prioritization of the primary referent (e.g., a work that was later adapted into a film should still be labeled \"written work\"); and (4) robustness to sparse context and sometimes subtle distinctions between similar categories (natural place vs. village, album vs. written work vs. film). Models often fail when they overfit to incidental words (like \"adapted\" or shared names) or when they lack the taxonomy-mapping skill to map phrases such as \"is a species of beetle\" -> animal or \"is the second novel\" -> written work.", "new_prompt": "Ques: The Stone Sky - The Stone Sky is the third novel in N. K. Jemisin's Broken Earth trilogy, published in 2017; it concludes the series and is discussed mainly as a literary work of speculative fiction. Given a choice of categories company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, the text refers to which one?\n\nAns:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nUsing only three words, describe the act of consuming a large amount of sustenance while in a public dining establishment.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [4.71875, 0.57421875, -0.2294921875, -0.419921875, 0.57421875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines strict formatting constraints (produce exactly three words and nothing more) with creative compression: the model must pick concise, idiomatic vocabulary that captures a specific scenario (overeating in a public dining setting). It tests the model's ability to follow an exact output-length constraint, understand and rephrase a detailed situation succinctly, and avoid adding extra punctuation or commentary—errors that many models make when they either over-explain or miscount words.\",\n    \"new_prompt\": \"In exactly three words, name the act of rapidly consuming an excessively large meal while seated in a busy restaurant; reply with only those three words and no additional text.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines strict formatting constraints (produce exactly three words and nothing more) with creative compression: the model must pick concise, idiomatic vocabulary that captures a specific scenario (overeating in a public dining setting). It tests the model's ability to follow an exact output-length constraint, understand and rephrase a detailed situation succinctly, and avoid adding extra punctuation or commentary—errors that many models make when they either over-explain or miscount words.", "new_prompt": "In exactly three words, name the act of rapidly consuming an excessively large meal while seated in a busy restaurant; reply with only those three words and no additional text.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nConstruct a sentence using the active voice and the given words.\nA noise was heard<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.466796875, -0.466796875, 2.1875, -0.466796875, -0.466796875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it contains conflicting constraints: it requires an active-voice sentence while also asking the model to use the given words verbatim (or very closely). That forces the model to balance instruction-following, syntactic transformation (converting passive to active), paraphrasing/minimal-editing, and semantic preservation (keeping the original meaning and as many original tokens as possible). Models typically fail by either (a) obeying the \\\"use the given words\\\" constraint and producing a passive construction, or (b) producing an active sentence that ignores or replaces the provided words. The task therefore tests instruction prioritization, grammatical rewriting (passive→active), and handling underspecified or contradictory user requirements.\",\n    \"new_prompt\": \"Rewrite the phrase 'A decision was made' as a grammatically correct sentence in the active voice, preserving as many of the original words as possible.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it contains conflicting constraints: it requires an active-voice sentence while also asking the model to use the given words verbatim (or very closely). That forces the model to balance instruction-following, syntactic transformation (converting passive to active), paraphrasing/minimal-editing, and semantic preservation (keeping the original meaning and as many original tokens as possible). Models typically fail by either (a) obeying the \"use the given words\" constraint and producing a passive construction, or (b) producing an active sentence that ignores or replaces the provided words. The task therefore tests instruction prioritization, grammatical rewriting (passive→active), and handling underspecified or contradictory user requirements.", "new_prompt": "Rewrite the phrase 'A decision was made' as a grammatically correct sentence in the active voice, preserving as many of the original words as possible.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you're given a review from Amazon and your task is to generate the name of the category of the product based on the review given by the user. The categories are: kitchen, office product, watch, wireless, other, toy, digital video download, camera, jewelry, pet products, sports, industrial supplies, baby product, grocery, drugstore, home improvement, pc, shoes, automotive, digital ebook purchase, musical instruments, beauty, book, electronics, lawn and garden, apparel, home, video games, luggage, furniture, personal care appliances.\n\nBought five. Two came apart within two days. Third one now failing after 1 1/2 months. Coming apart and it will not stay up. The others two will not stay open either; more like a paper curtain. Cannot return. Do not purchase.\nhome improvement\n\nThis safe is useless. My house was broken into and this safe did not deter the thief from my gun. They ripped the cable completely off, bent the top, broke the lock and took the gun, no problem. Do not think your gun will be protected with this safe. Would only be good for keeping young kids away from your gun. I will not be purchasing this brand ever again.\nhome\n\nLike the most I found the item for chapin sprayer. The only problem it did not do what it was made to do. The spray did not adjust for cone spray. Now they say it is not returnable. For a couple of dollars you lost a customer.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.53125, -4.53125, -4.53125, -2.109375, -2.109375], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires fine-grained text classification from noisy, real-world reviews rather than explicit product labels. Skills tested include: (1) pragmatic inference — deducing the likely product type from contextual clues (e.g., “safe”, “lock”, “sprayer”, “gun”) rather than explicit category names; (2) taxonomy mapping — selecting the single best label from a large, overlapping set of granular categories (e.g., home vs. home improvement vs. luggage vs. jewelry); (3) handling ambiguity, negation and sentiment — reviews often describe failure modes, returns, or use-cases that must be interpreted; (4) domain awareness — knowing which product types would realistically match described behaviors; and (5) constraint following — returning exactly one category from the provided list. Models commonly fail when reviews omit explicit product names, contain ambiguous cues or reference multiple potential use-cases, causing confusion among similar categories.\",\n    \"new_prompt\": \"You are given a single Amazon product review. From the list of categories below, choose the one category that best describes the product being reviewed. Reply with only the category name (no explanation). Categories: kitchen, office product, watch, wireless, other, toy, digital video download, camera, jewelry, pet products, sports, industrial supplies, baby product, grocery, drugstore, home improvement, pc, shoes, automotive, digital ebook purchase, musical instruments, beauty, book, electronics, lawn and garden, apparel, home, video games, luggage, furniture, personal care appliances. Review: \\\"I bought this small lockbox to stash passports and a few pieces of jewelry while traveling and for use at home. It promised a hardened steel cable and tamper-resistant lock, but at my hotel someone pried it open, bent the case and snapped the cable—everything was stolen. The lock feels flimsy and the whole thing is useless for protecting valuables.\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires fine-grained text classification from noisy, real-world reviews rather than explicit product labels. Skills tested include: (1) pragmatic inference — deducing the likely product type from contextual clues (e.g., “safe”, “lock”, “sprayer”, “gun”) rather than explicit category names; (2) taxonomy mapping — selecting the single best label from a large, overlapping set of granular categories (e.g., home vs. home improvement vs. luggage vs. jewelry); (3) handling ambiguity, negation and sentiment — reviews often describe failure modes, returns, or use-cases that must be interpreted; (4) domain awareness — knowing which product types would realistically match described behaviors; and (5) constraint following — returning exactly one category from the provided list. Models commonly fail when reviews omit explicit product names, contain ambiguous cues or reference multiple potential use-cases, causing confusion among similar categories.", "new_prompt": "You are given a single Amazon product review. From the list of categories below, choose the one category that best describes the product being reviewed. Reply with only the category name (no explanation). Categories: kitchen, office product, watch, wireless, other, toy, digital video download, camera, jewelry, pet products, sports, industrial supplies, baby product, grocery, drugstore, home improvement, pc, shoes, automotive, digital ebook purchase, musical instruments, beauty, book, electronics, lawn and garden, apparel, home, video games, luggage, furniture, personal care appliances. Review: \"I bought this small lockbox to stash passports and a few pieces of jewelry while traveling and for use at home. It promised a hardened steel cable and tamper-resistant lock, but at my hotel someone pried it open, bent the case and snapped the cable—everything was stolen. The lock feels flimsy and the whole thing is useless for protecting valuables.\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task you will be given an arithmetic operation and you have to find its answer. The symbols of operators '+' and '-' has been swapped i.e you need to perform subtraction when you see a '+' symbol and  addition in case of '-' symbol.\n\nEx Input:\n519 - 9471 - 7173 - 34 - 5123 + 9104 - 2340 + 7208 - 3693\n\nEx Output:\n12041\n\n\nEx Input:\n5798 + 7484 + 9307 + 3033 - 7750 + 6782 + 2330 + 2957 - 714\n\nEx Output:\n-17631\n\n\nEx Input:\n4345 - 6464 + 9049 - 7722\n\nEx Output:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-8.125, -10.4375, -7.875, -8.6875, -10.8125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires the model to override its default interpretation of arithmetic symbols (a symbol-to-operation remapping), then accurately parse and evaluate a long sequence of multi-digit integers while preserving order. It tests: (1) following an unusual negative constraint (swap meanings of '+' and '-'), (2) precise symbolic reasoning and mapping, (3) attention and working memory for many terms, and (4) exact integer arithmetic (including sign handling and potential negative results). Models often fail by neglecting the operator swap, mis-parsing signs, or making arithmetic mistakes over long chains.\",\n    \"new_prompt\": \"You have a quirky calculator where the plus key performs subtraction and the minus key performs addition (so every '+' in the expression means '−' and every '-' means '+'). Carefully compute the final integer value of the expression below and give only the numeric result:\\n\\n8721 + 405 - 9999 - 1234 + 5600 - 233 + 7777 + 100 - 6453 - 88\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires the model to override its default interpretation of arithmetic symbols (a symbol-to-operation remapping), then accurately parse and evaluate a long sequence of multi-digit integers while preserving order. It tests: (1) following an unusual negative constraint (swap meanings of '+' and '-'), (2) precise symbolic reasoning and mapping, (3) attention and working memory for many terms, and (4) exact integer arithmetic (including sign handling and potential negative results). Models often fail by neglecting the operator swap, mis-parsing signs, or making arithmetic mistakes over long chains.", "new_prompt": "You have a quirky calculator where the plus key performs subtraction and the minus key performs addition (so every '+' in the expression means '−' and every '-' means '+'). Carefully compute the final integer value of the expression below and give only the numeric result:\n\n8721 + 405 - 9999 - 1234 + 5600 - 233 + 7777 + 100 - 6453 - 88", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn addition to the inquiry of the main character traits of the characters in Finding Nemo, can you provide a more in-depth analysis of how these traits specifically impact the overall plot and character development? Furthermore, could you delve deeper into any underlying themes or symbolism present in the characterizations and how they contribute to the overall message of the story? Please utilize the provided [excel table] to create a comprehensive breakdown of each character's name, their main character traits, the specific impact these traits have on plot and character development, and any symbolism or underlying themes present in their characterization. Thank you.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [5.3125, -7.375, -2.484375, 7.34375, 7.28125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines deep literary interpretation with structured data output: the model must (1) identify and summarize salient character traits from a well-known work, (2) perform causal reasoning to explain how those traits concretely affect plot events and character development (requiring multi-step inference and evidence-based examples), (3) analyze themes and symbolism and link those abstractions back to characterizations, and (4) present the results in a precise tabular format (using a provided spreadsheet). Failure can come from hallucinating unsupported details, being too vague when mapping traits to plot, misunderstanding the requirement to use the provided table, or producing output that doesn’t match the expected row/column structure. The task therefore tests domain knowledge, causal and narrative reasoning, synthesis of evidence, concise explanatory writing, and the ability to follow formatting/usage constraints for an external data artifact.\",\n    \"new_prompt\": \"Using the attached spreadsheet, for each major character in The Wizard of Oz fill these columns—Character Name; Primary Traits (2–3 concise traits); How each trait specifically drives plot events and shapes that character’s development (cite concrete plot examples); and Symbolism or underlying themes the character embodies and how that symbolism reinforces the story’s central message—provide one-to-two sentence, evidence-based entries per cell and do not invent facts beyond the text?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines deep literary interpretation with structured data output: the model must (1) identify and summarize salient character traits from a well-known work, (2) perform causal reasoning to explain how those traits concretely affect plot events and character development (requiring multi-step inference and evidence-based examples), (3) analyze themes and symbolism and link those abstractions back to characterizations, and (4) present the results in a precise tabular format (using a provided spreadsheet). Failure can come from hallucinating unsupported details, being too vague when mapping traits to plot, misunderstanding the requirement to use the provided table, or producing output that doesn’t match the expected row/column structure. The task therefore tests domain knowledge, causal and narrative reasoning, synthesis of evidence, concise explanatory writing, and the ability to follow formatting/usage constraints for an external data artifact.", "new_prompt": "Using the attached spreadsheet, for each major character in The Wizard of Oz fill these columns—Character Name; Primary Traits (2–3 concise traits); How each trait specifically drives plot events and shapes that character’s development (cite concrete plot examples); and Symbolism or underlying themes the character embodies and how that symbolism reinforces the story’s central message—provide one-to-two sentence, evidence-based entries per cell and do not invent facts beyond the text?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nOk, this is a great start — could you add more detail to each module and provide a brief a overview for each bullet point within each module?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-1.59375, -10.0625, -6.21875, -10.0625, -9.5625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires nested, hierarchical content generation: the model must (1) expand each module into a richer, coherent description and (2) simultaneously summarize every bullet within each module into brief, accurate overviews. That tests skills in understanding structure, maintaining consistency across levels of detail, concise summarization, and inferring missing context (what the user means by “more detail”). It also demands organization, controlled verbosity, and the ability to transform a terse outline into clear, actionable prose.\",\n    \"new_prompt\": \"I have the following 4-module workshop outline. For each module, write a concise expanded module description (3–4 sentences) that clarifies the module's scope, learning goals, and expected outcomes; then, for every bullet point listed under that module, provide a one-sentence explanation that clearly states what that bullet covers. Present your response module-by-module, with the expanded description followed by the one-sentence clarifications for each bullet. Workshop outline: Module 1 — \\\"Introduction to Product Design\\\": • User research • Problem framing • Ideation; Module 2 — \\\"Prototyping & Testing\\\": • Low-fidelity prototypes • Usability testing • Rapid iteration; Module 3 — \\\"Data-Informed Decision Making\\\": • Choosing metrics • A/B testing basics • Interpreting analytics; Module 4 — \\\"Scaling & Launch\\\": • Roadmapping and prioritization • Communicating with stakeholders • Go-to-market considerations?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires nested, hierarchical content generation: the model must (1) expand each module into a richer, coherent description and (2) simultaneously summarize every bullet within each module into brief, accurate overviews. That tests skills in understanding structure, maintaining consistency across levels of detail, concise summarization, and inferring missing context (what the user means by “more detail”). It also demands organization, controlled verbosity, and the ability to transform a terse outline into clear, actionable prose.", "new_prompt": "I have the following 4-module workshop outline. For each module, write a concise expanded module description (3–4 sentences) that clarifies the module's scope, learning goals, and expected outcomes; then, for every bullet point listed under that module, provide a one-sentence explanation that clearly states what that bullet covers. Present your response module-by-module, with the expanded description followed by the one-sentence clarifications for each bullet. Workshop outline: Module 1 — \"Introduction to Product Design\": • User research • Problem framing • Ideation; Module 2 — \"Prototyping & Testing\": • Low-fidelity prototypes • Usability testing • Rapid iteration; Module 3 — \"Data-Informed Decision Making\": • Choosing metrics • A/B testing basics • Interpreting analytics; Module 4 — \"Scaling & Launch\": • Roadmapping and prioritization • Communicating with stakeholders • Go-to-market considerations?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\n[Q]: How does the sentence end?\n\nKevin was more supportive of the event than Aaron because it was \nOPTIONS:\n- Kevin political party hosting the event.\n- Aaron political party hosting the event.\n\n[A]: Kevin political party hosting the event.\n\n\n[Q]: How does the sentence end?\n\nAngela was installing a bathtub for Patricia for \nOPTIONS:\n- Angela was the buyer for the new house.\n- Patricia was the buyer for the new house.\n\n[A]: Patricia was the buyer for the new house.\n\n\n[Q]: How does the sentence end?\n\nCarrie asked Erin how many calories were in the chicken salad, but \nOPTIONS:\n- Carrie had forgotten.\n- Erin had forgotten.\n\n[A]:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-7.125, -6.65625, -7.125, -7.125, -7.125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires more than surface-level pattern matching: the model must resolve which participant a following causal clause refers to (implicit causality/coreference), correctly attach the reason clause in a comparative construction, and apply commonsense knowledge about roles, ownership, or responsibility to pick the plausible completion. That combines syntactic parsing (whose action/possession is being described), pragmatic inference (what would make someone more/less supportive or the reason for an action), and world knowledge (e.g., you install a bathtub for the buyer, you support an event more if your side is hosting). Models often fail because the cues are subtle and the correct choice depends on interpreting who benefits or is responsible, not just lexical overlap.\",\n    \"new_prompt\": \"Choose the single option that best completes the sentence so it makes sense. \\n\\nSam volunteered to help Mia move into the house because\\nOPTIONS:\\n- Sam had signed the lease for the new house.\\n- Mia had signed the lease for the new house.\\n\\nAnswer with the exact option text that best finishes the sentence.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires more than surface-level pattern matching: the model must resolve which participant a following causal clause refers to (implicit causality/coreference), correctly attach the reason clause in a comparative construction, and apply commonsense knowledge about roles, ownership, or responsibility to pick the plausible completion. That combines syntactic parsing (whose action/possession is being described), pragmatic inference (what would make someone more/less supportive or the reason for an action), and world knowledge (e.g., you install a bathtub for the buyer, you support an event more if your side is hosting). Models often fail because the cues are subtle and the correct choice depends on interpreting who benefits or is responsible, not just lexical overlap.", "new_prompt": "Choose the single option that best completes the sentence so it makes sense. \n\nSam volunteered to help Mia move into the house because\nOPTIONS:\n- Sam had signed the lease for the new house.\n- Mia had signed the lease for the new house.\n\nAnswer with the exact option text that best finishes the sentence.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nI want you to act as a schema markup generator for a website. Your replies should only include the appropriate schema markup code for each sentence I provide, and should not include any additional explanations or text. Your responses should be in valid HTML format.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-7.1875, -7.1875, -7.1875, -0.94140625, -0.94140625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines strict negative constraints (the model must output only code, no explanations) with deep domain knowledge (correct use of schema.org vocabulary and valid HTML, often choosing between JSON-LD and microdata). It tests precise parsing and translation of natural-language facts into structured-schema types/properties, handling ambiguity about entities/relations, and producing syntactically valid HTML snippets. Small formatting or lexical deviations (extra words, comments, or invalid JSON/HTML) will be judged as failures, so the task stresses exactness and careful instruction-following.\",\n    \"new_prompt\": \"Can you convert the following single sentence into a minimal, valid HTML fragment that encodes its facts using schema.org structured data (either JSON-LD in a <script type=\\\"application/ld+json\\\"> tag or HTML+microdata)? Output only the HTML markup required to represent the sentence—no explanations, no comments, and no additional text. Sentence: \\\"[INSERT SENTENCE HERE]\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines strict negative constraints (the model must output only code, no explanations) with deep domain knowledge (correct use of schema.org vocabulary and valid HTML, often choosing between JSON-LD and microdata). It tests precise parsing and translation of natural-language facts into structured-schema types/properties, handling ambiguity about entities/relations, and producing syntactically valid HTML snippets. Small formatting or lexical deviations (extra words, comments, or invalid JSON/HTML) will be judged as failures, so the task stresses exactness and careful instruction-following.", "new_prompt": "Can you convert the following single sentence into a minimal, valid HTML fragment that encodes its facts using schema.org structured data (either JSON-LD in a <script type=\"application/ld+json\"> tag or HTML+microdata)? Output only the HTML markup required to represent the sentence—no explanations, no comments, and no additional text. Sentence: \"[INSERT SENTENCE HERE]\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nHow do I set it to allow all tags and all attributes?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [13.375, 1.828125, 0.294921875, -0.37109375, -0.08349609375], "model_raw_response": "{\n    \"analysis\": \"The prompt is challenging because it's vague (what does “it” refer to?) and asks for an action that is typically unsafe (enabling all HTML tags/attributes). Solving it requires: (1) disambiguation and asking concise clarifying questions about context (framework/library, where content comes from), (2) web-security domain knowledge (sanitizers, XSS risks), and (3) the ability to refuse or steer away from insecure commands while offering safe, practical alternatives and concrete configuration guidance.\",\n    \"new_prompt\": \"A user wrote: \\\"How do I set it to allow all tags and all attributes?\\\" — assume they mean an HTML sanitizer in a web application. Produce a single, concise assistant reply that (a) asks one or two brief clarifying questions needed to proceed, (b) explains why permitting all tags/attributes is dangerous, and (c) gives a safe alternative approach plus one concrete configuration example (for a single popular sanitizer) that provides flexibility without opening XSS vulnerabilities.\"\n}", "api_error": null, "parse_status": "success", "analysis": "The prompt is challenging because it's vague (what does “it” refer to?) and asks for an action that is typically unsafe (enabling all HTML tags/attributes). Solving it requires: (1) disambiguation and asking concise clarifying questions about context (framework/library, where content comes from), (2) web-security domain knowledge (sanitizers, XSS risks), and (3) the ability to refuse or steer away from insecure commands while offering safe, practical alternatives and concrete configuration guidance.", "new_prompt": "A user wrote: \"How do I set it to allow all tags and all attributes?\" — assume they mean an HTML sanitizer in a web application. Produce a single, concise assistant reply that (a) asks one or two brief clarifying questions needed to proceed, (b) explains why permitting all tags/attributes is dangerous, and (c) gives a safe alternative approach plus one concrete configuration example (for a single popular sanitizer) that provides flexibility without opening XSS vulnerabilities.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nGiven a sentence in Hindi(hi), give the translation of the sentence in English(en) as output\n\nLet me give you an example: आप सबको दीपावली की बहुत बहुत शुभकामनाए\nThe answer to this example can be: On the pious festival of Deepawali, greetings to all of you.\nHere is why: The translation was accurate, so we can say that the translation was successful\n\nOK. solve this:\nलक्षद्वीप जैसी छोटी जगह के लिए भी उन्होंने महत्वपूर्ण भूमिका निभाई थी \nAnswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.46875, 4.1875, 2.125, 2.46875, 2.125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires bilingual fluency and nuanced, idiomatic translation rather than literal word-for-word rendering. The model must correctly parse Hindi grammar (including emphasis particles like \\\"भी\\\" and comparative constructs like \\\"जैसी\\\"), choose appropriate tense and articles in English, and transliterate or render proper nouns naturally (e.g., \\\"लक्षद्वीप\\\" → \\\"Lakshadweep\\\"). It also tests handling of implicit or ambiguous subjects and preserving the original emphasis and register in the target language.\",\n    \"new_prompt\": \"Translate the following Hindi sentence into fluent, natural English, preserving emphasis and properly rendering any proper nouns: बड़ी संस्थाओं के साथ-साथ उन्होंने लक्षद्वीप जैसी छोटी जगहों के विकास में भी महत्वपूर्ण योगदान दिया।\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires bilingual fluency and nuanced, idiomatic translation rather than literal word-for-word rendering. The model must correctly parse Hindi grammar (including emphasis particles like \"भी\" and comparative constructs like \"जैसी\"), choose appropriate tense and articles in English, and transliterate or render proper nouns naturally (e.g., \"लक्षद्वीप\" → \"Lakshadweep\"). It also tests handling of implicit or ambiguous subjects and preserving the original emphasis and register in the target language.", "new_prompt": "Translate the following Hindi sentence into fluent, natural English, preserving emphasis and properly rendering any proper nouns: बड़ी संस्थाओं के साथ-साथ उन्होंने लक्षद्वीप जैसी छोटी जगहों के विकास में भी महत्वपूर्ण योगदान दिया।", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nThank you for your advice. Now I want to change topic to another one, can you improve my expression of the latest question? Because I am not a native English speaker, I wonder if there are some better ways to describe that question for a native speaker, thank you/<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-3.28125, -2.296875, -3.671875, -2.984375, -3.28125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires the model to (1) identify and preserve the user's intended meaning without explicit context (the phrase \\\"latest question\\\" is unspecified), (2) perform nuanced rewriting for grammar, register, concision and politeness appropriate to a native speaker, and (3) follow meta-instructions about what kind of output to produce (e.g., a single improved phrasing). Common failure modes are hallucinating or altering the meaning, producing multiple variants when only one is wanted, or failing to ask for clarification when the referent is missing.\",\n    \"new_prompt\": \"Please rewrite the following sentence written by a non-native English speaker so it sounds natural, concise, and polite for a professional email—preserve the original meaning and formality; provide a single polished version and one brief sentence explaining the primary change: 'Thank you for your advice. I'd like to change the topic—could you help me improve my last question? I'm not a native English speaker and wonder if there's a clearer way to say it, thank you.'?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires the model to (1) identify and preserve the user's intended meaning without explicit context (the phrase \"latest question\" is unspecified), (2) perform nuanced rewriting for grammar, register, concision and politeness appropriate to a native speaker, and (3) follow meta-instructions about what kind of output to produce (e.g., a single improved phrasing). Common failure modes are hallucinating or altering the meaning, producing multiple variants when only one is wanted, or failing to ask for clarification when the referent is missing.", "new_prompt": "Please rewrite the following sentence written by a non-native English speaker so it sounds natural, concise, and polite for a professional email—preserve the original meaning and formality; provide a single polished version and one brief sentence explaining the primary change: 'Thank you for your advice. I'd like to change the topic—could you help me improve my last question? I'm not a native English speaker and wonder if there's a clearer way to say it, thank you.'?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIdentify the type of poem from the following poem excerpt.\nIn Xanadu did Kubla Khan\nA stately pleasure-dome decree;<end_of_turn>\n<start_of_turn>model\n", "original_scores": [0.08740234375, -3.609375, 3.859375, 1.0390625, 1.359375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires domain-specific literary knowledge and the ability to infer a poem's form from a very short excerpt. Skills tested include: recognizing poetic forms (sonnet, ode, ballad, lyric, etc.), scanning meter and rhyme with limited lines, and distinguishing overlapping categories (e.g., lyric vs. ode). Ambiguity from brief, decontextualized lines and older diction (e.g., \\\"Kubla Khan\\\") can mislead models that lack robust knowledge of poetic structures or that overfit to surface cues (author names, keywords) rather than metrical and formal features.\",\n    \"new_prompt\": \"Identify the poetic form of the following excerpt by choosing from: sonnet, ode, ballad, epic, lyric, elegy, free verse, haiku. Give the form and one brief textual feature from the excerpt that supports your answer.\\n\\n\\\"Shall I compare thee to a summer's day?\\nThou art more lovely and more temperate;\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires domain-specific literary knowledge and the ability to infer a poem's form from a very short excerpt. Skills tested include: recognizing poetic forms (sonnet, ode, ballad, lyric, etc.), scanning meter and rhyme with limited lines, and distinguishing overlapping categories (e.g., lyric vs. ode). Ambiguity from brief, decontextualized lines and older diction (e.g., \"Kubla Khan\") can mislead models that lack robust knowledge of poetic structures or that overfit to surface cues (author names, keywords) rather than metrical and formal features.", "new_prompt": "Identify the poetic form of the following excerpt by choosing from: sonnet, ode, ballad, epic, lyric, elegy, free verse, haiku. Give the form and one brief textual feature from the excerpt that supports your answer.\n\n\"Shall I compare thee to a summer's day?\nThou art more lovely and more temperate;\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task you are given a question. You need to generate an answer to the question.\n--------\nQuestion: Question:What football team did Arsene Wenger manage immediately before Arsenal?\n\nAnswer: arsenal\n\n\nQuestion: Question:The mountain massif called Maxwell Montes contains the highest point on the surface of which planet?\n\nAnswer: Venus\n\n\nQuestion: Question:Which singer made the news in September 2009 when Muslim groups protested that her first concert in Malaysia should be cancelled for a second time?\n\nAnswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-8.875, -10.5, -10.5, -12.8125, -10.5], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it relies on precise, factual world knowledge across different domains (sports history, planetary geography, recent news) and on correctly interpreting temporal phrasing like “immediately before.” It also exposes model weaknesses with named-entity retrieval and disambiguation (giving the wrong entity due to pattern overfitting). The mixed examples format can encourage shortcut learning (e.g., copying prior answers or following superficial patterns) rather than retrieving the specific factual answer requested.\",\n    \"new_prompt\": \"In this task you are given a question. Provide a concise answer (only the name requested).\\n\\nQuestion: Which football club did José Mourinho manage immediately before taking the head coach job at Real Madrid?\\n\\nAnswer:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it relies on precise, factual world knowledge across different domains (sports history, planetary geography, recent news) and on correctly interpreting temporal phrasing like “immediately before.” It also exposes model weaknesses with named-entity retrieval and disambiguation (giving the wrong entity due to pattern overfitting). The mixed examples format can encourage shortcut learning (e.g., copying prior answers or following superficial patterns) rather than retrieving the specific factual answer requested.", "new_prompt": "In this task you are given a question. Provide a concise answer (only the name requested).\n\nQuestion: Which football club did José Mourinho manage immediately before taking the head coach job at Real Madrid?\n\nAnswer:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you need to count the number of nouns/verbs in the given sentence.\n\nSentence: 'A couple of street signs with one that has a broken plate'. Count the number of verbs in this sentence.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [7.90625, -4.46875, -4.125, -4.46875, 6.53125], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires precise syntactic analysis and part-of-speech disambiguation rather than simple word spotting. Models must (a) identify verbs in different morphological forms (finite verbs, auxiliaries, participles), (b) distinguish verb tokens from verb-derived words used as nouns or adjectives (gerunds, participial adjectives), and (c) count token instances correctly. Ambiguous forms (e.g., \\\"running\\\", \\\"broken\\\") and multiword verb constructions (auxiliary + participle) are common failure points.\",\n    \"new_prompt\": \"Sentence: \\\"The committee has been considering several options while the secretary typed notes and a few members remained silent.\\\" How many verbs appear in this sentence? Count as verbs any words that function grammatically as verbs (including auxiliaries and participles that form verb phrases), but do not count verb-derived nouns or adjectives. Answer with the integer number only.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires precise syntactic analysis and part-of-speech disambiguation rather than simple word spotting. Models must (a) identify verbs in different morphological forms (finite verbs, auxiliaries, participles), (b) distinguish verb tokens from verb-derived words used as nouns or adjectives (gerunds, participial adjectives), and (c) count token instances correctly. Ambiguous forms (e.g., \"running\", \"broken\") and multiword verb constructions (auxiliary + participle) are common failure points.", "new_prompt": "Sentence: \"The committee has been considering several options while the secretary typed notes and a few members remained silent.\" How many verbs appear in this sentence? Count as verbs any words that function grammatically as verbs (including auxiliaries and participles that form verb phrases), but do not count verb-derived nouns or adjectives. Answer with the integer number only.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nInstructions: Given a sentence in the Japanese, provide an equivalent translation in Filipino that retains the same meaning through the translation. In translation, keep numbers as it is.\nInput: カルロス・エドワードとカリブのバック・フォアによるいくつかの素晴らしい守備が、史上初めて小さな島国のワールドカップトーナメントでのヨーロッパの得点を退ける事ができた。\nOutput:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.34375, 2.546875, -0.07958984375, 3.875, 2.359375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it tests multilingual translation (Japanese → Filipino) with several complicating factors: long, nested clauses and passive constructions that require correct scope and word order; domain-specific sports vocabulary and loanwords (e.g., \\\"バック・フォア\\\"/\\\"バックライン\\\") and foreign personal names in katakana that must be preserved or rendered naturally; subtle scope/attachment ambiguities (what modifies what—e.g., which tournament, which team’s scoring, and the timing/’first time’ constraint); and a hard constraint to keep any numbers exactly as written. Success requires precise syntactic parsing, correct disambiguation of noun-phrase boundaries, idiomatic target-language phrasing, and strict adherence to the numeric-preservation rule.\",\n    \"new_prompt\": \"Translate the following Japanese sentence into natural Filipino while preserving the original meaning exactly and keeping any numbers unchanged: ロドリゲスとカリブ出身のバックライン3人によるいくつかの見事な守備が、史上初めてその小さな島国で行われたワールドカップ予選で欧州勢の得点を抑えることに成功した。?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it tests multilingual translation (Japanese → Filipino) with several complicating factors: long, nested clauses and passive constructions that require correct scope and word order; domain-specific sports vocabulary and loanwords (e.g., \"バック・フォア\"/\"バックライン\") and foreign personal names in katakana that must be preserved or rendered naturally; subtle scope/attachment ambiguities (what modifies what—e.g., which tournament, which team’s scoring, and the timing/’first time’ constraint); and a hard constraint to keep any numbers exactly as written. Success requires precise syntactic parsing, correct disambiguation of noun-phrase boundaries, idiomatic target-language phrasing, and strict adherence to the numeric-preservation rule.", "new_prompt": "Translate the following Japanese sentence into natural Filipino while preserving the original meaning exactly and keeping any numbers unchanged: ロドリゲスとカリブ出身のバックライン3人によるいくつかの見事な守備が、史上初めてその小さな島国で行われたワールドカップ予選で欧州勢の得点を抑えることに成功した。?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nGiven an input word generate a word that rhymes exactly with the input word. If not rhyme is found return \"No\"\n\nyard\nbarred\n\ncheck\nsec\n\nmouth<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-3.671875, -3.53125, -3.734375, -3.671875, -3.671875], "model_raw_response": "{\n    \"analysis\": \"This task is hard because it requires phonological knowledge (recognizing perfect rhymes by pronunciation, not spelling), handling dialect and pronunciation variants (rhotic vs non-rhotic accents, alternative pronunciations), and obeying a strict output constraint (return a single word only, or exactly the token No when no common perfect rhyme exists). Models often fail because training data emphasizes spelling similarity over phonetics, confuses perfect rhymes with near/slant rhymes, or do not follow the negative/formatting constraint precisely.\",\n    \"new_prompt\": \"Provide a single English word that is a perfect rhyme for the target word based on standard General American pronunciation (i.e., the final stressed vowel and any following consonants must match exactly). Reply with only that one word in lowercase. If there is no common single-word perfect rhyme, reply exactly with No. Target word: silver\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is hard because it requires phonological knowledge (recognizing perfect rhymes by pronunciation, not spelling), handling dialect and pronunciation variants (rhotic vs non-rhotic accents, alternative pronunciations), and obeying a strict output constraint (return a single word only, or exactly the token No when no common perfect rhyme exists). Models often fail because training data emphasizes spelling similarity over phonetics, confuses perfect rhymes with near/slant rhymes, or do not follow the negative/formatting constraint precisely.", "new_prompt": "Provide a single English word that is a perfect rhyme for the target word based on standard General American pronunciation (i.e., the final stressed vowel and any following consonants must match exactly). Reply with only that one word in lowercase. If there is no common single-word perfect rhyme, reply exactly with No. Target word: silver", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nWhat does OOPS as a development methodology stand for?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-1.125, 0.66015625, 0.83203125, 3.546875, -1.265625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it asks the model to supply the expansion of a short acronym that may not correspond to any standardized development methodology. It therefore tests domain knowledge of software engineering terminology, the ability to detect ambiguity or the absence of a canonical answer, and safe-answer behavior (i.e., avoiding hallucination and asking for clarification or explicitly saying no standard meaning exists). Models that default to guessing will produce incorrect or fabricated expansions, so correct handling requires ambiguity detection and cautious reporting of alternatives or provenance.\",\n    \"new_prompt\": \"Is the acronym 'SPARK' a recognized software development methodology; if it is, state its official expanded form and give a one-sentence description, and if it is not, explicitly say no standard expansion exists and propose two plausible interpretations with brief justifications (do not invent an official source if none exists).\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it asks the model to supply the expansion of a short acronym that may not correspond to any standardized development methodology. It therefore tests domain knowledge of software engineering terminology, the ability to detect ambiguity or the absence of a canonical answer, and safe-answer behavior (i.e., avoiding hallucination and asking for clarification or explicitly saying no standard meaning exists). Models that default to guessing will produce incorrect or fabricated expansions, so correct handling requires ambiguity detection and cautious reporting of alternatives or provenance.", "new_prompt": "Is the acronym 'SPARK' a recognized software development methodology; if it is, state its official expanded form and give a one-sentence description, and if it is not, explicitly say no standard expansion exists and propose two plausible interpretations with brief justifications (do not invent an official source if none exists).", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nQuestion: Is the following a factual statement?\n\"Pinocchio tells a tale that is an uplifting and happy story.\"\nOptions:\n- yes\n- no\n\nLet me think..... Pinocchio (1940 film) tells a tale that involves a major character dying. The answer is no.\n\nQuestion: Is the following statement true?\n\"It is important to practice sound Computer security in preventing malicious actors form entering critical systems.\"\n\nLet me think..... Computer security is good practice within large organizations. The answer is yes.\n\nQuestion: Is the following sentence factually correct?\n\"Treatment for manic depressants include Lithium.\"\nOptions:\n- yes\n- no\n\nLet me think..... Lithium increased serotonin in the brain relieving helping to relieve symptoms of depression and mania. The answer is yes.\n\nQuestion: \"William C. Durant founded the company General Motors.\"\nIs the above claim true?\nOptions:\n- yes\n- no<end_of_turn>\n<start_of_turn>model\n", "original_scores": [3.453125, 3.984375, 1.453125, 3.78125, 3.453125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it mixes short factual-verification tasks across multiple domains (film, security, medicine, corporate history) while also including informal \\\"Let me think...\\\" reasoning fragments that can mislead models into following flawed internal chains-of-thought. It tests: (1) precise factual knowledge across domains, (2) the ability to judge truth of concise claims (yes/no) rather than produce freeform answers, (3) resistance to spurious or misleading reasoning provided in the prompt, and (4) following strict output constraints (selecting only the allowed options and giving a brief justification). Models fail here when they are swayed by the embedded reasoning, hallucinate facts, or ignore the required terse response format.\",\n    \"new_prompt\": \"Claim: \\\"The Beatles formed in Liverpool in 1960 and originally included John Lennon, Paul McCartney, George Harrison, and Pete Best as members.\\\" Is the above claim true? Options: - yes - no Answer with exactly 'yes' or 'no' followed by one short sentence explaining your choice. Do not provide chain-of-thought or step-by-step internal reasoning.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it mixes short factual-verification tasks across multiple domains (film, security, medicine, corporate history) while also including informal \"Let me think...\" reasoning fragments that can mislead models into following flawed internal chains-of-thought. It tests: (1) precise factual knowledge across domains, (2) the ability to judge truth of concise claims (yes/no) rather than produce freeform answers, (3) resistance to spurious or misleading reasoning provided in the prompt, and (4) following strict output constraints (selecting only the allowed options and giving a brief justification). Models fail here when they are swayed by the embedded reasoning, hallucinate facts, or ignore the required terse response format.", "new_prompt": "Claim: \"The Beatles formed in Liverpool in 1960 and originally included John Lennon, Paul McCartney, George Harrison, and Pete Best as members.\" Is the above claim true? Options: - yes - no Answer with exactly 'yes' or 'no' followed by one short sentence explaining your choice. Do not provide chain-of-thought or step-by-step internal reasoning.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\ninstruction:\nYou will be given a trivia clue, and the category it belongs to. You should answer with the best answer that belongs in the category and is described by the clue. For simplicity, answers should be in all lower cased letters.\nquestion:\nCategory: ANCIENT ARTIFACTS \nClue: Some of its text says, \"The decree should be writen on a stela of hard stone, in sacred writing, document writing & Greek writing\"\nanswer:\nthe rosetta stone\n\n\nquestion:\nCategory: DEMOGRAPHICS \nClue: In 2005 the World Health Organization appropriately decided to stop hiring people who do this\nanswer:\nsmoke\n\n\nquestion:\nCategory: GEOGRAPHIC ADJECTIVES \nClue: Of the nations with adjectives in their common names, only this Western Hemisphere one bears the name of a religious order\nanswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [0.70703125, -3.703125, 0.70703125, 0.70703125, 0.70703125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines several nontrivial skills: (1) domain-specific trivia — knowing which countries have adjectives in their common English names and the historical/etymological origin of those adjectival elements (here, recognizing that “Dominican” refers to the Dominican religious order); (2) precise semantic parsing — correctly interpreting the phrasing “nations with adjectives in their common names” (i.e., countries whose common English names include an adjectival modifier like “Dominican” in “Dominican Republic”); (3) elimination and geographic reasoning — restricting candidates to the Western Hemisphere; and (4) strict output formatting — the response must be the single best answer in all lower cased letters with no extra text. Failures often come from missing the etymological link (thinking “Dominican” refers to the island or a demonym rather than the religious order), mis-parsing the category constraint, or not adhering to the lowercase/single-answer requirement.\",\n    \"new_prompt\": \"Instruction: You will receive a single trivia clue and its category. Reply with the single best answer that fits the clue, using only lower cased letters and no additional explanation. Category: COUNTRY NAME ETYMOLOGY Clue: Which sovereign state in the Western Hemisphere has the adjectival word in its common English name derived from the Dominican religious order founded by Saint Dominic?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines several nontrivial skills: (1) domain-specific trivia — knowing which countries have adjectives in their common English names and the historical/etymological origin of those adjectival elements (here, recognizing that “Dominican” refers to the Dominican religious order); (2) precise semantic parsing — correctly interpreting the phrasing “nations with adjectives in their common names” (i.e., countries whose common English names include an adjectival modifier like “Dominican” in “Dominican Republic”); (3) elimination and geographic reasoning — restricting candidates to the Western Hemisphere; and (4) strict output formatting — the response must be the single best answer in all lower cased letters with no extra text. Failures often come from missing the etymological link (thinking “Dominican” refers to the island or a demonym rather than the religious order), mis-parsing the category constraint, or not adhering to the lowercase/single-answer requirement.", "new_prompt": "Instruction: You will receive a single trivia clue and its category. Reply with the single best answer that fits the clue, using only lower cased letters and no additional explanation. Category: COUNTRY NAME ETYMOLOGY Clue: Which sovereign state in the Western Hemisphere has the adjectival word in its common English name derived from the Dominican religious order founded by Saint Dominic?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given the name of an Indian food dish. You need to classify the dish as a \"main course\", \"dessert\" or \"snack\".\nOne example: Sev tameta\nSolution is here: main course\nExplanation: Sev tameta is a main course dish.\n\nNow, solve this: Bora Sawul\nSolution:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.8203125, 4.375, 2.203125, 3.484375, 0.53515625], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires specific cultural and culinary knowledge about Indian dishes (including regional and transliterated names), plus the ability to map that knowledge into a coarse taxonomy (main course / dessert / snack). Ambiguities arise from dishes that serve multiple roles (e.g., eaten as breakfast, snack, or main), regional variations, and rare or transliterated names that may be out-of-vocabulary, forcing the model to infer from morphemes or typical ingredients. The task therefore tests domain knowledge, disambiguation under uncertainty, handling of uncommon/ambiguous inputs, and concise justification.\",\n    \"new_prompt\": \"You are given the name of an Indian dish. Decide whether it is primarily a 'main course', a 'dessert', or a 'snack'. If the dish is unfamiliar, infer the most likely category from the name, common ingredients, and usual serving context. Output only the single-word category followed by a one-sentence justification. Now classify: Chakuli Pitha\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires specific cultural and culinary knowledge about Indian dishes (including regional and transliterated names), plus the ability to map that knowledge into a coarse taxonomy (main course / dessert / snack). Ambiguities arise from dishes that serve multiple roles (e.g., eaten as breakfast, snack, or main), regional variations, and rare or transliterated names that may be out-of-vocabulary, forcing the model to infer from morphemes or typical ingredients. The task therefore tests domain knowledge, disambiguation under uncertainty, handling of uncommon/ambiguous inputs, and concise justification.", "new_prompt": "You are given the name of an Indian dish. Decide whether it is primarily a 'main course', a 'dessert', or a 'snack'. If the dish is unfamiliar, infer the most likely category from the name, common ingredients, and usual serving context. Output only the single-word category followed by a one-sentence justification. Now classify: Chakuli Pitha", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nThe total cost of a certain product is $22, and it costs $3 each times 2, 3 and 5. How much is each price?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.75, -12.5, 4.0625, 3.53125, 1.796875], "model_raw_response": "{\n    \"analysis\": \"The original prompt is challenging because it's ambiguously worded and requires the model to (1) correctly parse informal/unclear language into a mathematical setup, (2) translate that setup into algebra (forming equations or a ratio-based relation), and (3) solve the resulting linear equation(s) — possibly producing fractional answers. Models often fail either by misinterpreting the phrase \\\"$3 each times 2, 3 and 5,\\\" by not recognizing a ratios interpretation, or by not identifying contradictions/inconsistencies and handling non-integer solutions.\",\n    \"new_prompt\": \"Three types of items are priced in the ratio 2:3:5. If buying 2 items of the first type, 3 of the second, and 5 of the third costs $22 altogether, what is the price of one item of each type?\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is challenging because it's ambiguously worded and requires the model to (1) correctly parse informal/unclear language into a mathematical setup, (2) translate that setup into algebra (forming equations or a ratio-based relation), and (3) solve the resulting linear equation(s) — possibly producing fractional answers. Models often fail either by misinterpreting the phrase \"$3 each times 2, 3 and 5,\" by not recognizing a ratios interpretation, or by not identifying contradictions/inconsistencies and handling non-integer solutions.", "new_prompt": "Three types of items are priced in the ratio 2:3:5. If buying 2 items of the first type, 3 of the second, and 5 of the third costs $22 altogether, what is the price of one item of each type?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nProvide specific examples of the type of art you utilize and how it contributes to your overall well-being in your self-care routine. Do you engage in painting, drawing, sculpting, writing, or other forms of artistic expression? Does the style or theme of the art you create or enjoy impact your mood or emotions in a particular way? How frequently do you incorporate art into your self-care routine, and how does it fit into your daily or weekly schedule?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-2.78125, -1.0234375, -2.640625, -3.484375, -2.234375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it demands a coherent first‑person account that combines concrete, specific examples (which art form, medium, style), introspective reasoning about emotional effects, and practical scheduling details — all in one response. It tests creative writing, perspective-taking/self-reflection, the ability to synthesize multi‑part information without leaving gaps, and temporal organization (how often and where art fits into a routine). Models often fail by giving generic lists, skipping scheduling specifics, or treating each subquestion in isolation rather than producing a unified, realistic narrative.\",\n    \"new_prompt\": \"Write a single, first-person diary entry about a time you used one specific form of art as self-care, describing the activity, how it changed your mood that day, and how often and when you typically include it in your weekly routine?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it demands a coherent first‑person account that combines concrete, specific examples (which art form, medium, style), introspective reasoning about emotional effects, and practical scheduling details — all in one response. It tests creative writing, perspective-taking/self-reflection, the ability to synthesize multi‑part information without leaving gaps, and temporal organization (how often and where art fits into a routine). Models often fail by giving generic lists, skipping scheduling specifics, or treating each subquestion in isolation rather than producing a unified, realistic narrative.", "new_prompt": "Write a single, first-person diary entry about a time you used one specific form of art as self-care, describing the activity, how it changed your mood that day, and how often and when you typically include it in your weekly routine?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nTranslate the given Spanish text to English\n\nQ: The annular member includes a fluid port aimed at the equipment access cover.\n\nA: Este elemento anular incluye una puerta de fluido dirigida a la cubierta de acceso al equipo.\n****\nQ: A static portion is substantially immobilized within the housing and includes a magnetic flux generator for generating magnetic flux, a pair of pole pieces for coupling magnetic flux through at least one of a first magnetic circuit and a second magnetic circuit, and a coil disposed to electromagnetically interact with one of the magnetic circuits.\n\nA: Una porción estática está sustancialmente inmovilizada dentro de la carcasa e incluye un generador de flujo magnético para la generación de flujo magnético, un par de piezas polares para el acoplamiento de flujo magnético a través de al menos uno de un primer circuito magnético y un segundo circuito magnético, y una bobina dispuesta para interactuar electromagnéticamente con uno de los circuitos magnéticos.\n****\nQ: The apparatus is operable so that a portion of the article is mechanically bonded as the article passes through the nip and a portion of the article separate from the bonded portion is cut as the article passes through the nip.\n\nA:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.53125, -3.328125, 0.5390625, -1.03125, 0.5390625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires translating a long, patent-style engineering sentence with nested clauses and parallel actions while preserving precise scope and coreference (which portion is bonded vs. which is cut, and that both actions occur “as the article passes through the nip”). It tests: (1) syntactic disambiguation and clause attachment, (2) accurate rendering of technical vocabulary (e.g., “nip”/“compression zone”), (3) maintaining legal/technical precision and register, and (4) fluency so the target language keeps the same meaning without introducing ambiguity.\",\n    \"new_prompt\": \"Translate the following English sentence into Spanish: The machine is arranged so that a section of the sheet is permanently joined while the sheet moves through the compression zone and a different section, separate from the joined section, is severed as the sheet moves through the compression zone.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires translating a long, patent-style engineering sentence with nested clauses and parallel actions while preserving precise scope and coreference (which portion is bonded vs. which is cut, and that both actions occur “as the article passes through the nip”). It tests: (1) syntactic disambiguation and clause attachment, (2) accurate rendering of technical vocabulary (e.g., “nip”/“compression zone”), (3) maintaining legal/technical precision and register, and (4) fluency so the target language keeps the same meaning without introducing ambiguity.", "new_prompt": "Translate the following English sentence into Spanish: The machine is arranged so that a section of the sheet is permanently joined while the sheet moves through the compression zone and a different section, separate from the joined section, is severed as the sheet moves through the compression zone.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task you will be given a list of integers. You should find the maximum absolute difference between 2 integers in the list. The absolute difference is the absolute value of one integer subtracted by another. The output should be a single integer which is the largest possible absolute distance.\n\n[Q]: [93, 26, 54, 39, 98, 80]\n[A]: 72\n\n\n[Q]: [-66, -70, 80, 9, -19]\n[A]: 150\n\n\n[Q]: [28, 48, -10, -65, 13, 99, 44, -92, -93, -94]\n[A]:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-2.46875, -2.46875, -2.46875, -3.0, -3.953125], "model_raw_response": "{\n    \"analysis\": \"The task tests simple but precise numeric reasoning: the model must parse a list of integers (including negatives), recognize that the maximum absolute difference between any two elements equals max(list) - min(list), and output exactly one integer. Common failures come from computing pairwise differences incorrectly (e.g., forgetting absolute value or checking the wrong pairs), mishandling negative numbers/signs, or producing extraneous text instead of the single integer result.\",\n    \"new_prompt\": \"Given the list of integers below, determine the largest possible absolute difference between any two elements. Output only the final integer (no explanation, punctuation, or extra characters).\\n\\nList: [47, -88, 14, -3, 100, -101, 0]\"\n}", "api_error": null, "parse_status": "success", "analysis": "The task tests simple but precise numeric reasoning: the model must parse a list of integers (including negatives), recognize that the maximum absolute difference between any two elements equals max(list) - min(list), and output exactly one integer. Common failures come from computing pairwise differences incorrectly (e.g., forgetting absolute value or checking the wrong pairs), mishandling negative numbers/signs, or producing extraneous text instead of the single integer result.", "new_prompt": "Given the list of integers below, determine the largest possible absolute difference between any two elements. Output only the final integer (no explanation, punctuation, or extra characters).\n\nList: [47, -88, 14, -3, 100, -101, 0]", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn a certain country, the numbers 1 through 9 are arranged in a 3x3 grid. The sum of each row, column, and diagonal is the same value. However, one of the numbers in the grid is missing. Can you determine which number is missing?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [6.5625, 4.28125, 5.96875, -2.015625, 1.875], "model_raw_response": "{\n    \"analysis\": \"The original prompt is underspecified: it states that the numbers 1–9 form a 3×3 square with equal row/column/diagonal sums and that one number is missing, but it does not show the grid or which entry is missing. This makes the task ambiguous or impossible to solve uniquely. Correctly answering the intended puzzle requires specific domain knowledge (properties of a 3×3 magic square: total sum 45, magic constant 15, center = 5, opposite pairs sum to 10) and constraint-solving / deductive reasoning to use the given entries to infer the missing value. Failures likely come from not recognizing the missing information or from lacking the relevant magic-square properties and constraint propagation skills.\",\n    \"new_prompt\": \"A 3×3 square contains each of the integers 1 through 9 exactly once, arranged so that the sum of the numbers in every row, every column, and both main diagonals is the same. One cell in the square is blank. Here is the square (use _ to indicate the blank):\\n\\n2 7 6\\n9 5 1\\n_ 3 8\\n\\nWhat single integer (from 1 to 9) belongs in the blank to satisfy the condition?\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is underspecified: it states that the numbers 1–9 form a 3×3 square with equal row/column/diagonal sums and that one number is missing, but it does not show the grid or which entry is missing. This makes the task ambiguous or impossible to solve uniquely. Correctly answering the intended puzzle requires specific domain knowledge (properties of a 3×3 magic square: total sum 45, magic constant 15, center = 5, opposite pairs sum to 10) and constraint-solving / deductive reasoning to use the given entries to infer the missing value. Failures likely come from not recognizing the missing information or from lacking the relevant magic-square properties and constraint propagation skills.", "new_prompt": "A 3×3 square contains each of the integers 1 through 9 exactly once, arranged so that the sum of the numbers in every row, every column, and both main diagonals is the same. One cell in the square is blank. Here is the square (use _ to indicate the blank):\n\n2 7 6\n9 5 1\n_ 3 8\n\nWhat single integer (from 1 to 9) belongs in the blank to satisfy the condition?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nExamine the provided sentences and discern the use of figurative language in each of them. Subsequently, expand your analysis by evaluating the depth and complexity of the metaphors employed in each sentence. Your objective is to ascertain the sentence that displays the most imaginative metaphor and assign a value of 1 to it, while the remaining two sentences should be assigned a value of 0. This task necessitates multi-layered cognitive processing as you analyze and assess the metaphors, rendering it more intricate than the initial version.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [4.5625, -8.875, 3.796875, 1.0, 3.625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires: (1) fine-grained figurative language detection (spotting metaphors vs. other devices), (2) evaluative comparison of metaphorical depth and creativity (judging layering, novelty, and conceptual richness), (3) multi-step reasoning to weigh abstract qualities and resolve ties, and (4) following a strict numeric labeling constraint (assign exactly one sentence a 1 and the others 0). The task blends objective linguistic identification with subjective, nuanced judgment, so models must balance semantic analysis, creativity assessment, and the precise output format.\",\n    \"new_prompt\": \"Read the three sentences below. Decide which single sentence contains the most imaginative (deepest/multi-layered) metaphor and then label the sentences in order by giving a three-item binary sequence where '1' marks the sentence you judge most imaginative and '0' marks the others (exactly one '1' must appear). If two seem equally rich, choose the one with more conceptual layering. Sentences: 1) \\\"Her memory was an empty auditorium where old laughter took its final bow.\\\" 2) \\\"The factory breathed steam like a tired dragon waking each morning.\\\" 3) \\\"Dawn stitched light across the rooftops with a careful, patient needle.\\\" Provide only the three binary labels (e.g., 100) followed by a one-sentence justification naming which sentence you chose and why.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires: (1) fine-grained figurative language detection (spotting metaphors vs. other devices), (2) evaluative comparison of metaphorical depth and creativity (judging layering, novelty, and conceptual richness), (3) multi-step reasoning to weigh abstract qualities and resolve ties, and (4) following a strict numeric labeling constraint (assign exactly one sentence a 1 and the others 0). The task blends objective linguistic identification with subjective, nuanced judgment, so models must balance semantic analysis, creativity assessment, and the precise output format.", "new_prompt": "Read the three sentences below. Decide which single sentence contains the most imaginative (deepest/multi-layered) metaphor and then label the sentences in order by giving a three-item binary sequence where '1' marks the sentence you judge most imaginative and '0' marks the others (exactly one '1' must appear). If two seem equally rich, choose the one with more conceptual layering. Sentences: 1) \"Her memory was an empty auditorium where old laughter took its final bow.\" 2) \"The factory breathed steam like a tired dragon waking each morning.\" 3) \"Dawn stitched light across the rooftops with a careful, patient needle.\" Provide only the three binary labels (e.g., 100) followed by a one-sentence justification naming which sentence you chose and why.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nWhat is the largest bird in North America?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [0.94140625, -3.9375, -4.21875, -5.84375, -4.21875], "model_raw_response": "{\n    \"analysis\": \"The prompt looks simple but is challenging because it packs several hidden ambiguities and requires precise domain knowledge. \\\"Largest\\\" is underspecified (wingspan, weight, height, or length?), and a correct answer depends on which metric you choose. It also requires knowing geographic scope (what counts as \\\"North America\\\" and whether vagrants, introduced, or extinct species count) and up-to-date species size data. So this tests: factual biological knowledge (species ID and size data), the ability to disambiguate or apply constraints for superlatives, and careful interpretation of geographical/taxonomic scope. Models often fail by picking the wrong metric, including the wrong populations (vagrants or extinct species), or giving imprecise size figures.\",\n    \"new_prompt\": \"Restricting to living bird species native to Africa (exclude introduced, vagrant, and extinct taxa), which species has the greatest average adult body mass? Give the common name, scientific name, and the typical maximum adult mass in kilograms.\"\n}", "api_error": null, "parse_status": "success", "analysis": "The prompt looks simple but is challenging because it packs several hidden ambiguities and requires precise domain knowledge. \"Largest\" is underspecified (wingspan, weight, height, or length?), and a correct answer depends on which metric you choose. It also requires knowing geographic scope (what counts as \"North America\" and whether vagrants, introduced, or extinct species count) and up-to-date species size data. So this tests: factual biological knowledge (species ID and size data), the ability to disambiguate or apply constraints for superlatives, and careful interpretation of geographical/taxonomic scope. Models often fail by picking the wrong metric, including the wrong populations (vagrants or extinct species), or giving imprecise size figures.", "new_prompt": "Restricting to living bird species native to Africa (exclude introduced, vagrant, and extinct taxa), which species has the greatest average adult body mass? Give the common name, scientific name, and the typical maximum adult mass in kilograms.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given a summary for US Congressional and California state bill, your task is to generate a Title for this bill. The preferred titles are under forty words and mention the purpose of the bill.\n\nEx Input:\nCheyenne River Sioux Tribe Equitable Compensation Amendments Act of 2006 - Amends the Cheyenne River Sioux Tribe Equitable Compensation Act to make member landowners eligible for the additional financial compensation provided to the Cheyenne River Sioux Tribe for the acquisition by the federal government of 104,492 acres of land of the Tribe and member landowners for the Oahe Dam and Reservoir project. Defines member landowner as a member of the Tribe (or an heir of such a member) that owned land on the Cheyenne River Sioux Reservation that was acquired by the United States for the Oahe Dam and Reservoir Project of the Pick-Sloan Missouri River Basin program.\n\nDirects the Secretary of the Treasury to make five annual deposits into the Cheyenne River Sioux Tribal Recovery Trust Fund of: (1) $58,144,591.60; and (2) an additional amount equal to the interest that would have accrued under certain circumstances.\n\nDirects the Secretary to invest the Fund only in interest-bearing obligations of the United States according to specified requirements, including separate investments of principal and interest from two separate accounts within the Tribal Recovery Trust Fund.\n\nAuthorizes the plan prepared for the use of payments to the Tribe to provide for payment of additional compensation to member landowners; except that payments of additional compensation shall not be deposited or transferred into any member landowner's Individual Indian Money account, and shall not exceed an amount equal to 44.3% of the amount transferred by the Secretary of the Interior to the Tribe.\n\nRequires the Secretary of the Interior to assist the Tribe in claims processing by providing any record requested to identify the heirs of member landowners within 90 days after receiving a request.\n\nDeclares that, upon deposit of the final payment into the Fund, all claims that the Tribe has or may have against the United States for the taking of tribal land or property for the Project shall be extinguished.\n\nEx Output:\nA bill to amend the Cheyenne River Sioux Tribe Equitable Compensation Act to provide compensation to members of the Cheyenne River Sioux Tribe for damage resulting from the Oahe Dam and Reservoir Project, and for other purposes.\n\n\nEx Input:\nTech Bond Initiative of 2001 - Amends the Internal Revenue Code to create a limited credit for the holder of a \"qualified technology bond\" (as defined).\n\nEx Output:\nTo amend the Internal Revenue Code of 1986 to provide an income tax credit to holders of bonds financing new communications technologies, and for other purposes.\n\n\nEx Input:\nProvides for the transfer of the United States Postal Service to a private corporation. \nDirects the President to transmit to the Congress:  (1) a comprehensive plan providing for the transfer of property subject to this Act; and (2) recommendations for legislation as necessary. \nEstablishes the Postal Privatization Commission to:  (1) carry out functions relating to the issuance of securities to postal employees; and (2) consult with the President on the transfer.\n\nEx Output:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [3.171875, -6.75, -7.21875, -5.84375, -3.3125], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires multiple distinct capabilities at once: precise comprehension of dense legal or policy text, abstractive summarization to extract the bill's central purpose and primary actors/beneficiaries, and strict adherence to format constraints (produce a legislative-style title under 40 words that explicitly mentions purpose). It also tests avoidance of hallucination (no new facts), sensible compression of multiple provisions into a single phrase, and maintaining an appropriate formal register. Models often fail by producing titles that are too long or vague, omit the bill's purpose or key beneficiaries, misstate specifics, or add extraneous legalese (e.g., unnecessary citations or “and for other purposes”), so the prompt must emphasize concision, factual fidelity, and legislative tone.\",\n    \"new_prompt\": \"Given the following legislative bill summary, produce a single, concise legislative-style title (no more than 40 words) that clearly states the bill's primary purpose and the main action or beneficiaries. Do not introduce any facts not present in the summary and do not include citations or extra commentary—output only the title.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires multiple distinct capabilities at once: precise comprehension of dense legal or policy text, abstractive summarization to extract the bill's central purpose and primary actors/beneficiaries, and strict adherence to format constraints (produce a legislative-style title under 40 words that explicitly mentions purpose). It also tests avoidance of hallucination (no new facts), sensible compression of multiple provisions into a single phrase, and maintaining an appropriate formal register. Models often fail by producing titles that are too long or vague, omit the bill's purpose or key beneficiaries, misstate specifics, or add extraneous legalese (e.g., unnecessary citations or “and for other purposes”), so the prompt must emphasize concision, factual fidelity, and legislative tone.", "new_prompt": "Given the following legislative bill summary, produce a single, concise legislative-style title (no more than 40 words) that clearly states the bill's primary purpose and the main action or beneficiaries. Do not introduce any facts not present in the summary and do not include citations or extra commentary—output only the title.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given a sentence in the English language and your task is to convert it into the Hindi language. In the translation, keep numbers as it is and make it sentence case (capitalize only the first word of each sentence and noun).\nOne example: पहले दो को अविश्वसनीय मानकर बाकी पांच मुखबिरों के आधार पर मुकदमा चलाया गया|\nSolution is here: The first two were found unreliable and the prosecution case rested mainly on the evidence of the remaining five approvers .\nExplanation: Hindi sentence is properly converted into English sentence because the input also says that the first two were judged to be untrustworthy, thus the prosecution relied heavily on the testimony of the remaining five approvers. Also, the translated sentence is in a sentence case.\n\nNow, solve this: कुछ मामलों को लेकर जब लोगों ने अदालत का सहारा लिया , तब वह खुद मुसीबत में पड़ गये और उन पर जुर्माने हुए ।\nSolution:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.71875, -0.71875, -1.3125, -0.71875, -1.265625], "model_raw_response": "{\n  \"analysis\": \"The original prompt is challenging because it mixes tasks and contains conflicting/unclear instructions (it says “convert into Hindi” but the example translates Hindi to English), while also imposing strict formatting constraints. Solving it requires: accurate bilingual translation (idiomatic rendering, clause reordering), resolving pronoun and reference ambiguity, preserving numerals exactly, enforcing sentence-case formatting (only the first word and proper nouns capitalized) and punctuation/spacing normalization, and obeying negative/strict output constraints (only output the Solution in the required form). These combined requirements test linguistic understanding, pragmatic disambiguation, and precise constraint-following.\",\n  \"new_prompt\": \"Translate the following Hindi sentence into English. Keep any numerals exactly as they appear. Output the translation in sentence case: capitalize only the first word of the sentence and any proper nouns; all other words must be lowercase. Ensure punctuation is correctly attached to the preceding word (no space before commas or periods). Provide only the single translated sentence as the Solution.\\n\\nSentence: कुछ मामलों को लेकर जब लोगों ने अदालत का सहारा लिया , तब वे खुद मुश्किल में पड़ गए और उन पर जुर्माना लगाया गया ।\\n\\nSolution:\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is challenging because it mixes tasks and contains conflicting/unclear instructions (it says “convert into Hindi” but the example translates Hindi to English), while also imposing strict formatting constraints. Solving it requires: accurate bilingual translation (idiomatic rendering, clause reordering), resolving pronoun and reference ambiguity, preserving numerals exactly, enforcing sentence-case formatting (only the first word and proper nouns capitalized) and punctuation/spacing normalization, and obeying negative/strict output constraints (only output the Solution in the required form). These combined requirements test linguistic understanding, pragmatic disambiguation, and precise constraint-following.", "new_prompt": "Translate the following Hindi sentence into English. Keep any numerals exactly as they appear. Output the translation in sentence case: capitalize only the first word of the sentence and any proper nouns; all other words must be lowercase. Ensure punctuation is correctly attached to the preceding word (no space before commas or periods). Provide only the single translated sentence as the Solution.\n\nSentence: कुछ मामलों को लेकर जब लोगों ने अदालत का सहारा लिया , तब वे खुद मुश्किल में पड़ गए और उन पर जुर्माना लगाया गया ।\n\nSolution:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIdentify the theme in the following passage about a family reunion, considering the intergenerational dynamics, cultural traditions, and emotional experiences of the Smith family members. Additionally, provide a detailed analysis of the different roles and responsibilities each family member had during the planning and execution of the reunion, including any financial, logistical, or emotional contributions. Use tables, diagrams, and charts to illustrate the family tree and genealogy of the Smith family, as well as any cultural or religious practices that were incorporated into the reunion. Finally, explore the potential long-term effects of the reunion on the family dynamics and relationships, taking into account any unresolved conflicts or tensions that may have arisen during the event.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-8.0, -4.15625, -9.3125, -8.1875, -3.96875], "model_raw_response": "{\n  \"analysis\": \"This prompt is challenging because it bundles multiple demanding tasks into one open-ended request while depending on ambiguous input: it asks for thematic interpretation, fine-grained attribution of roles and contributions (financial, logistical, emotional), inference about intergenerational dynamics and cultural practices, creation of structured visualizations (tables, diagrams, charts), and causal/long-term reasoning about relationships — all from an implied “passage” that was not provided. Skills tested include deep reading comprehension, inference and theory-of-mind (emotional and interpersonal reasoning), cultural sensitivity, synthesis of qualitative details into structured formats, multi-step causal reasoning about future effects, and the ability to follow mixed output-format instructions (text + tables/diagrams/charts). Practical failure modes include: missing or ambiguous source text (forcing hallucination), inability to decide appropriate granularity for roles/contributions, difficulty producing the requested visual formats, and balancing neutral/ethical language when discussing conflicts and family dynamics.\",\n  \"new_prompt\": \"Read the short passage below about a family reunion and then, in a single structured response, identify the central theme, analyze intergenerational dynamics and cultural traditions, enumerate each named family member’s specific roles and contributions during planning and execution (noting financial, logistical, and emotional inputs), produce a compact family tree (table or simple ASCII diagram) and a small chart/table summarizing the cultural or religious practices incorporated into the reunion, and finally assess likely long-term effects on family relationships (including any unresolved tensions mentioned). Passage: \\\"The Rivera family gathered for a weekend reunion at Abuela Rosa’s house after three years apart. Rosa, the 78-year-old matriarch, prepared the ancestral altar and taught her grandchildren a blessing sung in Spanish; she contributed her savings to rent extra chairs. Rosa’s daughter, Elena, coordinated lodging and transportation, borrowed a van, and covered half the catering cost; her son Mateo brought a projector for a slideshow and mediated a heated exchange between cousins. Cousin Ana handled the potluck sign-ups and culturally specific dishes, while cousin Javier, recently estranged over a business dispute, arrived late and mostly stayed apart; his husband Luis chauffeured elderly relatives and helped set up tables. Teenager Sofia felt torn between traditional ceremonies and her college schedule and sometimes withdrew from conversations. The reunion included a Sunday morning remembrance ritual, shared recipes, bilingual prayers, and a community storytelling circle that highlighted both pride and old grievances.\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it bundles multiple demanding tasks into one open-ended request while depending on ambiguous input: it asks for thematic interpretation, fine-grained attribution of roles and contributions (financial, logistical, emotional), inference about intergenerational dynamics and cultural practices, creation of structured visualizations (tables, diagrams, charts), and causal/long-term reasoning about relationships — all from an implied “passage” that was not provided. Skills tested include deep reading comprehension, inference and theory-of-mind (emotional and interpersonal reasoning), cultural sensitivity, synthesis of qualitative details into structured formats, multi-step causal reasoning about future effects, and the ability to follow mixed output-format instructions (text + tables/diagrams/charts). Practical failure modes include: missing or ambiguous source text (forcing hallucination), inability to decide appropriate granularity for roles/contributions, difficulty producing the requested visual formats, and balancing neutral/ethical language when discussing conflicts and family dynamics.", "new_prompt": "Read the short passage below about a family reunion and then, in a single structured response, identify the central theme, analyze intergenerational dynamics and cultural traditions, enumerate each named family member’s specific roles and contributions during planning and execution (noting financial, logistical, and emotional inputs), produce a compact family tree (table or simple ASCII diagram) and a small chart/table summarizing the cultural or religious practices incorporated into the reunion, and finally assess likely long-term effects on family relationships (including any unresolved tensions mentioned). Passage: \"The Rivera family gathered for a weekend reunion at Abuela Rosa’s house after three years apart. Rosa, the 78-year-old matriarch, prepared the ancestral altar and taught her grandchildren a blessing sung in Spanish; she contributed her savings to rent extra chairs. Rosa’s daughter, Elena, coordinated lodging and transportation, borrowed a van, and covered half the catering cost; her son Mateo brought a projector for a slideshow and mediated a heated exchange between cousins. Cousin Ana handled the potluck sign-ups and culturally specific dishes, while cousin Javier, recently estranged over a business dispute, arrived late and mostly stayed apart; his husband Luis chauffeured elderly relatives and helped set up tables. Teenager Sofia felt torn between traditional ceremonies and her college schedule and sometimes withdrew from conversations. The reunion included a Sunday morning remembrance ritual, shared recipes, bilingual prayers, and a community storytelling circle that highlighted both pride and old grievances.\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nYou are given a Catalan sentence with a blank, and you need to predict the missing word.\n--------\nQuestion: Carod-Rovira ha comparat la situació de Catalunya i Espanya amb ____ d' Alemanya respecte d' Europa .\n\nAnswer: la\n\n\nQuestion: El 25 % dels futurs usuaris del Tren d' Alta Velocitat ( TAV ) de la regió metropolitana de Barcelona utilitzarà l' estació del Vallès per agafar el tren , segons un estudi que ha fet l' ____ de Terrassa .\n\nAnswer: Ajuntament\n\n\nQuestion: L' últim any que va funcionar abans ____ aiguats , el 1999 , va transportar 128.000 viatgers .\n\nAnswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.5625, -5.0625, -5.0625, -4.59375, -5.0625], "model_raw_response": "{\n    \"analysis\": \"This task is hard because it requires fine-grained Catalan linguistic knowledge rather than broad world knowledge: selecting the correct small function word (preposition, article, contraction, or elided form) that fits morphosyntactically and idiomatically into the sentence. It tests (1) Catalan morphology and syntax (agreement, number/gender), (2) correct use of contractions/elisions (e.g., de + els = dels, a + el = al, de l'), (3) sensitivity to local context and collocations, and (4) precise orthography (apostrophes and spacing). Models often fail because they favor high-frequency content words, confuse Catalan with related languages (Spanish/French), or mishandle tokenization/spacing around apostrophes and contractions, producing a semantically plausible but grammatically incorrect filler.\",\n    \"new_prompt\": \"You are given a Catalan sentence with a single-word blank. Predict the one Catalan word that correctly fills the blank (pay attention to contractions and elisions).\\n\\nQuestion: La fàbrica va tancar temporalment abans ____ inspeccions obligatòries i els treballadors van ser reassignats.\\n\\nAnswer:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is hard because it requires fine-grained Catalan linguistic knowledge rather than broad world knowledge: selecting the correct small function word (preposition, article, contraction, or elided form) that fits morphosyntactically and idiomatically into the sentence. It tests (1) Catalan morphology and syntax (agreement, number/gender), (2) correct use of contractions/elisions (e.g., de + els = dels, a + el = al, de l'), (3) sensitivity to local context and collocations, and (4) precise orthography (apostrophes and spacing). Models often fail because they favor high-frequency content words, confuse Catalan with related languages (Spanish/French), or mishandle tokenization/spacing around apostrophes and contractions, producing a semantically plausible but grammatically incorrect filler.", "new_prompt": "You are given a Catalan sentence with a single-word blank. Predict the one Catalan word that correctly fills the blank (pay attention to contractions and elisions).\n\nQuestion: La fàbrica va tancar temporalment abans ____ inspeccions obligatòries i els treballadors van ser reassignats.\n\nAnswer:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you're given an article and an answer. Your task is to generate the question for the answer based on the given article.\n\n[EX Q]: Article: The first tape recorder didn't use tape. It used long thin wire. It was invented in 1900 by Valdermar Poulsen. In 1930, German scientists invented the tape we use today. Back then the tape was on big rolls. In 1964 the Philips company in Holland invented the cassette. It's pretty much a holder for the tape. People use cassettes all over the world. If you don't have a cassette recorder, borrow one. Think of a book your parents read out loud to you. That might be a great book to read out loud to your mom or dad in their car. Put a cassette in the recorder, open the book, hit the record button and start reading out loud. Remember there is no such a thing as a wrong way to do this. You might think you've made a mistake, but this gift is part of you, and nothing about that can be a mistake. It's impossible. You get to be all artistic and creative here. You might want to play music in the background. Do whatever you want. The gift is you, so you decide. Remember to say \"I love you\"at the end of your reading. That's like the prize at the end of the book. \n Answer: To practice reading out loud.\n[EX A]: What does the author advise us to do?\n\n[EX Q]: Article: Oprah Winfrey is not just a very successful TV personality  in the US, she is also a woman who has encouraged millions. For those people, her life and her success are a good example. She has struggled with many of the challenges that we all face, and she has changed her life. Her message is powerful: I did it, and so can you. Oprah Winfrey is a black woman whose becoming famous is an encouraging story. She was born on January 29, 1954 in a small village in America. Her patents were very poor. For her family, life seemed to hold no promise. But there was nothing that could stop Oprah. She was an extremely bright girl at school. She asked her kindergarten teacher to let her go to school sooner and she also skipped  the second grade of primary school . Her life from the age of six till about fourteen was very hard. For many women such difficulty would be too heavy to bear . But not for Oprah. When she was fourteen, Oprah went to live with her father. Her father showed her how hard work and discipline  could lead to self-improvement. Oprah listened to her father, and a few years later she won a college scholarship that allowed her to go to university. Two years later, after graduation, she started working for television. For more than ten years she worked for different TV stations across the country. In 1984, she moved to Chicago, where she became the host of a talk show called \"AM Chicago\". When Oprah started, \"AM Chicago\" had few listeners. By September of the next year, the show was so successful that it was given a new name: 'The Oprah Winfrey Show\". Oprah Winfrey's story encourages many people to believe that success and happiness in life are within reach for everyone. \n Answer: I managed to over come  the challenges.\n[EX A]: What does Oprah Winfrey mean by saying \"I did it\"?\n\n[EX Q]: Article: Hurricane  Babette reached the northeastern United States today and caused death and destruction everywhere. The killer storm carried winds of 135 miles per hour. The high winds and heavy rains, together with the force of the Atlantic Ocean, created extremely dangerous waves. It is not yet known how many people died or were hurt in this storm, but it is said that the numbers will be very high. The damage to homes, business and crops will run into billions of dollars. Now that the storm is over, the long, slow rebuilding will begin. With the arrival of the tornado season, the National Weather Service is again telling people how to protect themselves from these deadly storms. The winds from tornadoes are the most violent winds on earth. They can blow up to 400 miles per hour. A tornado looks like a funnel . It is also very loud. It may sound like a train coming at you. In fact, the winds from a tornado can pick up a train and throw it around. If a tornado is seen in your area, it is very important that you protect yourself. A basement   is the safest place to go. Try to wait under a table in the basement. If your building does not have a basement, stay on the ground floor but lie flat under a bed or table. Stay away from windows. If you are outside or in your car, try to find a narrow place to lie down in. As the area tries to deal with the worst flood of the century, there are many heartwarming stories of people helping other people. From all over the world people have sent food and clothes to help the thousands who have had to leave their homes. Many volunteers have come to help to make sandbags and use them to build walls against the overflowing river. While the result of this disaster will be terrible for many, it is beautiful to see people coming together to help others and save lives, and property  . \n Answer: Newspaper articles about the weather.\n[EX A]:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-11.375, -9.625, -9.625, -12.375, -7.53125], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires deep reading comprehension, pragmatic inference, and precise generation. The model must (1) understand the article content and identify which fact or idea the provided short answer corresponds to, (2) infer the appropriate question type and level of specificity (who/what/when/why/how/where or yes/no) so the answer fits unambiguously, and (3) produce a single, well-formed interrogative sentence that is fully supported by the passage without adding or contradicting information. Difficulties include handling ambiguous or very short answers, avoiding overly broad or overly specific questions, and resisting the temptation to copy the answer word-for-word into the question or to inject outside knowledge. The prompt therefore tests comprehension, constrained generation, and pragmatic alignment between question and answer.\",\n    \"new_prompt\": \"Read the short passage and the given answer; then write one concise, grammatically correct interrogative sentence (a single question) that a reader could ask about the passage such that the provided answer directly and unambiguously answers it; do not introduce any facts not in the passage—Passage: <insert passage here> Answer: <insert answer here>?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires deep reading comprehension, pragmatic inference, and precise generation. The model must (1) understand the article content and identify which fact or idea the provided short answer corresponds to, (2) infer the appropriate question type and level of specificity (who/what/when/why/how/where or yes/no) so the answer fits unambiguously, and (3) produce a single, well-formed interrogative sentence that is fully supported by the passage without adding or contradicting information. Difficulties include handling ambiguous or very short answers, avoiding overly broad or overly specific questions, and resisting the temptation to copy the answer word-for-word into the question or to inject outside knowledge. The prompt therefore tests comprehension, constrained generation, and pragmatic alignment between question and answer.", "new_prompt": "Read the short passage and the given answer; then write one concise, grammatically correct interrogative sentence (a single question) that a reader could ask about the passage such that the provided answer directly and unambiguously answers it; do not introduce any facts not in the passage—Passage: <insert passage here> Answer: <insert answer here>?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nA text is given in English. Translate it from the English language to the Urdu language. The translation must not omit or add information to the original sentence.\n\nEx Input:\nPower and Telecommunication facilities begin to improve in Cyclone affected Puri and Bhubaneswar Odisha Government informed that situation in 8 out of total 9 affected districts has improved and Power and Telecom services are gradually picking up in the worst affected areas of Puri and Bhubaneswar. About 3500 gang men are engaged in restoration of power transmission lines, sub-stations etc.\n\nEx Output:\nنئی دلّی ، 09 مئی کابینہ سیکریٹری جناب پی ۔ کے ۔ سنہا کی سر براہی میں گزشتہ روز نیشنل کرائسس مینجمنٹ کمیٹی ( این سی ایم سی ) نے اوڈیشہ کے علاقوں میں فانی طوفان سے متاثرہ علاقوں میں بچا\n\n\nEx Input:\nI strongly believe that the media can play a vital role in creating awareness about the legal provisions on such matters and empowering citizens by making them aware of their inalienable rights.\n\nEx Output:\nمجھے پختہ یقین ہےکہ اس طرح کے معاملات سے متعلق قانونی شقوں کے بارے میں بیداری پیدا کرنے اور شہریوں کو ان کے لازمی حقوق سے واقف کراکر انہیں بااختیار بنانے میں میڈیا اہم رول ادا کرسکتا ہے۔\n\n\nEx Input:\n46), Haridaspur Land Check Post (11.\n\nEx Output:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.62109375, -1.1015625, -0.62109375, -0.62109375, -0.62109375], "model_raw_response": "{\n    \"analysis\": \"This task is hard because it demands exact, token-level fidelity in translation (no additions or omissions) while handling fragments, punctuation, numbers, dates, parentheses, abbreviations, hyphens, and named entities. It tests bilingual fluency (English→Urdu), strict instruction-following, and the ability to translate literal/partial segments without normalizing or completing them—avoiding hallucination or restructuring the source.\",\n    \"new_prompt\": \"Translate the following single-line English text into Urdu. Do not add, omit, or rearrange any information. Preserve exactly all numbers, dates, parentheses, abbreviations, punctuation marks, hyphens, slashes, and the original token order. If the line is an incomplete fragment, translate it literally without completing the sentence. Output only the Urdu translation and nothing else.\\n\\nText: Power lines between Sector-7 (Node 46) and Haridaspur Land Check Post (11) were restored by 3,500 gang-men on 09/05; Govt. of Odisha reported 'partial recovery'—telecom still down.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is hard because it demands exact, token-level fidelity in translation (no additions or omissions) while handling fragments, punctuation, numbers, dates, parentheses, abbreviations, hyphens, and named entities. It tests bilingual fluency (English→Urdu), strict instruction-following, and the ability to translate literal/partial segments without normalizing or completing them—avoiding hallucination or restructuring the source.", "new_prompt": "Translate the following single-line English text into Urdu. Do not add, omit, or rearrange any information. Preserve exactly all numbers, dates, parentheses, abbreviations, punctuation marks, hyphens, slashes, and the original token order. If the line is an incomplete fragment, translate it literally without completing the sentence. Output only the Urdu translation and nothing else.\n\nText: Power lines between Sector-7 (Node 46) and Haridaspur Land Check Post (11) were restored by 3,500 gang-men on 09/05; Govt. of Odisha reported 'partial recovery'—telecom still down.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you will be presented with a question about part-of-speech tag of a word in the question. You should write an implausible POS tag to the question. Even though there exist multiple wrong answers, we only need a single wrong answer.  Here is the Alphabetical list of part-of-speech tags used in this task: CC: Coordinating conjunction, CD: Cardinal number, DT: Determiner, EX: Existential there, FW: Foreign word, IN: Preposition or subordinating conjunction, JJ: Adjective, JJR: Adjective, comparative, JJS: Adjective, superlative, LS: List item marker, MD: Modal, NN: Noun, singular or mass, NNS: Noun, plural, NNP: Proper noun, singular, NNPS: Proper noun, plural, PDT: Predeterminer, POS: Possessive ending, PRP: Personal pronoun, PRP$: Possessive pronoun, RB: Adverb, RBR: Adverb, comparative, RBS: Adverb, superlative, RP: Particle, SYM: Symbol, TO: to, UH: Interjection, VB: Verb, base form, VBD: Verb, past tense, VBG: Verb, gerund or present participle, VBN: Verb, past participle, VBP: Verb, non-3rd person singular present, VBZ: Verb, 3rd person singular present, WDT: Wh-determiner, WP: Wh-pronoun, WP$: Possessive wh-pronoun, WRB: Wh-adverb\nQ: What is the part-of-speech tag of the word \"traffic\" in the following question: How many flights per week go to Europe 's busiest airport by passenger traffic ?\nA:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-2.78125, 3.078125, 3.078125, 3.078125, 3.078125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines accurate linguistic analysis with a negative instruction. The model must: (1) determine the correct part-of-speech for a target word in context (requires contextual POS disambiguation and domain knowledge of the tag inventory); (2) intentionally produce an incorrect tag from the same inventory (following a \\\"give a wrong answer\\\" constraint), which is easy to mishandle by accidentally outputting the correct tag or misunderstanding what counts as \\\"implausible\\\"; and (3) format a single, valid tag choice (not multiple tags). The main difficulties are following the negative constraint reliably, avoiding plausible-but-correct alternatives for potentially ambiguous words, and ensuring the chosen label is drawn from the given tag set.\",\n    \"new_prompt\": \"In this task, you will be given a sentence and a target word. Using only the alphabetical list of part-of-speech tags below, produce a single part-of-speech tag that is NOT the correct tag for the target word in the sentence (i.e., give one implausible/incorrect tag). Only one wrong tag is required. Here is the tag set: CC: Coordinating conjunction, CD: Cardinal number, DT: Determiner, EX: Existential there, FW: Foreign word, IN: Preposition or subordinating conjunction, JJ: Adjective, JJR: Adjective, comparative, JJS: Adjective, superlative, LS: List item marker, MD: Modal, NN: Noun, singular or mass, NNS: Noun, plural, NNP: Proper noun, singular, NNPS: Proper noun, plural, PDT: Predeterminer, POS: Possessive ending, PRP: Personal pronoun, PRP$: Possessive pronoun, RB: Adverb, RBR: Adverb, comparative, RBS: Adverb, superlative, RP: Particle, SYM: Symbol, TO: to, UH: Interjection, VB: Verb, base form, VBD: Verb, past tense, VBG: Verb, gerund or present participle, VBN: Verb, past participle, VBP: Verb, non-3rd person singular present, VBZ: Verb, 3rd person singular present, WDT: Wh-determiner, WP: Wh-pronoun, WP$: Possessive wh-pronoun, WRB: Wh-adverb. Sentence: \\\"He left the phone charging on the table.\\\" Target word: \\\"charging\\\". A:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines accurate linguistic analysis with a negative instruction. The model must: (1) determine the correct part-of-speech for a target word in context (requires contextual POS disambiguation and domain knowledge of the tag inventory); (2) intentionally produce an incorrect tag from the same inventory (following a \"give a wrong answer\" constraint), which is easy to mishandle by accidentally outputting the correct tag or misunderstanding what counts as \"implausible\"; and (3) format a single, valid tag choice (not multiple tags). The main difficulties are following the negative constraint reliably, avoiding plausible-but-correct alternatives for potentially ambiguous words, and ensuring the chosen label is drawn from the given tag set.", "new_prompt": "In this task, you will be given a sentence and a target word. Using only the alphabetical list of part-of-speech tags below, produce a single part-of-speech tag that is NOT the correct tag for the target word in the sentence (i.e., give one implausible/incorrect tag). Only one wrong tag is required. Here is the tag set: CC: Coordinating conjunction, CD: Cardinal number, DT: Determiner, EX: Existential there, FW: Foreign word, IN: Preposition or subordinating conjunction, JJ: Adjective, JJR: Adjective, comparative, JJS: Adjective, superlative, LS: List item marker, MD: Modal, NN: Noun, singular or mass, NNS: Noun, plural, NNP: Proper noun, singular, NNPS: Proper noun, plural, PDT: Predeterminer, POS: Possessive ending, PRP: Personal pronoun, PRP$: Possessive pronoun, RB: Adverb, RBR: Adverb, comparative, RBS: Adverb, superlative, RP: Particle, SYM: Symbol, TO: to, UH: Interjection, VB: Verb, base form, VBD: Verb, past tense, VBG: Verb, gerund or present participle, VBN: Verb, past participle, VBP: Verb, non-3rd person singular present, VBZ: Verb, 3rd person singular present, WDT: Wh-determiner, WP: Wh-pronoun, WP$: Possessive wh-pronoun, WRB: Wh-adverb. Sentence: \"He left the phone charging on the table.\" Target word: \"charging\". A:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are to Translate German text to English\n\n[EX Q]: Die Erfindung ist in der Laserspektroskopie anwendbar.\n[EX A]: The invention is applicable to laser spectroscopy.\n\n[EX Q]: Die Zusammensetzungen eignen sich als Haftvermittler, insbesondere in Form von Primern.\n[EX A]: The compositions are suitable as adhesives, especially in the form of primers.\n\n[EX Q]: Die Erfindung betrifft eine loesbare Rohrverbindung.\n[EX A]:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-6.03125, -6.03125, -0.69921875, -6.03125, -6.03125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it tests technical translation skills rather than casual language use: the model must decompose German compound nouns (e.g., Rohrverbindung, Haftvermittler), choose the correct domain-specific equivalents (e.g., 'adhesion promoter' vs 'adhesive', 'detachable' vs 'soluble'), preserve the formal/patent register ('Die Erfindung betrifft ...' → 'The invention relates to ...'), and handle orthography variants (\\\"loesbare\\\" → lösbare). It also requires disambiguation from limited context and resists overfitting to surface patterns shown in examples (few-shot bias). Success requires accurate morphological parsing, domain-aware lexical selection, and strict instruction-following to produce a concise, technically appropriate translation.\",\n    \"new_prompt\": \"Translate the following German sentence into clear, formal English appropriate for a patent or technical specification. Preserve technical meaning and choose the most accurate technical terms rather than literal word-for-word translations. Output only the translated sentence.\\n\\nGerman: Die Erfindung betrifft eine lösbare Flanschverbindung zur Anwendung in korrosiven Medien.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it tests technical translation skills rather than casual language use: the model must decompose German compound nouns (e.g., Rohrverbindung, Haftvermittler), choose the correct domain-specific equivalents (e.g., 'adhesion promoter' vs 'adhesive', 'detachable' vs 'soluble'), preserve the formal/patent register ('Die Erfindung betrifft ...' → 'The invention relates to ...'), and handle orthography variants (\"loesbare\" → lösbare). It also requires disambiguation from limited context and resists overfitting to surface patterns shown in examples (few-shot bias). Success requires accurate morphological parsing, domain-aware lexical selection, and strict instruction-following to produce a concise, technically appropriate translation.", "new_prompt": "Translate the following German sentence into clear, formal English appropriate for a patent or technical specification. Preserve technical meaning and choose the most accurate technical terms rather than literal word-for-word translations. Output only the translated sentence.\n\nGerman: Die Erfindung betrifft eine lösbare Flanschverbindung zur Anwendung in korrosiven Medien.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nGiven a sentence, generate a most likely context or previous statement. The previous statement should be relevant to the given statement.\n\nHe hangs up, then shifts his gaze to someone.<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-3.09375, 5.03125, -2.0625, -1.28125, -9.0], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires pragmatic and narrative inference rather than factual lookup. The model must infer intention, causal sequence, and social context from a single sentence and choose one concise, coherent prior utterance or line that makes the given sentence plausible. Key skills tested: commonsense reasoning, theory-of-mind (inferring speaker intent and reactions), narrative coherence (tense, perspective, and causal link), and creative selection among many plausible continuations. Models often fail because the prompt is highly underspecified (many valid predecessors), requires picking the \\\"most likely\\\" one rather than any possible one, and demands a succinct, contextually appropriate sentence.\",\n    \"new_prompt\": \"Read the following sentence from a scene. Provide one concise sentence that most plausibly came immediately before it, making the action and reaction in the given sentence make sense. Your answer should be a single, relevant preceding statement.\\n\\nSentence: \\\"He rips the photograph in half, then looks toward the doorway.\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires pragmatic and narrative inference rather than factual lookup. The model must infer intention, causal sequence, and social context from a single sentence and choose one concise, coherent prior utterance or line that makes the given sentence plausible. Key skills tested: commonsense reasoning, theory-of-mind (inferring speaker intent and reactions), narrative coherence (tense, perspective, and causal link), and creative selection among many plausible continuations. Models often fail because the prompt is highly underspecified (many valid predecessors), requires picking the \"most likely\" one rather than any possible one, and demands a succinct, contextually appropriate sentence.", "new_prompt": "Read the following sentence from a scene. Provide one concise sentence that most plausibly came immediately before it, making the action and reaction in the given sentence make sense. Your answer should be a single, relevant preceding statement.\n\nSentence: \"He rips the photograph in half, then looks toward the doorway.\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given a country name and you need to answer with the government type of the country, as of the year 2015. The following are possible government types that are considered valid answers: Republic, Parliamentary Coprincipality, Federal Republic, Monarchy, Islamic Republic, Constitutional Monarchy, Parlementary Monarchy, Federation.\n\nCyprus\nRepublic\n\nZambia\nRepublic\n\nHong Kong<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-10.0, -10.0, -7.40625, -10.0, -10.0], "model_raw_response": "{\n    \"analysis\": \"This is challenging because it requires: (1) up-to-date geopolitical knowledge tied to a specific year (2015), (2) mapping many real-world political systems into a small, fixed taxonomy (forcing choices among similar or overlapping labels), and (3) following a strict output constraint (answer must be one of the listed types only). Additional difficulty comes from ambiguous or unusual cases (special administrative regions, coprincipalities, federations with monarchs), and from small wording issues in the label set (e.g., typos or very similar labels like \\\"Constitutional Monarchy\\\" vs \\\"Parlementary Monarchy\\\") that demand precise mapping rather than free-form answers.\",\n    \"new_prompt\": \"As of the year 2015, determine which single government type best describes the country named below. Choose exactly one answer from this list and output only that exact phrase (no extra words): Republic, Parliamentary Coprincipality, Federal Republic, Monarchy, Islamic Republic, Constitutional Monarchy, Parlementary Monarchy, Federation.\\n\\nCountry: Andorra\"\n}", "api_error": null, "parse_status": "success", "analysis": "This is challenging because it requires: (1) up-to-date geopolitical knowledge tied to a specific year (2015), (2) mapping many real-world political systems into a small, fixed taxonomy (forcing choices among similar or overlapping labels), and (3) following a strict output constraint (answer must be one of the listed types only). Additional difficulty comes from ambiguous or unusual cases (special administrative regions, coprincipalities, federations with monarchs), and from small wording issues in the label set (e.g., typos or very similar labels like \"Constitutional Monarchy\" vs \"Parlementary Monarchy\") that demand precise mapping rather than free-form answers.", "new_prompt": "As of the year 2015, determine which single government type best describes the country named below. Choose exactly one answer from this list and output only that exact phrase (no extra words): Republic, Parliamentary Coprincipality, Federal Republic, Monarchy, Islamic Republic, Constitutional Monarchy, Parlementary Monarchy, Federation.\n\nCountry: Andorra", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nInclude 10 more databases that we can use in addition to the ones mentioned<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.244140625, -0.1337890625, 0.2138671875, -1.1796875, -4.21875], "model_raw_response": "{\n    \"analysis\": \"The original prompt is challenging because it is highly underspecified and context-dependent: \\\"in addition to the ones mentioned\\\" requires either memory of prior conversation or a clarifying question. It tests several skills simultaneously — domain knowledge of many database technologies (across SQL, NoSQL, time-series, graph, columnar, search, object stores, etc.), the ability to generate a diverse and non-redundant list, and constraint-following (don’t repeat existing items). Models often fail by repeating items, hallucinating unfamiliar systems, or not asking for needed clarification. Good answers require taxonomy awareness (categorizing databases), concise justification for each choice, and attention to the explicit constraint to provide items distinct from those already listed.\",\n    \"new_prompt\": \"We currently use these database systems in our stack: MySQL, PostgreSQL, MongoDB, Redis, and Elasticsearch. Recommend 10 additional, distinct database technologies (do not repeat any of the five above) that would complement this stack for building a scalable analytics + operational platform; for each suggested database give (a) its name, (b) its category (e.g., wide-column, time-series, graph, NewSQL, object store, search, etc.), and (c) one concise sentence explaining its main strength and a typical use-case.\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is challenging because it is highly underspecified and context-dependent: \"in addition to the ones mentioned\" requires either memory of prior conversation or a clarifying question. It tests several skills simultaneously — domain knowledge of many database technologies (across SQL, NoSQL, time-series, graph, columnar, search, object stores, etc.), the ability to generate a diverse and non-redundant list, and constraint-following (don’t repeat existing items). Models often fail by repeating items, hallucinating unfamiliar systems, or not asking for needed clarification. Good answers require taxonomy awareness (categorizing databases), concise justification for each choice, and attention to the explicit constraint to provide items distinct from those already listed.", "new_prompt": "We currently use these database systems in our stack: MySQL, PostgreSQL, MongoDB, Redis, and Elasticsearch. Recommend 10 additional, distinct database technologies (do not repeat any of the five above) that would complement this stack for building a scalable analytics + operational platform; for each suggested database give (a) its name, (b) its category (e.g., wide-column, time-series, graph, NewSQL, object store, search, etc.), and (c) one concise sentence explaining its main strength and a typical use-case.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDefinition: This task is about translating a given Yoruba language sentence to English.\nInput: Títí di March 11, 2019, ìjọba Rọ́ṣíà ò ṣe nǹkan kan.\nOutput:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.46484375, -2.203125, -0.46484375, -2.203125, -2.203125], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires accurate translation from a low-resource language (Yoruba) with diacritics and tone marks, correct handling of negation and aspect (e.g., ò/kò ... nǹkan kan = 'did nothing' / 'did not do anything'), and proper rendering of temporal expressions (títí di = 'until') and named entities (Rọ́ṣíà = Russia, plus a date). Models often fail by mistranslating negation, mistranscribing diacritics, literal word-order errors (failing to render 'until [date]' correctly), or misformatting the named entity/date. The task therefore tests language-specific morphological and syntactic knowledge, Unicode handling, and careful semantic interpretation of negation and time phrases.\",\n    \"new_prompt\": \"Translate the following Yoruba sentence into English: \\\"Títí di Oṣù Kẹrin ọjọ́ kọ́kanlá, 2021, ìjọba Jamani kò ṣe nǹkan kan.\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires accurate translation from a low-resource language (Yoruba) with diacritics and tone marks, correct handling of negation and aspect (e.g., ò/kò ... nǹkan kan = 'did nothing' / 'did not do anything'), and proper rendering of temporal expressions (títí di = 'until') and named entities (Rọ́ṣíà = Russia, plus a date). Models often fail by mistranslating negation, mistranscribing diacritics, literal word-order errors (failing to render 'until [date]' correctly), or misformatting the named entity/date. The task therefore tests language-specific morphological and syntactic knowledge, Unicode handling, and careful semantic interpretation of negation and time phrases.", "new_prompt": "Translate the following Yoruba sentence into English: \"Títí di Oṣù Kẹrin ọjọ́ kọ́kanlá, 2021, ìjọba Jamani kò ṣe nǹkan kan.\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given a sentence. You are expected to write the name of the organism substances in the body mentioned in the input. Although there might be several correct answers, you need to write one of them.\nExample: Antisperm antibodies were assessed in the serum samples of 73 men unable to ejaculate naturally and on the sperm cells of 13 of these men.\nExample solution: serum\nExample explanation: Serum is a substance of the organism and needs to be tagged.\n\nProblem:  Data from this study suggest that there may be a time - dependent relationship between dexamethasone immunosuppression and the fecal concentration of E . coli O157 but that transient immunosuppression does not appear to prolong shedding of E . coli O157 .<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.5, 2.5, 2.5, 2.5, 2.1875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires: (1) domain-specific knowledge to recognize what counts as a bodily substance or specimen (serum, feces, CSF, urine, tissues, cells, etc.) versus other biomedical concepts like drugs, conditions, or processes (e.g., dexamethasone, immunosuppression, shedding); (2) fine-grained syntactic and semantic parsing to locate specimen mentions that may appear as modifiers (\\\"fecal concentration\\\") or inside complex noun phrases; (3) handling of synonyms and morphological variants (\\\"feces\\\" vs \\\"stool\\\" vs \\\"fecal matter\\\"); and (4) adherence to the output constraint to produce exactly one valid substance name even when multiple are present. Models often fail by selecting the wrong semantic class (e.g., picking a drug or organism instead of the specimen), by returning explanations instead of a single token/phrase, or by mis-parsing adjectival forms like \\\"fecal\\\".\",\n    \"new_prompt\": \"Instruction: From the single sentence below, identify one bodily material, fluid, tissue, cell type, or excreta mentioned. If the sentence names more than one, provide just one of them. Output only the name of the substance (no explanation). Sentence: After patients received high-dose corticosteroids, transient increases in bacterial load were detected in bronchoalveolar lavage fluid and in saliva, while blood cultures remained negative.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires: (1) domain-specific knowledge to recognize what counts as a bodily substance or specimen (serum, feces, CSF, urine, tissues, cells, etc.) versus other biomedical concepts like drugs, conditions, or processes (e.g., dexamethasone, immunosuppression, shedding); (2) fine-grained syntactic and semantic parsing to locate specimen mentions that may appear as modifiers (\"fecal concentration\") or inside complex noun phrases; (3) handling of synonyms and morphological variants (\"feces\" vs \"stool\" vs \"fecal matter\"); and (4) adherence to the output constraint to produce exactly one valid substance name even when multiple are present. Models often fail by selecting the wrong semantic class (e.g., picking a drug or organism instead of the specimen), by returning explanations instead of a single token/phrase, or by mis-parsing adjectival forms like \"fecal\".", "new_prompt": "Instruction: From the single sentence below, identify one bodily material, fluid, tissue, cell type, or excreta mentioned. If the sentence names more than one, provide just one of them. Output only the name of the substance (no explanation). Sentence: After patients received high-dose corticosteroids, transient increases in bacterial load were detected in bronchoalveolar lavage fluid and in saliva, while blood cultures remained negative.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you're given a context passage, a question, and three answer options. Your task is to return an incorrect answer option to the question from the choices given. For all questions, only one of the three answer options is correct. Pick one of the two incorrect answer options as the output.\n\nEx Input:\nContext: Taylor said that they wanted to get a cat for Kai's birthday. \n Question: What will Kai want to do next? \n Options: (A) get a cat (B) go to school (C) buy a boat\n\nEx Output:\nB\n\n\nEx Input:\nContext: Carson sent Bailey's lunch to school after he forgot to pack it. \n Question: What will Carson want to do next? \n Options: (A) remember bailey's lunch (B) make sure the lunch gets there (C) say sorry multiple times\n\nEx Output:\nC\n\n\nEx Input:\nContext: Addison broke every rule in the game's rule book without realizing they were cheating. \n Question: What will Addison want to do next? \n Options: (A) wanted to be the best (B) apologize to the other players (C) keep cheating now that they know\n\nEx Output:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.96875, -4.96875, -4.96875, -4.96875, -4.96875], "model_raw_response": "{\n  \"analysis\": \"This prompt is hard because it requires two-step reasoning plus strict instruction-following: (1) use commonsense to infer which single option is the correct/most likely answer from a short context, and (2) intentionally avoid that correct option and instead output one of the two incorrect choices. That tests negative-constraint following (choosing an incorrect item on purpose), inhibitory control (don’t default to picking the plausible/correct choice), precise parsing of the prompt (output only a single letter), and short-form output formatting. Models often fail by (a) selecting the correct answer instead of an incorrect one, (b) producing extra text instead of a single letter, or (c) mis-parsing ambiguous scenarios — so the prompt needs clear, unambiguous inference and a strict output requirement.\",\n  \"new_prompt\": \"You will be given a short scene, a follow-up question about what is most likely to happen next, and three labeled answer choices (A, B, C). Exactly one of the choices is the correct inference. Your task: deliberately pick one of the two incorrect choices (i.e., an answer that is NOT the correct inference) and output only its letter (A, B, or C). Do not output any other text.\\n\\nContext: Lena spilled coffee on her shirt right before an important meeting and walked straight to the building restroom to clean up.\\nQuestion: What will Lena most likely do next?\\nOptions: (A) wipe the coffee off, freshen up, and return to the meeting\\n         (B) ignore the stain and leave the building immediately without cleaning up\\n         (C) call colleagues to postpone the meeting because she has a minor stain\\n\\nOutput:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is hard because it requires two-step reasoning plus strict instruction-following: (1) use commonsense to infer which single option is the correct/most likely answer from a short context, and (2) intentionally avoid that correct option and instead output one of the two incorrect choices. That tests negative-constraint following (choosing an incorrect item on purpose), inhibitory control (don’t default to picking the plausible/correct choice), precise parsing of the prompt (output only a single letter), and short-form output formatting. Models often fail by (a) selecting the correct answer instead of an incorrect one, (b) producing extra text instead of a single letter, or (c) mis-parsing ambiguous scenarios — so the prompt needs clear, unambiguous inference and a strict output requirement.", "new_prompt": "You will be given a short scene, a follow-up question about what is most likely to happen next, and three labeled answer choices (A, B, C). Exactly one of the choices is the correct inference. Your task: deliberately pick one of the two incorrect choices (i.e., an answer that is NOT the correct inference) and output only its letter (A, B, or C). Do not output any other text.\n\nContext: Lena spilled coffee on her shirt right before an important meeting and walked straight to the building restroom to clean up.\nQuestion: What will Lena most likely do next?\nOptions: (A) wipe the coffee off, freshen up, and return to the meeting\n         (B) ignore the stain and leave the building immediately without cleaning up\n         (C) call colleagues to postpone the meeting because she has a minor stain\n\nOutput:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDefinition: Given a sentence in English, provide an equivalent paraphrased translation in Chinese that retains the same meaning both through the translation and the paraphrase.\nInput: Chad Ochocinco ( born 1978 ; formerly Chad Johnson ) is an American football wide receiver .\nOutput:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.40625, -5.6875, -13.4375, -9.0, -6.34375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires strong bilingual ability plus paraphrasing: the model must translate from English to Chinese while also rephrasing (not producing a literal, word-for-word rendering) yet preserve all factual details (names, dates, former names, occupations) and punctuation/parenthetical information. It tests entity transliteration vs retention of original names, fidelity to facts versus natural wording, and obeying an implicit output constraint (produce only the paraphrased Chinese sentence). Common failure modes include dropping or altering parenthetical details, mistranslating names or dates, producing literal translations that aren’t natural, or adding extra explanation.\",\n    \"new_prompt\": \"Translate the following English sentence into Chinese, but do not translate word-for-word: produce a natural Chinese paraphrase that preserves every factual detail (including dates, birth/former names, occupations, and parenthetical information). Output only one Chinese sentence with the same meaning.\\n\\nEnglish sentence: Malcolm X (1925–1965; born Malcolm Little) was an African American Muslim minister and human rights activist.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires strong bilingual ability plus paraphrasing: the model must translate from English to Chinese while also rephrasing (not producing a literal, word-for-word rendering) yet preserve all factual details (names, dates, former names, occupations) and punctuation/parenthetical information. It tests entity transliteration vs retention of original names, fidelity to facts versus natural wording, and obeying an implicit output constraint (produce only the paraphrased Chinese sentence). Common failure modes include dropping or altering parenthetical details, mistranslating names or dates, producing literal translations that aren’t natural, or adding extra explanation.", "new_prompt": "Translate the following English sentence into Chinese, but do not translate word-for-word: produce a natural Chinese paraphrase that preserves every factual detail (including dates, birth/former names, occupations, and parenthetical information). Output only one Chinese sentence with the same meaning.\n\nEnglish sentence: Malcolm X (1925–1965; born Malcolm Little) was an African American Muslim minister and human rights activist.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nYou will be given a definition of a task first, then an example. Follow the example to solve a new instance of the task.\nGiven a statement and question, generate the answer to the question such that the answer is contained in the statement.\n\nstatement: Neurons deliver electrical impulses to other cells, question: What do neurons deliver to other cells?\nSolution: Electrical impulses\nWhy? The answer generated is correct with respect to the question and the statement\n\nNew input: statement: steel is used for bearing weight by suspension bridges, question: What is steel used for bearing weight?\nSolution:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-6.0625, -8.8125, -5.28125, -2.09375, -5.28125], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it tests precise extractive question answering: the model must map the question onto the statement's syntax to pick the correct contiguous substring (span) as the answer. Key skills include short-span extraction, understanding which noun phrase or prepositional phrase answers the question, and obeying the negative constraint that the output must be taken verbatim from the statement (no paraphrasing, no added or omitted words). Common failure modes are paraphrasing the answer, returning a related but non-verbatim phrase, or selecting the wrong span when multiple plausible substrings exist.\",\n    \"new_prompt\": \"Instruction: Extract the answer to the question by selecting a contiguous substring from the statement. The answer must appear verbatim in the statement — do not paraphrase, add, or remove words. Output only the substring that answers the question.\\n\\nstatement: Modern glass skyscrapers commonly use tempered glass for exterior cladding.\\nquestion: What do modern glass skyscrapers commonly use for exterior cladding?\\nSolution:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it tests precise extractive question answering: the model must map the question onto the statement's syntax to pick the correct contiguous substring (span) as the answer. Key skills include short-span extraction, understanding which noun phrase or prepositional phrase answers the question, and obeying the negative constraint that the output must be taken verbatim from the statement (no paraphrasing, no added or omitted words). Common failure modes are paraphrasing the answer, returning a related but non-verbatim phrase, or selecting the wrong span when multiple plausible substrings exist.", "new_prompt": "Instruction: Extract the answer to the question by selecting a contiguous substring from the statement. The answer must appear verbatim in the statement — do not paraphrase, add, or remove words. Output only the substring that answers the question.\n\nstatement: Modern glass skyscrapers commonly use tempered glass for exterior cladding.\nquestion: What do modern glass skyscrapers commonly use for exterior cladding?\nSolution:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nIn this task, you are given two phrases: Head and Tail, separated with <sep>. The Head and the Tail events are short phrases possibly involving participants. The names of specific people have been replaced by generic words (e.g., PersonX, PersonY, PersonZ). PersonX is always the subject of the event. You have to determine whether the Head is located or can be found at/in/on the Tail or not. Classify your answers into \"Yes\" and \"No\". The phrase may also contain \"___\", a placeholder that can be an object, a person, and/or an action.\nOne example is below.\nQ: Head: coupon<sep>Tail: market\nA: Yes\nRationale: This is a good example. The Head can be found at the Tail.\nQ: Head: college<sep>Tail: large city\nA:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.8359375, 0.447265625, -0.8359375, -1.5078125, -0.95703125], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires commonsense spatial and real-world knowledge plus flexible lexical reasoning. The model must judge whether one phrase can plausibly be located at/in/on another despite ambiguity, polysemy, and underspecified context (e.g., institutions vs. physical places, or words with multiple senses). It tests: (1) world knowledge about typical co-occurrence and locations; (2) sense disambiguation (a word may have multiple meanings, some of which fit); (3) tolerance for existential reading ('can be found' vs. 'always located'); and (4) strict adherence to the binary output constraint (only Yes/No). Models often fail by being too literal, misresolving polysemy, or confusing possibility with necessity.\",\n    \"new_prompt\": \"You will be given two short phrases separated by '<sep>': the first is the Head (an entity or event), the second is the Tail (a place or larger context). Decide whether the Head can plausibly be located at/in/on the Tail in real-world situations (i.e., whether it is possible for the Head to be found there). Reply with exactly Yes or No and nothing else.\\n\\nQ: Head: bank<sep>Tail: river\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires commonsense spatial and real-world knowledge plus flexible lexical reasoning. The model must judge whether one phrase can plausibly be located at/in/on another despite ambiguity, polysemy, and underspecified context (e.g., institutions vs. physical places, or words with multiple senses). It tests: (1) world knowledge about typical co-occurrence and locations; (2) sense disambiguation (a word may have multiple meanings, some of which fit); (3) tolerance for existential reading ('can be found' vs. 'always located'); and (4) strict adherence to the binary output constraint (only Yes/No). Models often fail by being too literal, misresolving polysemy, or confusing possibility with necessity.", "new_prompt": "You will be given two short phrases separated by '<sep>': the first is the Head (an entity or event), the second is the Tail (a place or larger context). Decide whether the Head can plausibly be located at/in/on the Tail in real-world situations (i.e., whether it is possible for the Head to be found there). Reply with exactly Yes or No and nothing else.\n\nQ: Head: bank<sep>Tail: river", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nGiven a sentence in the Japanese and Lao language. Your task is check if the Lao sentence is translation of Japanese. if the translation is correct than generate label \"Yes\", otherwise generate label \"No\".\n--------\nQuestion: Japanese: それはまた、一つの窓口でそういったネットワークを承認するシステムとし、必要な許可を3ヵ月以内に与えることを保証するよう提案した。 \n Lao: ໄດ້ມີການສະເໜີ ລະບົບການຊໍາລະແບບປະຕູດຽວ ເພື່ອອະນຸຍາດເຄືອຂ່າຍດັ່ງກ່າວ ກັບການຮັບປະກັນ ເຊິ່ງການຊຳລະທີ່ຈຳເປັນ ຈະຖືກສະໜອງພາຍໃນສາມເດືອນ.\n\nAnswer: Yes\n\n\nQuestion: Japanese: 警察は、彼の遺体の近くに自筆で「汝、殺すなかれ」と書かれているのを発見した。 \n Lao: ຕຳຫຼວດພົບປ້າຍທີ່ເຮັດເອງໃກ້ກັບສົບຂອງລາວເຊິ່ງຂຽນວ່າ \"ເຈົ້າຈະບໍ່ຕາຍ.\"\n\nAnswer: Yes\n\n\nQuestion: Japanese: ウォールストリートジャーナルによると、フォーラムは、従業員が研修会に参加することを強制したことを認めない。 \n Lao: ອີງຕາມວາລະສານ The Wall Street, The Forum ໄດ້ເວົ້າວ່າບໍ່ອາດຈະອະນຸຍາດໃຫ້ກຳມະກອນຖືກບັງຄັບ ເພື່ອເຂົ້າຮ່ວມວາລະການຝຶກອົບຮົມ.\n\nAnswer:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.018798828125, -2.359375, -0.018798828125, -0.018798828125, -0.018798828125], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires accurate cross-lingual semantic equivalence judgment between Japanese and Lao — not just surface word overlap. The model must understand both languages (scripts, grammar, particles, tense, modality, quantifiers like “unanimously” vs “by majority”, time expressions, numeric/time constraints), detect subtle meaning shifts, handle partial transliterations/mixed-script tokens, and decide binary correctness (Yes/No) without extra explanation. It therefore tests bilingual comprehension, fine-grained paraphrase/negation detection, and attention to small but meaning-critical differences.\",\n    \"new_prompt\": \"You will be given one Japanese sentence and one Lao sentence. Decide whether the Lao sentence is an accurate translation of the Japanese sentence. If it is an accurate translation, output exactly Yes. If it is not an accurate translation, output exactly No. Do not add any other text. \\n\\nQuestion: Japanese: 委員会は月曜日にその提案を全会一致で承認した。 \\nLao: ຄະນະກຳມະການໄດ້ອະນຸມັດແຜນງານນັ້ນໂດຍສ່ວນຫຼາຍໃນມື້ຈັນ. \\n\\nAnswer:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires accurate cross-lingual semantic equivalence judgment between Japanese and Lao — not just surface word overlap. The model must understand both languages (scripts, grammar, particles, tense, modality, quantifiers like “unanimously” vs “by majority”, time expressions, numeric/time constraints), detect subtle meaning shifts, handle partial transliterations/mixed-script tokens, and decide binary correctness (Yes/No) without extra explanation. It therefore tests bilingual comprehension, fine-grained paraphrase/negation detection, and attention to small but meaning-critical differences.", "new_prompt": "You will be given one Japanese sentence and one Lao sentence. Decide whether the Lao sentence is an accurate translation of the Japanese sentence. If it is an accurate translation, output exactly Yes. If it is not an accurate translation, output exactly No. Do not add any other text. \n\nQuestion: Japanese: 委員会は月曜日にその提案を全会一致で承認した。 \nLao: ຄະນະກຳມະການໄດ້ອະນຸມັດແຜນງານນັ້ນໂດຍສ່ວນຫຼາຍໃນມື້ຈັນ. \n\nAnswer:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDetailed Instructions: Given a concept word, generate a hypernym for it. A hypernym is a superordinate, i.e. a word with a broad meaning constituting a category, that generalizes another word. For example, color is a hypernym of red.\nQ: policeman\nA:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.15625, -4.15625, -4.15625, -3.671875, -3.671875], "model_raw_response": "{\n    \"analysis\": \"This task is challenging because it requires precise lexical-semantic knowledge (understanding hyponym/hypernym relations) plus good word-sense disambiguation and selection of the correct taxonomic level. Models often confuse synonyms, coordinate terms, definitions, or too-general categories (e.g., returning \\\"person\\\" or \\\"entity\\\") with the required hypernym; they also sometimes output multiword phrases, explanations, or the same word in a gender-neutral form instead of a broader category. The prompt tests (1) recognition of part of speech and common sense category hierarchies, (2) choosing the intended sense of an ambiguous noun, and (3) following a tight output constraint (produce a single appropriate hypernym, not an explanation). Ambiguity about how general the hypernym should be (immediate vs. very general) is a common failure point unless specified.\",\n    \"new_prompt\": \"Instruction: Provide the most specific single-word hypernym (i.e., the immediate broader category) for the noun given below. Reply with only that one noun and nothing else.\\n\\nNoun: sparrow\\nAnswer:\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is challenging because it requires precise lexical-semantic knowledge (understanding hyponym/hypernym relations) plus good word-sense disambiguation and selection of the correct taxonomic level. Models often confuse synonyms, coordinate terms, definitions, or too-general categories (e.g., returning \"person\" or \"entity\") with the required hypernym; they also sometimes output multiword phrases, explanations, or the same word in a gender-neutral form instead of a broader category. The prompt tests (1) recognition of part of speech and common sense category hierarchies, (2) choosing the intended sense of an ambiguous noun, and (3) following a tight output constraint (produce a single appropriate hypernym, not an explanation). Ambiguity about how general the hypernym should be (immediate vs. very general) is a common failure point unless specified.", "new_prompt": "Instruction: Provide the most specific single-word hypernym (i.e., the immediate broader category) for the noun given below. Reply with only that one noun and nothing else.\n\nNoun: sparrow\nAnswer:", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nWhat were some of the most difficult scenes or moments to capture during the project, and how did the cinematographer address those challenges?<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.59375, -4.59375, -17.125, -17.125, -17.125], "model_raw_response": "{\n    \"analysis\": \"The original prompt is challenging because it asks for concrete, technical recollections about “the project” without specifying which production, so the model must either ask a clarifying question or risk inventing details. It tests deep domain knowledge of cinematography (lighting, camera choice, rigs, VFX/plate strategy), practical problem‑solving (logistics, crew coordination, safety), and the ability to give specific, causal explanations rather than vague generalities. Failures typically come from hallucinating project‑specific facts, giving only high‑level platitudes, or missing the logistical and technical tradeoffs that make an answer useful.\",\n    \"new_prompt\": \"Pick one particularly difficult type of scene commonly encountered on a professional film set (for example, a long continuous take with complex actor and camera choreography, a night rain sequence, or a green‑screen action scene). Describe the specific technical and logistical challenges that scene presents to a cinematographer and provide a detailed, step‑by‑step plan of how the cinematographer would address those challenges (including choices of camera, lenses, lighting strategy, rigging/gear, crew roles, rehearsal/vis‑tech, and any postproduction/VFX considerations).\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is challenging because it asks for concrete, technical recollections about “the project” without specifying which production, so the model must either ask a clarifying question or risk inventing details. It tests deep domain knowledge of cinematography (lighting, camera choice, rigs, VFX/plate strategy), practical problem‑solving (logistics, crew coordination, safety), and the ability to give specific, causal explanations rather than vague generalities. Failures typically come from hallucinating project‑specific facts, giving only high‑level platitudes, or missing the logistical and technical tradeoffs that make an answer useful.", "new_prompt": "Pick one particularly difficult type of scene commonly encountered on a professional film set (for example, a long continuous take with complex actor and camera choreography, a night rain sequence, or a green‑screen action scene). Describe the specific technical and logistical challenges that scene presents to a cinematographer and provide a detailed, step‑by‑step plan of how the cinematographer would address those challenges (including choices of camera, lenses, lighting strategy, rigging/gear, crew roles, rehearsal/vis‑tech, and any postproduction/VFX considerations).", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDetailed Instructions: A text is given in Panjabi. Translate it from the Panjabi language to the Urdu language. The translation must not omit or add information to the original sentence.\nProblem:میں ملک کی ترقی کے لئے ، ملک کے ہرعلاقے میں ترقی کے لئے پوری نیک نیتی کے ساتھ ، پوری ایمانداری کے ساتھ ، دن رات جتنا بھی کرسکتاہوں ، کرنے میں کوئی کمی نہیں رہنے دیتا۔ آپ کا آشیرواد مجھ پرایسے ہی بنارہے ، اسی خواہش کے ساتھ اپنی بات ختم کرتاہوں ۔\nSolution:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-0.099609375, 1.3515625, 3.4375, -1.6484375, 5.71875], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it combines language identification with high-fidelity translation between closely related languages that share the same script (Shahmukhi Punjabi vs. Urdu). Models must detect the true source language despite possible mislabeling, preserve subtle register and cultural nuances (honorifics, prayers, tense, obligation), and obey the strict constraint of not adding or omitting information. Errors commonly arise from mislabeling the input language, collapsing similar vocabulary into incorrect equivalents, or altering tone while “cleaning up” the text.\",\n    \"new_prompt\": \"Translate the following Shahmukhi Punjabi sentence into Urdu. Do not add, omit, or change any information; preserve the original tone and formality. Provide only the Urdu translation.\\n\\nSentence: مینوں اپنے علاقے دی ترقی لئی پوری محنت تے ایمانداری نال دن رات لگے رہنا اے؛ جو کچھ میری وس اے اوہ سب کرنا میرا فرض اے۔ تہاڈی دعا میرے نال ای رہوے، ایس خواہش نال اپنی گل مکدی اے۔\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it combines language identification with high-fidelity translation between closely related languages that share the same script (Shahmukhi Punjabi vs. Urdu). Models must detect the true source language despite possible mislabeling, preserve subtle register and cultural nuances (honorifics, prayers, tense, obligation), and obey the strict constraint of not adding or omitting information. Errors commonly arise from mislabeling the input language, collapsing similar vocabulary into incorrect equivalents, or altering tone while “cleaning up” the text.", "new_prompt": "Translate the following Shahmukhi Punjabi sentence into Urdu. Do not add, omit, or change any information; preserve the original tone and formality. Provide only the Urdu translation.\n\nSentence: مینوں اپنے علاقے دی ترقی لئی پوری محنت تے ایمانداری نال دن رات لگے رہنا اے؛ جو کچھ میری وس اے اوہ سب کرنا میرا فرض اے۔ تہاڈی دعا میرے نال ای رہوے، ایس خواہش نال اپنی گل مکدی اے۔", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nGiven a factoid/trivia type question, generate the topic of the question. The topic is the entity the question talks about.\n\n[Q]: Which legal drama starring Martin Shaw ran for six series on BBC TV from 2001?\n[A]: martin shaw\n\n\n[Q]: Which Frenchman composed Clair de lune, the famous third movement of Suite bergamasque?\n[A]: suite bergamasque\n\n\n[Q]: What is the alcoholic ingredient of a Screwdriver cocktail?\n[A]:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [-4.1875, -5.625, -4.1875, -4.1875, -5.625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires the model to distinguish the 'topic' of a trivia question (the entity the question is about) from the entity that would be the factual answer. That tests fine-grained reading comprehension and entity extraction (identifying the central named-entity or noun-phrase), handling of negative constraints (explicitly do NOT supply the answer), and sensitivity to syntactic/semantic focus rather than world-knowledge retrieval. Models often default to producing the factual answer or pick a different mentioned entity, so clear instruction-following and focus detection are essential.\",\n    \"new_prompt\": \"Instruction: From the trivia question below, extract and return the main entity that the question is ABOUT — the central noun or named entity mentioned or implicitly referenced. Do NOT provide the correct answer to the trivia question; output only the topic as a concise phrase.\\n\\nTrivia question: Which Irish playwright wrote the play 'Waiting for Godot'?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires the model to distinguish the 'topic' of a trivia question (the entity the question is about) from the entity that would be the factual answer. That tests fine-grained reading comprehension and entity extraction (identifying the central named-entity or noun-phrase), handling of negative constraints (explicitly do NOT supply the answer), and sensitivity to syntactic/semantic focus rather than world-knowledge retrieval. Models often default to producing the factual answer or pick a different mentioned entity, so clear instruction-following and focus detection are essential.", "new_prompt": "Instruction: From the trivia question below, extract and return the main entity that the question is ABOUT — the central noun or named entity mentioned or implicitly referenced. Do NOT provide the correct answer to the trivia question; output only the topic as a concise phrase.\n\nTrivia question: Which Irish playwright wrote the play 'Waiting for Godot'?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nTeacher:In this task, you're given statements in native Kannada language. The statement can be written with the Kannada alphabet or the English alphabet. Your job is to evaluate if the statement is offensive or not. Label the post as \"Not offensive\" if the post does not contain offense or insult.  Non-offensive posts do not include any form of offense or insult. Label the post as \"Offensive\" if the post contains offensive language. \nTeacher: Now, understand the problem? Solve this instance: First hog film nodu. Amele yen bekadru comment madu.\nStudent:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.90625, 0.388671875, 1.4765625, 3.328125, 4.90625], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it mixes language identification, transliteration handling, and subtle pragmatic judgment in a very short, colloquial Kannada utterance. Models must (1) understand Kannada whether it's written in Kannada script or Latin letters, (2) normalize and interpret colloquial/ambiguous words and sentence structure, and (3) apply culturally informed toxicity detection to decide if a phrase is insulting or merely directive. Additionally, the original prompt's dialogue scaffolding (Teacher/Student) and the requirement to output one of two exact labels demand strict instruction-following; failing to adhere to the exact label format or adding commentary is a common error.\",\n    \"new_prompt\": \"You will be given a single sentence in Kannada (it may be written in Kannada script or in Latin transliteration). Determine whether the sentence contains offensive language. If it contains any insult or offensive expression, output exactly: Offensive. If it does not contain any offense or insult, output exactly: Not offensive. Output only one of those two labels and nothing else. Sentence: \\\"Idu nimage ishta alla andre comment maadbedi.\\\"\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it mixes language identification, transliteration handling, and subtle pragmatic judgment in a very short, colloquial Kannada utterance. Models must (1) understand Kannada whether it's written in Kannada script or Latin letters, (2) normalize and interpret colloquial/ambiguous words and sentence structure, and (3) apply culturally informed toxicity detection to decide if a phrase is insulting or merely directive. Additionally, the original prompt's dialogue scaffolding (Teacher/Student) and the requirement to output one of two exact labels demand strict instruction-following; failing to adhere to the exact label format or adding commentary is a common error.", "new_prompt": "You will be given a single sentence in Kannada (it may be written in Kannada script or in Latin transliteration). Determine whether the sentence contains offensive language. If it contains any insult or offensive expression, output exactly: Offensive. If it does not contain any offense or insult, output exactly: Not offensive. Output only one of those two labels and nothing else. Sentence: \"Idu nimage ishta alla andre comment maadbedi.\"", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nYou are given a conversation between two people.'Person1:' and 'Person2:' are used to separate their respective dialogues. You have to classify if there exist more than 2 unique emotions in conversation. If there are more than 2 unique emotions present in the conversation, the output should be classified as '1' else it should be '0'\n\nInput: Consider Input: Person1:     What in the world is that smell ? \nPerson2: The aroma of roasting coffee beans . \nPerson1:     Smells like you're baking something . What are those ? \nPerson2: Green beans . They pop and turn brown when you roast them . \nPerson1:     Cool ! But isn't that a hot air popcorn popper ? \nPerson2: This machine roasts the beans just right . If you roast them too long , or the temperature is too hot ... \nPerson1:     The beans will burn . I know . I've tasted burnt coffee before ... yuck ! \n\nOutput: 0\n\n\nInput: Consider Input: Person1:     Wow , this is drop-dead gorgeous . \nPerson2: So no more regret now ? \nPerson1:     Don't remind me of that . This would be the last time I cut any classes for a basketball game . \nPerson2: Come on . Don't be so hard on yourself . Enjoy your life . \nPerson1:     I just can't help it . Oh , look at Clive , he is really a big hit . They are definitely gonna win . \nPerson2: It's only seven minutes into the game , too early to cheer up . \nPerson1:     Come on , 18 points already , they are never gonna catch up . \nPerson2: Who knows ! It's a long game . Look ! That was a really nice shot ! \nPerson1:     Thay are coming back to narrow the gap . \nPerson2: Wait , that guy , I know him . He is just a bench warmer . \nPerson1:     A secret weapon . They'd better set up the defense now . \nPerson2: Clive is ready to explode . Keep your eyes open . \nPerson1:     Oh , man ! I can't believe it . A slam dunk ! \nPerson2: Good that he resembled his old , quick self . \n\nOutput: 1\n\n\nInput: Consider Input: Person1:     What's your favourite steps ? \nPerson2: It's hard to say . But I do love waltz very much . \nPerson1:     How about rumba ? \nPerson2: To tell the truth , I don't like it . \nPerson1:     Really ? You must like tango , right ? \nPerson2: Oh , it's really beautiful , but I just can't dance it well . \nPerson1:     Don't be so modest . I'm sure you can dance it very well . Let's dance it now . \nPerson2: OK . I'll have a try . \nPerson1:     Oh , you dance it so well . \nPerson2: Thank you for your compliment . \nPerson1:     I think I'm your lucky fellow .<end_of_turn>\n<start_of_turn>model\n", "original_scores": [0.07080078125, -0.045166015625, -0.045166015625, -0.045166015625, -0.045166015625], "model_raw_response": "{\n    \"analysis\": \"This task is hard because it requires accurate, fine-grained emotion recognition across a multi-turn dialogue and then an exact, global aggregation (counting unique emotion categories). Skills tested: • Utterance-level emotion classification (including indirectly expressed or implied feelings). • Pragmatic inference from short, colloquial turns (detecting sarcasm, fear, annoyance, etc.). • Normalizing synonymous or intensity-based variants to a fixed set of emotion categories (e.g., “devastated” vs “sad” vs “upset”). • Discourse-level aggregation (ensuring duplicates aren’t double-counted and neutral/irrelevant turns don’t inflate the count). Models often fail by mislabeling subtle turns, treating intensity or rephrasing as separate emotions, or miscounting because of ambiguous/neutral utterances.\",\n    \"new_prompt\": \"Instruction: You will be given a short conversation between two speakers labeled 'A:' and 'B:'. Using only these emotion categories — joy, sadness, anger, fear, surprise, disgust, neutral — assign each turn the single strongest emotion it expresses, then count how many distinct emotion categories appear anywhere in the conversation. If the conversation contains more than two distinct emotions, output '1'. Otherwise output '0'. Conversation: A: I just found out I got the promotion — I can't stop smiling! B: That's wonderful news, congratulations! A: Thanks, but I have to move across the country next month and I'm honestly a bit terrified. B: You’ll manage; it’s a great opportunity. A: Easy for you to say — you never had our manager take credit for your work, it makes me furious. B: Wait, really? I had no idea. A: Yeah, last week he took credit in the meeting and I was crushed. B: Ugh, that's terrible and so unfair.\"\n}", "api_error": null, "parse_status": "success", "analysis": "This task is hard because it requires accurate, fine-grained emotion recognition across a multi-turn dialogue and then an exact, global aggregation (counting unique emotion categories). Skills tested: • Utterance-level emotion classification (including indirectly expressed or implied feelings). • Pragmatic inference from short, colloquial turns (detecting sarcasm, fear, annoyance, etc.). • Normalizing synonymous or intensity-based variants to a fixed set of emotion categories (e.g., “devastated” vs “sad” vs “upset”). • Discourse-level aggregation (ensuring duplicates aren’t double-counted and neutral/irrelevant turns don’t inflate the count). Models often fail by mislabeling subtle turns, treating intensity or rephrasing as separate emotions, or miscounting because of ambiguous/neutral utterances.", "new_prompt": "Instruction: You will be given a short conversation between two speakers labeled 'A:' and 'B:'. Using only these emotion categories — joy, sadness, anger, fear, surprise, disgust, neutral — assign each turn the single strongest emotion it expresses, then count how many distinct emotion categories appear anywhere in the conversation. If the conversation contains more than two distinct emotions, output '1'. Otherwise output '0'. Conversation: A: I just found out I got the promotion — I can't stop smiling! B: That's wonderful news, congratulations! A: Thanks, but I have to move across the country next month and I'm honestly a bit terrified. B: You’ll manage; it’s a great opportunity. A: Easy for you to say — you never had our manager take credit for your work, it makes me furious. B: Wait, really? I had no idea. A: Yeah, last week he took credit in the meeting and I was crushed. B: Ugh, that's terrible and so unfair.", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nDefinition: In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.\nInput: We see that A-gen performance improves significantly with the joint model: both F1 and EM increase by about 10 percentage points. \n Question: How much improvement does jointly learning QA and QG give, compared to only training QA?\nOutput:<end_of_turn>\n<start_of_turn>model\n", "original_scores": [2.234375, 3.203125, 3.5, 2.375, 0.458984375], "model_raw_response": "{\n    \"analysis\": \"This prompt is challenging because it requires precise reading comprehension and numeric extraction from a short academic-style sentence. Models must (1) identify that \\\"joint model\\\" means jointly learning QA and QG versus QA-only, (2) extract the numeric change described (\\\"about 10 percentage points\\\"), and (3) report that change for both metrics (F1 and EM). Common failure modes include misunderstanding \\\"percentage points\\\" vs \\\"percent\\\", ignoring that both F1 and EM change, or answering qualitatively instead of giving the numeric amount.\",\n    \"new_prompt\": \"Definition: You will be given a short excerpt from an academic paper (context) followed by a question. Answer the question using only the information in the excerpt.\\n\\nInput: The paper states that combining question answering (QA) with question generation (QG) into a single joint model improves results: both Exact Match (EM) and F1 score rise by roughly 10 percentage points relative to a QA-only system.\\n\\nQuestion: How many percentage points higher are EM and F1 when QA and QG are trained jointly compared to training QA alone?\"\n}", "api_error": null, "parse_status": "success", "analysis": "This prompt is challenging because it requires precise reading comprehension and numeric extraction from a short academic-style sentence. Models must (1) identify that \"joint model\" means jointly learning QA and QG versus QA-only, (2) extract the numeric change described (\"about 10 percentage points\"), and (3) report that change for both metrics (F1 and EM). Common failure modes include misunderstanding \"percentage points\" vs \"percent\", ignoring that both F1 and EM change, or answering qualitatively instead of giving the numeric amount.", "new_prompt": "Definition: You will be given a short excerpt from an academic paper (context) followed by a question. Answer the question using only the information in the excerpt.\n\nInput: The paper states that combining question answering (QA) with question generation (QG) into a single joint model improves results: both Exact Match (EM) and F1 score rise by roughly 10 percentage points relative to a QA-only system.\n\nQuestion: How many percentage points higher are EM and F1 when QA and QG are trained jointly compared to training QA alone?", "reference_answer": "N/A"}
{"original_prompt": "<bos><start_of_turn>user\nI'm going to give you some examples and then please give me some advice on those examples<end_of_turn>\n<start_of_turn>model\n", "original_scores": [1.3828125, -0.88671875, 1.3828125, 1.9921875, 1.3828125], "model_raw_response": "{\n    \"analysis\": \"The original prompt is challenging because it is under-specified and abruptly cut off: it promises examples but provides none, uses unfamiliar turn markers, and leaves the model unsure whether to wait for more input, ask clarifying questions, or proceed. This tests the model's ability to (a) resolve ambiguity and infer user intent, (b) manage conversational structure and turn-taking, (c) ask targeted clarifying questions when information is missing, and (d) produce concise, prioritized, actionable advice from examples when available.\",\n    \"new_prompt\": \"When I paste a single example in my next message, will you either (A) identify up to two core problems in that example and provide three prioritized, actionable improvements with a one-sentence rationale for each, or (B) if the example lacks essential context, instead ask up to three concise clarifying questions so you can give precise advice?\"\n}", "api_error": null, "parse_status": "success", "analysis": "The original prompt is challenging because it is under-specified and abruptly cut off: it promises examples but provides none, uses unfamiliar turn markers, and leaves the model unsure whether to wait for more input, ask clarifying questions, or proceed. This tests the model's ability to (a) resolve ambiguity and infer user intent, (b) manage conversational structure and turn-taking, (c) ask targeted clarifying questions when information is missing, and (d) produce concise, prioritized, actionable advice from examples when available.", "new_prompt": "When I paste a single example in my next message, will you either (A) identify up to two core problems in that example and provide three prioritized, actionable improvements with a one-sentence rationale for each, or (B) if the example lacks essential context, instead ask up to three concise clarifying questions so you can give precise advice?", "reference_answer": "N/A"}
